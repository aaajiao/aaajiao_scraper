#!/usr/bin/env python3
"""
aaajiao ä½œå“é›†çˆ¬è™« (Optimized v3 - Firecrawl Edition)
ä» https://eventstructure.com/ æŠ“å–æ‰€æœ‰ä½œå“è¯¦ç»†ä¿¡æ¯

v3 æ”¹è¿›ï¼š
1. ä½¿ç”¨ Firecrawl AI æå–ç»“æ„åŒ–æ•°æ® (ç²¾å‡†åº¦å¤§å¹…æå‡)
2. API Key å®‰å…¨ç®¡ç† (ç¯å¢ƒå˜é‡)
3. æ™ºèƒ½é€Ÿç‡æ§åˆ¶ (é¿å… Rate Limit)
4. æœ¬åœ°ç¼“å­˜ (èŠ‚çœ API è°ƒç”¨)
5. å®æ—¶è¿›åº¦æ¡ (ç”¨æˆ·å‹å¥½)
"""

import os
import sys
import time
import re
import json
import logging
import hashlib
import pickle
import concurrent.futures
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin
import argparse
from threading import Lock
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


class RateLimiter:
    """çº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶å™¨"""
    def __init__(self, calls_per_minute: int = 5):
        self.interval = 60.0 / calls_per_minute
        self.last_call = 0
        self.lock = Lock()
    
    def wait(self):
        """ç­‰å¾…ç›´åˆ°å…è®¸ä¸‹ä¸€æ¬¡è°ƒç”¨"""
        with self.lock:
            elapsed = time.time() - self.last_call
            if elapsed < self.interval:
                sleep_time = self.interval - elapsed
                logger.debug(f"Rate limit: sleeping {sleep_time:.2f}s")
                time.sleep(sleep_time)
            self.last_call = time.time()

class AaajiaoScraper:
    BASE_URL = "https://eventstructure.com"
    SITEMAP_URL = "https://eventstructure.com/sitemap.xml"
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    MAX_WORKERS = 2  # é™ä½å¹¶å‘æ•°ï¼Œé…åˆé€Ÿç‡æ§åˆ¶
    TIMEOUT = 15
    FC_TIMEOUT = 30  # Firecrawl ä¸“ç”¨è¶…æ—¶
    CACHE_DIR = ".cache"
    
    # ==================== æå– Schema å®šä¹‰ ====================
    # Quick æ¨¡å¼ï¼šä»…æå–æ ¸å¿ƒå­—æ®µï¼ŒèŠ‚çœ credits
    QUICK_SCHEMA = {
        "type": "object",
        "properties": {
            "title": {"type": "string", "description": "English title of the artwork"},
            "title_cn": {"type": "string", "description": "Chinese title if available"},
            "year": {"type": "string", "description": "Creation year or year range"},
            "category": {"type": "string", "description": "Art category (e.g. Video, Installation)"},
            "has_images": {"type": "boolean", "description": "Whether the page contains images"}
        }
    }
    
    # Full æ¨¡å¼ï¼šå®Œæ•´å­—æ®µæå–
    FULL_SCHEMA = {
        "type": "object",
        "properties": {
            "title": {"type": "string", "description": "English title"},
            "title_cn": {"type": "string", "description": "Chinese title"},
            "year": {"type": "string", "description": "Creation year"},
            "category": {"type": "string", "description": "Art category"},
            "description_en": {"type": "string", "description": "Full English description"},
            "description_cn": {"type": "string", "description": "Full Chinese description"},
            "high_res_images": {
                "type": "array",
                "items": {"type": "string"},
                "description": "High-res image URLs, prefer 'src_o' attribute"
            },
            "video_link": {"type": "string", "description": "Vimeo/YouTube URL if present"},
            "materials": {"type": "string", "description": "Materials used in the artwork"}
        }
    }
    
    # ==================== Prompt æ¨¡æ¿åº“ ====================
    PROMPT_TEMPLATES = {
        "quick": "Extract basic artwork info: title (English and Chinese if available), year, and category. Return JSON only, no explanation.",
        "full": "Extract complete artwork details including title, year, category, full descriptions in English and Chinese, materials, and all high-resolution image URLs (use 'src_o' attribute when available). Return JSON only.",
        "images_only": "Extract all high-resolution image URLs from the page. Prioritize 'src_o' attributes for high-res versions. Exclude thumbnails and icons. Return as JSON array of URLs.",
        "default": "Extract all text content from the page (title, description, metadata, full text). Also extract the URL of the first visible image (or main artwork image) and map it to the field 'image'. IMPORTANT: If the image has a 'src_o' attribute, extract that URL for high resolution."
    }


    def __init__(self, use_cache: bool = True):
        self.session = self._create_retry_session()
        self.works: List[Dict[str, Any]] = []
        self.use_cache = use_cache
        
        # åŠ è½½ API Key
        self.firecrawl_key = self._load_api_key()
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨ (5 calls/min)
        self.rate_limiter = RateLimiter(calls_per_minute=5)
        
        logger.info(f"Scraper åˆå§‹åŒ–å®Œæˆ (ç¼“å­˜: {'å¼€å¯' if use_cache else 'å…³é—­'})")
    
    def _load_api_key(self) -> str:
        """ä»ç¯å¢ƒå˜é‡æˆ– .env æ–‡ä»¶åŠ è½½ API Key"""
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
        key = os.getenv("FIRECRAWL_API_KEY")
        
        # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•è¯»å– .env æ–‡ä»¶
        if not key:
            env_file = os.path.join(os.path.dirname(__file__), '.env')
            if os.path.exists(env_file):
                with open(env_file, 'r') as f:
                    for line in f:
                        if line.startswith('FIRECRAWL_API_KEY='):
                            key = line.split('=', 1)[1].strip()
                            break
        
        if not key:
            raise ValueError(
                "æœªæ‰¾åˆ° Firecrawl API Keyï¼\n"
                "è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export FIRECRAWL_API_KEY='your-key'\n"
                "æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶"
            )
        
        logger.info(f"API Key åŠ è½½æˆåŠŸ (é•¿åº¦: {len(key)})")
        return key

    def _create_retry_session(self, retries: int = 3, backoff_factor: float = 0.5) -> requests.Session:
        session = requests.Session()
        retry = Retry(
            total=retries,
            read=retries,
            connect=retries,
            backoff_factor=backoff_factor,
            status_forcelist=[500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.headers.update(self.HEADERS)
        return session

    def get_all_work_links(self, incremental: bool = False) -> List[str]:
        """
        ä» Sitemap è·å–æ‰€æœ‰ä½œå“é“¾æ¥
        
        Args:
            incremental: æ˜¯å¦åªè¿”å›æ›´æ–°/æ–°å¢çš„é“¾æ¥
        
        Returns:
            æœ‰æ•ˆä½œå“é“¾æ¥åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨è¯»å– Sitemap: {self.SITEMAP_URL}")
        try:
            response = self.session.get(self.SITEMAP_URL, timeout=self.TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è§£æ URL å’Œ lastmod
            current_sitemap = {}  # {url: lastmod}
            raw_urls = soup.find_all('url')
            logger.info(f"Sitemap raw url tags found: {len(raw_urls)}")
            
            for url_tag in raw_urls:
                loc = url_tag.find('loc')
                lastmod = url_tag.find('lastmod')
                if loc:
                    url = loc.get_text().strip()
                    if self._is_valid_work_link(url):
                        current_sitemap[url] = lastmod.get_text().strip() if lastmod else ""
                    else:
                        # logger.debug(f"Filtered: {url}") # Optional: log filtered
                        pass
            
            logger.info(f"Sitemap ä¸­æ‰¾åˆ° {len(current_sitemap)} ä¸ªæœ‰æ•ˆä½œå“é“¾æ¥ (Filtered from {len(raw_urls)})")
            
            if not incremental:
                # å…¨é‡æ¨¡å¼ï¼šä¿å­˜ç¼“å­˜åè¿”å›æ‰€æœ‰é“¾æ¥
                self._save_sitemap_cache(current_sitemap)
                return sorted(list(current_sitemap.keys()))
            
            # å¢é‡æ¨¡å¼ï¼šæ¯”è¾ƒç¼“å­˜
            cached_sitemap = self._load_sitemap_cache()
            changed_urls = []
            
            for url, lastmod in current_sitemap.items():
                if url not in cached_sitemap:
                    # æ–°å¢ URL
                    changed_urls.append(url)
                    logger.info(f"ğŸ†• æ–°å¢: {url}")
                elif lastmod and lastmod != cached_sitemap.get(url, ""):
                    # lastmod å˜åŒ–
                    changed_urls.append(url)
                    logger.info(f"ğŸ”„ æ›´æ–°: {url} ({cached_sitemap.get(url)} â†’ {lastmod})")
            
            if changed_urls:
                logger.info(f"ğŸ“Š å¢é‡æ£€æµ‹: {len(changed_urls)} ä¸ªæ›´æ–°/æ–°å¢")
            else:
                logger.info("âœ… æ²¡æœ‰æ£€æµ‹åˆ°æ›´æ–°")
            
            # ä¿å­˜æ–°ç¼“å­˜
            self._save_sitemap_cache(current_sitemap)
            
            return sorted(changed_urls)
            
        except Exception as e:
            logger.error(f"Sitemap è¯»å–å¤±è´¥: {e}")
            return self._fallback_scan_main_page()
    
    def _load_sitemap_cache(self) -> Dict[str, str]:
        """åŠ è½½ sitemap lastmod ç¼“å­˜"""
        cache_path = os.path.join(self.CACHE_DIR, "sitemap_lastmod.json")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _save_sitemap_cache(self, sitemap: Dict[str, str]):
        """ä¿å­˜ sitemap lastmod ç¼“å­˜"""
        cache_path = os.path.join(self.CACHE_DIR, "sitemap_lastmod.json")
        os.makedirs(self.CACHE_DIR, exist_ok=True)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(sitemap, f, ensure_ascii=False, indent=2)

    def _fallback_scan_main_page(self):
        """å¤‡ç”¨æ–¹æ¡ˆï¼šä»ä¸»é¡µæ‰«æé“¾æ¥"""
        logger.info("å°è¯•æ‰«æä¸»é¡µé“¾æ¥ (å¤‡ç”¨æ–¹æ¡ˆ)...")
        try:
            r = self.session.get(self.BASE_URL, timeout=self.TIMEOUT)
            soup = BeautifulSoup(r.text, 'html.parser')
            links = set()
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith('/'):
                    full = urljoin(self.BASE_URL, href)
                    if self._is_valid_work_link(full):
                        links.add(full)
            return list(links)
        except Exception as e:
            logger.error(f"ä¸»é¡µæ‰«æå¤±è´¥: {e}")
            return []

    def _is_valid_work_link(self, url: str) -> bool:
        """è¿‡æ»¤éä½œå“é“¾æ¥"""
        if not url.startswith(self.BASE_URL):
            return False
            
        path = url.replace(self.BASE_URL, '')
        
        # æ’é™¤åˆ—è¡¨
        excludes = [
            '/', '/rss', '/feed', '/filter', '/aaajiao', 
            '/contact', '/cv', '/about', '/index', '/sitemap'
        ]
        
        if path in ['/', '']: return False
        
        for ex in excludes:
            if ex in path and len(path) < 20: # simple heuristic
                if path == ex or path.startswith(ex + '/'):
                    return False
        
        # Cargo ç‰¹æ€§: å¾€å¾€ä½œå“é“¾æ¥éƒ½å¾ˆçŸ­ï¼Œæˆ–è€…åŒ…å«ç‰¹å®šå…³é”®è¯
        # è¿™é‡Œä¸»è¦æ’é™¤ filter é¡µé¢
        if '/filter/' in path: return False
        
        return True

    def extract_work_details(self, url: str, retry_count: int = 0) -> Optional[Dict[str, Any]]:
        """æå–è¯¦æƒ… (ä½¿ç”¨ Firecrawl AI æå–ï¼Œå¸¦ç¼“å­˜å’Œé‡è¯•)"""
        max_retries = 3
        
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            cached = self._load_cache(url)
            if cached:
                logger.debug(f"å‘½ä¸­ç¼“å­˜: {url}")
                return cached
        
        # 2. é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait()
        
        try:
            logger.info(f"[{retry_count+1}/{max_retries}] æ­£åœ¨æŠ“å–: {url}")
            
            fc_endpoint = "https://api.firecrawl.dev/v2/scrape"
            
            schema = {
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "The English title of the work"},
                    "title_cn": {"type": "string", "description": "The Chinese title of the work. If not explicitly found, leave empty."},
                    "year": {"type": "string", "description": "Creation year or year range (e.g. 2018-2022)"},
                    "year": {"type": "string", "description": "Creation year or year range (e.g. 2018-2022)"},
                    "category": {"type": "string", "description": "The art category (e.g. Video Installation, Software, Website)"},
                    "materials": {"type": "string", "description": "Materials list (e.g. LED screen, 3D printing)"},
                    "materials": {"type": "string", "description": "Materials list (e.g. LED screen, 3D printing)"},
                    "description_en": {"type": "string", "description": "Detailed work description in English. Exclude navigation text."},
                    "description_cn": {"type": "string", "description": "Detailed work description in Chinese. Exclude navigation text."},
                    "video_link": {"type": "string", "description": "Vimeo URL if present"}
                },
                "required": ["title"]
            }
            
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {
                    "schema": schema,
                    "systemPrompt": "You are an art archivist. Extract the artwork metadata from the portfolio page. Ignore navigation links like 'Previous/Next project'. The title usually appears as 'English Title / Chinese Title'. Separate them."
                }
            }
            
            headers = {
                "Authorization": f"Bearer {self.firecrawl_key}",
                "Content-Type": "application/json"
            }
            
            resp = requests.post(fc_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
            
            if resp.status_code == 200:
                data = resp.json()
                if 'data' in data and 'extract' in data['data']:
                    result = data['data']['extract']
                    
                    work = {
                        'url': url,
                        'title': result.get('title', ''),
                        'title_cn': result.get('title_cn', ''),
                        'type': result.get('category', '') or result.get('type', ''),
                        'materials': result.get('materials', ''),
                        'year': result.get('year', ''),
                        'description_cn': result.get('description_cn', ''),
                        'description_en': result.get('description_en', ''),
                        'video_link': result.get('video_link', ''),
                        'size': '',
                        'duration': '',
                        'tags': []
                    }
                    
                    # åå¤„ç†ï¼šå¦‚æœ AI æ²¡åˆ†æ¸…æ ‡é¢˜
                    if not work['title_cn'] and '/' in work['title']:
                        parts = work['title'].split('/')
                        work['title'] = parts[0].strip()
                        if len(parts) > 1:
                            work['title_cn'] = parts[1].strip()
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    if self.use_cache:
                        self._save_cache(url, work)
                            
                    return work
                else:
                    logger.error(f"Firecrawl è¿”å›æ ¼å¼å¼‚å¸¸: {data}")
                    
            elif resp.status_code == 429:
                # Rate Limit - æŒ‡æ•°é€€é¿é‡è¯•
                if retry_count >= max_retries:
                    logger.error(f"é‡è¯•æ¬¡æ•°è¶…é™: {url}")
                    return None
                wait_time = 2 ** retry_count  # 1s, 2s, 4s
                logger.warning(f"Rate Limitï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                time.sleep(wait_time)
                return self.extract_work_details(url, retry_count + 1)
                
            else:
                logger.error(f"Firecrawl Error {resp.status_code}: {resp.text[:200]}")
                
            return None

        except Exception as e:
            logger.error(f"API è¯·æ±‚é”™è¯¯ {url}: {e}")
            return None
    
    # ==================== ç¼“å­˜ç³»ç»Ÿ ====================
    
    def _get_cache_path(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_dir = os.path.join(os.path.dirname(__file__), '.cache')
        return os.path.join(cache_dir, f"{url_hash}.pkl")
    
    def _load_cache(self, url: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                pass
        return None
    
    def _save_cache(self, url: str, data: Dict):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        cache_dir = os.path.dirname(cache_path)
        os.makedirs(cache_dir, exist_ok=True)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.debug(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # ==================== Extract ç¼“å­˜ï¼ˆv2/extract ä¸“ç”¨ï¼‰====================
    
    @property
    def cache_dir(self) -> str:
        """ç¼“å­˜ç›®å½•è·¯å¾„"""
        return os.path.join(os.path.dirname(__file__), '.cache')
    
    def _get_extract_cache_path(self, url: str, prompt_hash: str) -> str:
        """ç”Ÿæˆ Extract ç¼“å­˜è·¯å¾„ï¼ˆåŒ…å« prompt hash é˜²æ­¢ä¸åŒ prompt å†²çªï¼‰"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return os.path.join(self.cache_dir, f"extract_{url_hash}_{prompt_hash[:8]}.pkl")
    
    def _load_extract_cache(self, url: str, prompt: str) -> Optional[Dict]:
        """åŠ è½½ Extract ç¼“å­˜"""
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        cache_path = self._get_extract_cache_path(url, prompt_hash)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    logger.debug(f"Extract ç¼“å­˜å‘½ä¸­: {url[:50]}...")
                    return pickle.load(f)
            except Exception:
                pass
        return None
    
    def _save_extract_cache(self, url: str, prompt: str, data: Dict):
        """ä¿å­˜ Extract ç¼“å­˜"""
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        cache_path = self._get_extract_cache_path(url, prompt_hash)
        os.makedirs(self.cache_dir, exist_ok=True)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.debug(f"Extract ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # ==================== Discovery ç¼“å­˜ï¼ˆæ‰«æç»“æœæŒä¹…åŒ–ï¼‰====================
    
    def _get_discovery_cache_path(self, url: str, scroll_mode: str) -> str:
        """ç”Ÿæˆ Discovery ç¼“å­˜è·¯å¾„"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return os.path.join(self.cache_dir, f"discovery_{url_hash}_{scroll_mode}.json")
    
    def _is_discovery_cache_valid(self, cache_path: str, ttl_hours: int = 24) -> bool:
        """æ£€æŸ¥ Discovery ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆé»˜è®¤ 24h TTLï¼‰"""
        if not os.path.exists(cache_path):
            return False
        mtime = os.path.getmtime(cache_path)
        return (time.time() - mtime) < (ttl_hours * 3600)
    
    # ==================== æ•°æ®éªŒè¯ ====================
    
    def validate_work(self, work: Dict) -> bool:
        """éªŒè¯ä½œå“æ•°æ®å®Œæ•´æ€§"""
        if not work.get('title'):
            logger.warning(f"ä½œå“ç¼ºå°‘æ ‡é¢˜: {work.get('url')}")
            return False
        return True

    def scrape_all(self, incremental: bool = False):
        """
        æŠ“å–æ‰€æœ‰ä½œå“ï¼ˆå¸¦è¿›åº¦æ¡å’ŒéªŒè¯ï¼‰
        
        Args:
            incremental: å¢é‡æ¨¡å¼ï¼ŒåªæŠ“å–æ›´æ–°/æ–°å¢çš„é¡µé¢
        """
        work_links = self.get_all_work_links(incremental=incremental)
        
        if incremental and not work_links:
            logger.info("âœ… å¢é‡æ¨¡å¼ï¼šæ²¡æœ‰æ£€æµ‹åˆ°æ›´æ–°ï¼Œè·³è¿‡æŠ“å–")
            return 0, 0  # (valid_count, failed_count)
        
        total = len(work_links)
        valid_count = 0
        failed_count = 0
        
        mode_label = "å¢é‡æŠ“å–" if incremental else "å…¨é‡æŠ“å–"
        logger.info(f"å¼€å§‹{mode_label} {total} ä¸ªä½œå“...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self.extract_work_details, url): url for url in work_links}
            
            for future in tqdm(concurrent.futures.as_completed(future_to_url), 
                               total=total, 
                               desc=mode_label,
                               unit="ä½œå“"):
                url = future_to_url[future]
                try:
                    data = future.result()
                    if data and self.validate_work(data):
                        self.works.append(data)
                        valid_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
                    failed_count += 1

        logger.info(f"æŠ“å–å®Œæˆï¼æœ‰æ•ˆ: {valid_count}/{total}, å¤±è´¥: {failed_count}")
        return valid_count, failed_count

    def save_to_json(self, filename: str = 'aaajiao_works.json'):
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.works, f, ensure_ascii=False, indent=2)

    def generate_markdown(self, filename: str = 'aaajiao_portfolio.md'):
        """ç”Ÿæˆ Markdown æ ¼å¼çš„ä½œå“é›†æ–‡æ¡£"""
        lines = [
            "# aaajiao ä½œå“é›† / aaajiao Portfolio\n",
            f"Source: {self.BASE_URL}\n",
            "Generated by aaajiao Scraper v3 (Firecrawl Edition)\n",
            "\n---\n\n"
        ]
        
        # Sort by year
        sorted_works = sorted(self.works, key=lambda x: x.get('year') or '0000', reverse=True)
        
        current_year = None
        for work in sorted_works:
            year = work.get('year', 'Unknown')
            if year != current_year:
                lines.append(f"## {year}\n\n")
                current_year = year
                
            title = work.get('title', 'Untitled')
            title_cn = work.get('title_cn', '')
            
            header = f"### [{title}]({work['url']})"
            if title_cn:
                header += f" / {title_cn}"
            lines.append(header + "\n\n")
            
            if work.get('type'): 
                lines.append(f"**Type**: {work['type']}\n\n")
            if work.get('materials'):
                lines.append(f"**Materials**: {work['materials']}\n\n")
            if work.get('video_link'): 
                lines.append(f"**Video**: {work['video_link']}\n\n")
            
            if work.get('description_cn'):
                lines.append(f"> {work['description_cn']}\n\n")
                
            if work.get('description_en'):
                lines.append(f"{work['description_en']}\n\n")
                 
            lines.append("---\n")
            
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("".join(lines))
        
        logger.info(f"Markdown æ–‡ä»¶å·²ç”Ÿæˆ: {filename}")

    # ==================== Discovery Mode ====================

    def discover_urls_with_scroll(self, url: str, scroll_mode: str = "auto", use_cache: bool = True) -> List[str]:
        """
        Phase 1: ä½¿ç”¨ Scrape æ¨¡å¼ + æ»šåŠ¨åŠ¨ä½œå»å‘ç°ä½œå“é“¾æ¥
        
        Args:
            url: ç›®æ ‡åˆ—è¡¨é¡µ URL
            scroll_mode: æ»šåŠ¨æ¨¡å¼ ("auto", "horizontal", "vertical")
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼Œ24h TTLï¼‰
            
        Returns:
            å‘ç°çš„ä½œå“ URL åˆ—è¡¨
        """
        
        # === ç¼“å­˜æ£€æŸ¥ ===
        cache_path = self._get_discovery_cache_path(url, scroll_mode)
        if use_cache and self._is_discovery_cache_valid(cache_path):
            try:
                with open(cache_path, 'r') as f:
                    cached = json.load(f)
                    logger.info(f"âœ… Discovery ç¼“å­˜å‘½ä¸­: {len(cached)} é“¾æ¥ (TTL: 24h)")
                    return cached
            except Exception:
                pass
        
        logger.info(f"ğŸ•µï¸  å¯åŠ¨ Discovery Phase: {url} (Mode: {scroll_mode})")
        
        # 1. é…ç½®æ»šåŠ¨åŠ¨ä½œ (æŒ‰ç…§ Firecrawl å®˜æ–¹æ–‡æ¡£æ ¼å¼)
        actions = []
        
        # åˆå§‹ç­‰å¾…é¡µé¢åŠ è½½
        actions.append({"type": "wait", "milliseconds": 2000})
        
        if scroll_mode == "horizontal":
            # æ¨ªå‘æ»šåŠ¨ï¼šä½¿ç”¨å¢å¼ºç‰ˆ JS è„šæœ¬ (æ¨¡æ‹Ÿæ»šåŠ¨åˆ°åº•éƒ¨è§¦å‘åŠ è½½)
            # è°ƒæ•´ä¸º 20 æ¬¡å¾ªç¯ (è¿™æ˜¯ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼š15æ¬¡ä¸å¤Ÿå…¨ï¼Œ30æ¬¡ä¼šè¶…æ—¶)
            # æ¯æ¬¡ 1.5sï¼Œæ€»è€—æ—¶çº¦ 35sï¼Œå®‰å…¨å¯é 
            for i in range(20):
                actions.append({
                    "type": "executeJavascript", 
                    "script": """
                        // 1. æ»šåŠ¨åˆ°å½“å‰æœ€å³ä¾§
                        window.scrollTo(document.documentElement.scrollWidth, 0);
                        // 2. è§¦å‘ scroll äº‹ä»¶ä»¥æ¿€æ´»æ‡’åŠ è½½
                        window.dispatchEvent(new Event('scroll'));
                    """
                })
                # ç­‰å¾… Carg CMS åŠ è½½æ–°å†…å®¹
                actions.append({"type": "wait", "milliseconds": 1500})
                
        elif scroll_mode == "vertical":
            # å‚ç›´æ»šåŠ¨ï¼šä½¿ç”¨åŸç”Ÿ scroll
            for _ in range(5):
                actions.append({"type": "scroll", "direction": "down"})
                actions.append({"type": "wait", "milliseconds": 1500})
            
        else:  # auto Mode
            # æ··åˆæ¨¡å¼ï¼šæ¨ªå‘å¢å¼º + å‚ç›´
            # 1. æ¨ªå‘æ»šåŠ¨ (JS)
            for i in range(15):
                actions.append({
                    "type": "executeJavascript", 
                    "script": "window.scrollTo(document.documentElement.scrollWidth, 0); window.dispatchEvent(new Event('scroll'));"
                })
                actions.append({"type": "wait", "milliseconds": 1500})
            
            # 2. å‚ç›´æ»šåŠ¨
            for _ in range(3):
                actions.append({"type": "scroll", "direction": "down"})
                actions.append({"type": "wait", "milliseconds": 1500})
        
        payload = {
            "url": url,
            "formats": ["html"],
            "actions": actions,
            "onlyMainContent": False,  # è·å–å®Œæ•´ DOM ä»¥ä¾¿æå–é“¾æ¥
            "timeout": 300000 # 5åˆ†é’Ÿè¶…æ—¶ï¼Œç¡®ä¿è·‘å®Œæ‰€æœ‰æ»šåŠ¨åŠ¨ä½œ
        }
        
        # ä½¿ç”¨ v2 endpoint (å®˜æ–¹æ–‡æ¡£æ¨è)
        endpoint = "https://api.firecrawl.dev/v2/scrape"
        headers = {
            "Authorization": f"Bearer {self.firecrawl_key}",
            "Content-Type": "application/json"
        }
        
        try:
            logger.info(f"   æ­£åœ¨æ‰§è¡Œ Scrape + Actions (å…± {len(actions)} æ­¥)...")
            resp = requests.post(endpoint, json=payload, headers=headers, timeout=300)
            
            if resp.status_code != 200:
                logger.error(f"Scrape å¤±è´¥: {resp.status_code} - {resp.text[:200]}")
                return []
                
            data = resp.json()
            if not data.get("success"):
                logger.error(f"Scrape ä»»åŠ¡å¤±è´¥: {data}")
                return []
                
            html_content = data.get("data", {}).get("html", "")
            if not html_content:
                logger.error("æœªè·å–åˆ° HTML å†…å®¹")
                return []
                
            # 2. ä» HTML æå–é“¾æ¥
            logger.info(f"   è·å–åˆ° HTML ({len(html_content)} chars)ï¼Œæ­£åœ¨æå–é“¾æ¥...")
            found_links = self._extract_links_from_html(html_content, url)
            
            # === ä¿å­˜åˆ°ç¼“å­˜ ===
            if found_links and use_cache:
                try:
                    os.makedirs(self.cache_dir, exist_ok=True)
                    with open(cache_path, 'w') as f:
                        json.dump(found_links, f)
                    logger.info(f"ğŸ“ Discovery ç»“æœå·²ç¼“å­˜ ({len(found_links)} é“¾æ¥)")
                except Exception as e:
                    logger.debug(f"Discovery ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            
            return found_links
            
        except Exception as e:
            logger.error(f"Discovery å¼‚å¸¸: {e}")
            return []

    def _extract_links_from_html(self, html: str, base_url: str) -> List[str]:
        """ä» HTML ä¸­æå–æœ‰ä»·å€¼çš„ä½œå“é“¾æ¥"""
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin
        
        soup = BeautifulSoup(html, 'html.parser')
        links = set()
        
        # ç»è¿‡åˆ†æï¼Œä½œå“é“¾æ¥ä½¿ç”¨ 'nohover' classï¼Œä¸”ä½äº #content_container å†…
        # ç¤ºä¾‹: <a class="nohover" href="/Project-Name">Title</a>
        # æˆ‘ä»¬ä½¿ç”¨ç²¾å‡†çš„ CSS selector æ¥æå–
        
        # 1. å°è¯•ä½¿ç”¨ç²¾å‡† selector
        artwork_links = soup.select('a.nohover')
        
        if not artwork_links:
             # Fallback if class changes: search inside content container
             container = soup.select_one('#content_container')
             if container:
                 artwork_links = container.find_all('a', href=True)
             else:
                 artwork_links = soup.find_all('a', href=True)
                 
        for a_tag in artwork_links:
            href = a_tag.get('href')
            if not href:
                continue
                
            full_url = urljoin(base_url, href)
            
            # è¿‡æ»¤é€»è¾‘ï¼šå†æ¬¡ç¡®ä¿ä¸åŒ…å«éä½œå“é¡µ
            if base_url in full_url:
                # æ’é™¤å¸¸è§éä½œå“é¡µé¢ (Double Check)
                if not any(x in full_url.lower() for x in ['contact', 'about', 'cv', 'text', 'press', 'index', 'filter']):
                    links.add(full_url)
                
        sorted_links = sorted(list(links))
        logger.info(f"   å‘ç° {len(sorted_links)} ä¸ªæ½œåœ¨ä½œå“é“¾æ¥")
        return sorted_links

    # ==================== Agent æ¨¡å¼ ====================
    
    def agent_search(self, prompt: str, urls: Optional[List[str]] = None, 
                      max_credits: int = 50, extraction_level: str = "custom") -> Optional[Dict[str, Any]]:
        """
        æ™ºèƒ½æœç´¢/æå–å…¥å£
        
        Args:
            prompt: æå–æŒ‡ä»¤
            urls: è¦æå–çš„ URL åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            max_credits: æœ€å¤§å¤„ç†æ•°é‡ / Agent é¢„ç®—
            extraction_level: æå–çº§åˆ« - "quick"(æ ¸å¿ƒå­—æ®µ), "full"(å®Œæ•´), "custom"(ç”¨æˆ·è‡ªå®šä¹‰)
        
        ç­–ç•¥åˆ†ç¦»:
        1. æŒ‡å®š URLs -> ä½¿ç”¨ v2/extract æ‰¹é‡æå– -> é’ˆå¯¹å·²çŸ¥é¡µé¢è¿›è¡Œç»“æ„åŒ–/å†…å®¹æå–
        2. æ—  URLs (å¼€æ”¾æŸ¥è¯¢) -> ä½¿ç”¨ v2/agent -> æˆæœ¬é«˜ (è‡ªä¸»è°ƒç ”)
        """
        
        # === æ ¹æ®æå–çº§åˆ«é€‰æ‹© Schema å’Œ Prompt ===
        schema = None
        if extraction_level == "quick":
            schema = self.QUICK_SCHEMA
            if not prompt or prompt == self.PROMPT_TEMPLATES["default"]:
                prompt = self.PROMPT_TEMPLATES["quick"]
            logger.info(f"ğŸ“‹ ä½¿ç”¨ Quick æ¨¡å¼ (æ ¸å¿ƒå­—æ®µ)")
        elif extraction_level == "full":
            schema = self.FULL_SCHEMA
            if not prompt or prompt == self.PROMPT_TEMPLATES["default"]:
                prompt = self.PROMPT_TEMPLATES["full"]
            logger.info(f"ğŸ“‹ ä½¿ç”¨ Full æ¨¡å¼ (å®Œæ•´å­—æ®µ)")
        elif extraction_level == "images_only":
            if not prompt or prompt == self.PROMPT_TEMPLATES["default"]:
                prompt = self.PROMPT_TEMPLATES["images_only"]
            logger.info(f"ğŸ–¼ï¸ ä½¿ç”¨ Images Only æ¨¡å¼ (ä»…é«˜æ¸…å›¾)")
        # custom æ¨¡å¼ä½¿ç”¨ç”¨æˆ·æä¾›çš„ promptï¼Œä¸æ·»åŠ  schema
        
        # === åœºæ™¯ 1: æ‰¹é‡æå– (æŒ‡å®š URL) ===
        if urls and len(urls) > 0:
            # é™åˆ¶ URL æ•°é‡ä»¥ç¬¦åˆ Max Credits
            target_urls = urls[:max_credits]
            
            # === ç¼“å­˜æ£€æŸ¥ï¼šåˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„ URL ===
            cached_results = []
            uncached_urls = []
            for url in target_urls:
                cached = self._load_extract_cache(url, prompt)
                if cached:
                    cached_results.append(cached)
                else:
                    uncached_urls.append(url)
            
            logger.info(f"ğŸ” ç¼“å­˜æ£€æŸ¥: å‘½ä¸­ {len(cached_results)}, å¾…æå– {len(uncached_urls)}")
            
            # å¦‚æœå…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
            if not uncached_urls:
                logger.info(f"âœ… å…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼ŒèŠ‚çœ API è°ƒç”¨ï¼")
                return {"data": cached_results, "from_cache": True, "cached_count": len(cached_results)}
            
            logger.info(f"ğŸš€ å¯åŠ¨æ‰¹é‡æå–ä»»åŠ¡ (Target: {len(uncached_urls)} URLs)")
            logger.info(f"   Prompt: {prompt[:100]}...")
            
            extract_endpoint = "https://api.firecrawl.dev/v2/extract"
            headers = {
                "Authorization": f"Bearer {self.firecrawl_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "urls": uncached_urls,  # åªæå–æœªç¼“å­˜çš„ URL
                "prompt": prompt,
                "enableWebSearch": False
            }
            
            # å¦‚æœæŒ‡å®šäº† Schemaï¼Œæ·»åŠ åˆ° payload
            if schema:
                payload["schema"] = schema

            try:
                # 1. æäº¤ä»»åŠ¡
                resp = requests.post(extract_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
                
                if resp.status_code != 200:
                    logger.error(f"Extract å¯åŠ¨å¤±è´¥: {resp.status_code} - {resp.text}")
                    # å¦‚æœ API è°ƒç”¨å¤±è´¥ä½†æœ‰ç¼“å­˜ç»“æœï¼Œè¿”å›ç¼“å­˜éƒ¨åˆ†
                    if cached_results:
                        return {"data": cached_results, "from_cache": True, "cached_count": len(cached_results)}
                    return None
                    
                result = resp.json()
                if not result.get("success"):
                    logger.error(f"Extract å¯åŠ¨å¤±è´¥: {result}")
                    if cached_results:
                        return {"data": cached_results, "from_cache": True, "cached_count": len(cached_results)}
                    return None
                
                job_id = result.get("id")
                if not job_id:
                     if result.get("status") == "completed":
                         new_data = result.get("data", [])
                         # ä¿å­˜æ–°ç»“æœåˆ°ç¼“å­˜
                         for item in new_data if isinstance(new_data, list) else [new_data]:
                             item_url = item.get("url") or item.get("sourceURL") or item.get("source_url")
                             if item_url:
                                 self._save_extract_cache(item_url, prompt, item)
                         return {"data": cached_results + (new_data if isinstance(new_data, list) else [new_data])}
                     return None

                # 2. è½®è¯¢ç­‰å¾…
                logger.info(f"   Extract ä»»åŠ¡ ID: {job_id}")
                status_endpoint = f"{extract_endpoint}/{job_id}"
                max_wait = 600 # 10åˆ†é’Ÿ
                poll_interval = 5
                elapsed = 0
                
                while elapsed < max_wait:
                    time.sleep(poll_interval)
                    elapsed += poll_interval
                    
                    status_resp = requests.get(status_endpoint, headers=headers, timeout=self.FC_TIMEOUT)
                    if status_resp.status_code != 200: continue
                    
                    status_data = status_resp.json()
                    status = status_data.get("status")
                    
                    if status == "processing":
                        logger.info(f"   â³ æå–ä¸­... ({elapsed}s)")
                    elif status == "completed":
                        credits = status_data.get("creditsUsed", "N/A")
                        new_data = status_data.get("data", [])
                        
                        # === ä¿å­˜æ–°ç»“æœåˆ°ç¼“å­˜ ===
                        for item in new_data if isinstance(new_data, list) else [new_data]:
                            item_url = item.get("url") or item.get("sourceURL") or item.get("source_url")
                            if item_url:
                                self._save_extract_cache(item_url, prompt, item)
                                logger.debug(f"   ğŸ’¾ å·²ç¼“å­˜: {item_url[:50]}...")
                        
                        logger.info(f"âœ… æå–å®Œæˆ (Credits: {credits}, æ–°å¢ç¼“å­˜: {len(new_data) if isinstance(new_data, list) else 1})")
                        
                        # åˆå¹¶ç¼“å­˜å’Œæ–°ç»“æœ
                        all_data = cached_results + (new_data if isinstance(new_data, list) else [new_data])
                        return {"data": all_data, "cached_count": len(cached_results), "new_count": len(new_data) if isinstance(new_data, list) else 1}
                    elif status == "failed":
                        logger.error(f"æå–ä»»åŠ¡å¤±è´¥: {status_data}")
                        if cached_results:
                            return {"data": cached_results, "from_cache": True, "cached_count": len(cached_results)}
                        return None
                        
                return None
                
            except Exception as e:
                logger.error(f"Extract Exception: {e}")
                if cached_results:
                    return {"data": cached_results, "from_cache": True, "cached_count": len(cached_results)}
                return None

        # === åœºæ™¯ 2: å¼€æ”¾å¼ Agent æœç´¢ (æ—  URL) ===
        else:
            logger.info(f"ğŸ¤– å¯åŠ¨ Smart Agent ä»»åŠ¡ (å¼€æ”¾æœç´¢)...")
            logger.info(f"   Prompt: {prompt}")
            
            agent_endpoint = "https://api.firecrawl.dev/v2/agent"
            headers = {
                "Authorization": f"Bearer {self.firecrawl_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "prompt": prompt,
                "maxCredits": max_credits
            }
            
            try:
                # 1. å¯åŠ¨ Agent ä»»åŠ¡
                resp = requests.post(agent_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
                
                if resp.status_code != 200:
                    logger.error(f"Agent å¯åŠ¨å¤±è´¥: {resp.status_code} - {resp.text[:200]}")
                    return None
                
                result = resp.json()
                
                if not result.get("success"):
                    logger.error(f"Agent å¯åŠ¨å¤±è´¥: {result}")
                    return None
                
                job_id = result.get("id")
                
                if not job_id:
                    # åŒæ­¥æ¨¡å¼
                    if result.get("status") == "completed":
                        logger.info(f"âœ… Agent ä»»åŠ¡å®Œæˆ (credits: {result.get('creditsUsed', 'N/A')})")
                        return result.get("data")
                    return None
                
                # 2. è½®è¯¢ç­‰å¾…ä»»åŠ¡å®Œæˆ
                logger.info(f"   ä»»åŠ¡ ID: {job_id}")
                status_endpoint = f"{agent_endpoint}/{job_id}"
                max_wait = 300 
                elapsed = 0
                
                while elapsed < max_wait:
                    time.sleep(5)
                    elapsed += 5
                    
                    status_resp = requests.get(status_endpoint, headers=headers, timeout=self.FC_TIMEOUT)
                    
                    if status_resp.status_code != 200: continue
                    
                    status_data = status_resp.json()
                    status = status_data.get("status")
                    
                    if status == "processing":
                        logger.info(f"   â³ å¤„ç†ä¸­... ({elapsed}s)")
                        continue
                    elif status == "completed":
                        credits_used = status_data.get("creditsUsed", "N/A")
                        logger.info(f"âœ… Agent ä»»åŠ¡å®Œæˆ (è€—æ—¶: {elapsed}s, credits: {credits_used})")
                        return status_data.get("data")
                    elif status == "failed":
                        logger.error(f"Agent ä»»åŠ¡å¤±è´¥: {status_data}")
                        return None
                
                logger.error(f"Agent ä»»åŠ¡è¶…æ—¶ ({max_wait}s)")
                return None
                
            except Exception as e:
                logger.error(f"Agent è¯·æ±‚é”™è¯¯: {e}")
                return None


    # ==================== å›¾ç‰‡ä¸‹è½½å’ŒæŠ¥å‘Šç”Ÿæˆ ====================
    
    def download_images(self, image_urls: List[str], output_dir: str, timestamp: str = "") -> List[str]:
        """
        ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
        
        Args:
            image_urls: å›¾ç‰‡ URL åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            timestamp: æ—¶é—´æˆ³ï¼Œç”¨äºåˆ›å»ºç‹¬ç«‹çš„å›¾ç‰‡æ–‡ä»¶å¤¹
            
        Returns:
            æœ¬åœ°å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹å
        folder_name = f"images_{timestamp}" if timestamp else "images"
        images_dir = os.path.join(output_dir, folder_name)
        os.makedirs(images_dir, exist_ok=True)
        
        local_paths = []
        for i, url in enumerate(image_urls):
            try:
                # æ¸…ç† URL
                url = url.strip()
                if not url or not url.startswith(('http://', 'https://')):
                    continue
                
                # ç”Ÿæˆæ–‡ä»¶å
                ext = os.path.splitext(url.split('?')[0])[-1] or '.jpg'
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                    ext = '.jpg'
                filename = f"{i+1:02d}_image{ext}"
                local_path = os.path.join(images_dir, filename)
                
                logger.info(f"   ä¸‹è½½å›¾ç‰‡ [{i+1}/{len(image_urls)}]: {filename}")
                
                # ä¸‹è½½
                resp = self.session.get(url, timeout=30, stream=True)
                if resp.status_code == 200:
                    with open(local_path, 'wb') as f:
                        for chunk in resp.iter_content(chunk_size=8192):
                            f.write(chunk)
                    local_paths.append(f"{folder_name}/{filename}")
                else:
                    logger.warning(f"   ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {resp.status_code})")
                    
            except Exception as e:
                logger.warning(f"   ä¸‹è½½å¼‚å¸¸: {url} - {e}")
                
        return local_paths
    
    def generate_agent_report(self, data: Dict[str, Any], output_dir: str, prompt: str = "", extraction_level: str = "custom"):
        """
        æ ¹æ® Agent è¿”å›çš„æ•°æ®ç”Ÿæˆ Markdown æŠ¥å‘Šå’Œä¸‹è½½å›¾ç‰‡
        
        Args:
            data: Agent è¿”å›çš„æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            prompt: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ prompt
            extraction_level: æå–çº§åˆ«
        """
        from datetime import datetime
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"ğŸ“ ç”ŸæˆæŠ¥å‘Šåˆ°: {output_dir}")
        
        # è§£ææ•°æ®åˆ—è¡¨
        data_list = data.get("data", [data]) if isinstance(data, dict) else [data]
        if not isinstance(data_list, list):
            data_list = [data_list]
        
        # 1. åˆ›å»ºå›¾ç‰‡ç›®å½•å¹¶ä¸‹è½½æ‰€æœ‰å›¾ç‰‡ï¼Œå»ºç«‹ URL -> æœ¬åœ°è·¯å¾„æ˜ å°„
        images_dir = f"images_{timestamp}"
        images_path = os.path.join(output_dir, images_dir)
        os.makedirs(images_path, exist_ok=True)
        
        url_to_local = {}  # URL -> ç›¸å¯¹è·¯å¾„æ˜ å°„
        img_counter = 0
        
        for item in data_list:
            if not isinstance(item, dict):
                continue
            images = item.get("high_res_images") or item.get("images") or []
            for img_url in images:
                if img_url in url_to_local:
                    continue  # å·²ä¸‹è½½
                img_counter += 1
                try:
                    ext = os.path.splitext(img_url.split("?")[0])[-1] or ".jpg"
                    if not ext.startswith("."):
                        ext = ".jpg"
                    local_filename = f"{img_counter:02d}{ext}"
                    local_path = os.path.join(images_path, local_filename)
                    
                    resp = requests.get(img_url, timeout=30)
                    if resp.status_code == 200:
                        with open(local_path, "wb") as f:
                            f.write(resp.content)
                        url_to_local[img_url] = f"{images_dir}/{local_filename}"
                        logger.info(f"ğŸ“¥ ä¸‹è½½å›¾ç‰‡ [{img_counter}]: {local_filename}")
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url[:50]}... - {e}")
        
        logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(url_to_local)} å¼ å›¾ç‰‡")
        
        # 2. ç”Ÿæˆ Markdown æŠ¥å‘Š
        report_filename = f"report_{timestamp}.md"
        report_path = os.path.join(output_dir, report_filename)
        
        lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        lines.append("# ä½œå“æå–æŠ¥å‘Š\n\n")
        lines.append(f"> **æå–æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        lines.append(f"> **æå–æ¨¡å¼:** {extraction_level.upper()}\n")
        lines.append(f"> **ä½œå“æ•°é‡:** {len(data_list)}\n")
        lines.append("\n---\n\n")
        
        # æ¯ä¸ªä½œå“ä¸€ä¸ªç« èŠ‚
        for i, item in enumerate(data_list, 1):
            if not isinstance(item, dict):
                continue
            
            title = item.get("title", f"ä½œå“ {i}")
            title_cn = item.get("title_cn", "")
            year = item.get("year", "")
            
            if title_cn and title_cn != title:
                lines.append(f"## {i}. {title} / {title_cn}\n\n")
            else:
                lines.append(f"## {i}. {title}\n\n")
            
            # å±æ€§åˆ—è¡¨
            if year:
                lines.append(f"| å¹´ä»½ | {year} |\n")
            if item.get("category") or item.get("type"):
                lines.append(f"| ç±»å‹ | {item.get('category') or item.get('type')} |\n")
            if item.get("video_link"):
                lines.append(f"| è§†é¢‘ | [{item['video_link']}]({item['video_link']}) |\n")
            if item.get("materials"):
                lines.append(f"| ææ–™ | {item['materials']} |\n")
            lines.append("\n")
            
            # æè¿°
            desc_en = item.get("description_en") or item.get("description", "")
            desc_cn = item.get("description_cn", "")
            
            if desc_en or desc_cn:
                lines.append("### Description / æè¿°\n\n")
                if desc_en:
                    lines.append(f"**English:**\n\n{desc_en}\n\n")
                if desc_cn:
                    lines.append(f"**ä¸­æ–‡:**\n\n{desc_cn}\n\n")
            
            # å›¾ç‰‡ï¼ˆä½¿ç”¨æœ¬åœ°ç›¸å¯¹è·¯å¾„ï¼‰
            images = item.get("high_res_images") or item.get("images") or []
            if images:
                lines.append("### å›¾ç‰‡\n\n")
                for img_url in images[:6]:
                    local_rel_path = url_to_local.get(img_url)
                    if local_rel_path:
                        lines.append(f"![]({local_rel_path})\n\n")
                    else:
                        lines.append(f"![]({img_url})\n\n")  # fallback to URL
            
            lines.append("---\n\n")
        
        # å†™å…¥æ–‡ä»¶
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("".join(lines))
        
        logger.info(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # åŒæ—¶ä¿å­˜åŸå§‹ JSON
        json_filename = f"data_{timestamp}.json"
        json_path = os.path.join(output_dir, json_filename)
        
        output_data = {
            "_meta": {
                "prompt": prompt,
                "extraction_level": extraction_level,
                "timestamp": datetime.now().isoformat(),
            },
            **data
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ JSON æ•°æ®å·²ä¿å­˜: {json_path}")
    
    def _extract_image_urls(self, data: Dict[str, Any]) -> List[str]:
        """ä» Agent è¿”å›æ•°æ®ä¸­æå–æ‰€æœ‰å›¾ç‰‡ URL"""
        urls = []
        
        # å¸¸è§çš„å›¾ç‰‡å­—æ®µå
        image_fields = ['image_urls', 'images', 'image', 'imageUrls', 'imageUrl', 
                       'cover_image', 'thumbnail', 'photos', 'gallery']
        
        def extract_from_value(value):
            if isinstance(value, str):
                if value.startswith(('http://', 'https://')) and any(ext in value.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', 'image']):
                    urls.append(value)
            elif isinstance(value, list):
                for item in value:
                    extract_from_value(item)
            elif isinstance(value, dict):
                for v in value.values():
                    extract_from_value(v)
        
        # ä¼˜å…ˆæ£€æŸ¥å·²çŸ¥å­—æ®µ
        for field in image_fields:
            if field in data:
                extract_from_value(data[field])
        
        # é€’å½’æœç´¢æ‰€æœ‰å€¼
        if not urls:
            extract_from_value(data)
        
        return list(set(urls))  # å»é‡


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="aaajiao ä½œå“é›†çˆ¬è™« - Firecrawl Edition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æŠ“å–æ‰€æœ‰ä½œå“ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
  python3 aaajiao_scraper.py
  
  # Agent æ¨¡å¼ï¼šå¼€æ”¾å¼æŸ¥è¯¢
  python3 aaajiao_scraper.py --agent "Find all video installations by aaajiao"
  
  # Agent æ¨¡å¼ + æŒ‡å®š URL + å›¾ç‰‡ä¸‹è½½
  python3 aaajiao_scraper.py --agent "Get complete info including images" --urls "https://eventstructure.com/Absurd-Reality-Check" --output-dir ./agent_output
        """
    )
    
    parser.add_argument(
        "--agent", "-a",
        type=str,
        metavar="PROMPT",
        help="ä½¿ç”¨ Agent æ¨¡å¼è¿›è¡Œå¼€æ”¾å¼æŸ¥è¯¢"
    )
    
    parser.add_argument(
        "--urls", "-u",
        type=str,
        metavar="URL1,URL2",
        help="Agent æ¨¡å¼ä¸‹æŒ‡å®šçš„ URL åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰"
    )
    
    parser.add_argument(
        "--max-credits",
        type=int,
        default=50,
        help="Agent æ¨¡å¼ä¸‹çš„æœ€å¤§ credits æ¶ˆè€—ï¼ˆé»˜è®¤: 50ï¼‰"
    )
    
    parser.add_argument(
        "--discovery-url", "-d",
        type=str,
        help="[New] ä½¿ç”¨ Scrape+Agent æ¨¡å¼ï¼šå…ˆæ»šåŠ¨å‘ç° URLï¼Œå†ç”¨ Agent æå–"
    )
    
    parser.add_argument(
        "--scroll-mode",
        choices=["auto", "horizontal", "vertical"],
        default="auto",
        help="Discovery æ¨¡å¼ä¸‹çš„æ»šåŠ¨ç­–ç•¥ (default: auto)"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        type=str,
        metavar="DIR",
        help="Agent æ¨¡å¼ä¸‹çš„è¾“å‡ºç›®å½•ï¼ˆå°†ä¸‹è½½å›¾ç‰‡å¹¶ç”Ÿæˆ Markdown æŠ¥å‘Šï¼‰"
    )
    
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æŠ“å–"
    )
    
    args = parser.parse_args()
    
    scraper = AaajiaoScraper(use_cache=not args.no_cache)
    
    if args.discovery_url:
        # ====================== Discovery Mode ======================
        logger.info(f"ğŸš€ å¯åŠ¨æ··åˆæ¨¡å¼ (Scrape Discovery -> Agent Extraction) [Scroll: {args.scroll_mode}]")
        
        # Phase 1: Discovery
        found_urls = scraper.discover_urls_with_scroll(args.discovery_url, scroll_mode=args.scroll_mode)
        
        if not found_urls:
            logger.error("âŒ æœªå‘ç°ä»»ä½•é“¾æ¥ï¼Œé€€å‡º")
            sys.exit(1)
            
        logger.info(f"ğŸ“‹ å…±å‘ç° {len(found_urls)} ä¸ªä½œå“é“¾æ¥")
        
        # é™åˆ¶æ•°é‡ç”¨äºæµ‹è¯• (å¯é€‰ï¼Œè¿™é‡Œå…ˆå¤„ç†å‰ 5 ä¸ªé¿å…æ¶ˆè€—è¿‡å¤š)
        # found_urls = found_urls[:5] 
        # logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ 5 ä¸ªé“¾æ¥")
        
        # Phase 2: Agent Extraction
        prompt = args.agent or "Deeply analyze these artworks. Extract title, year, materials, description, concept, and exhibition history."
        # Enhanced Prompt logic
        final_prompt = prompt
        if args.output_dir and "image" not in prompt.lower():
            final_prompt = f"{prompt}. IMPORTANT: For images, extract the 'src_o' attribute (if available) or 'src'. 'src_o' contains the high-res version. Ignore sidebar thumbnails. for each artwork."
        
        logger.info("ğŸ¤– æäº¤ Agent æ‰¹é‡å¤„ç†ä»»åŠ¡ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
        
        # ä¼ é€’å‘ç°çš„æ‰€æœ‰ URL ç»™ Agent
        # æ³¨æ„ï¼šURL å¤ªå¤šå¯èƒ½ä¼šå¯¼è‡´ Agent ä»»åŠ¡è¿‡å¤§ï¼ŒFirecrawl å»ºè®®ä¸€æ¬¡å¤„ç†å°‘é‡ URL
        # è¿™é‡Œæ¼”ç¤ºåŸç†ï¼Œå®é™…ä½¿ç”¨å¯èƒ½éœ€è¦åˆ‡ç‰‡åˆ†æ‰¹å¤„ç†
        
        result = scraper.agent_search(enhanced_prompt, urls=found_urls, max_credits=args.max_credits)
        
        if result:
            print("\n" + "="*50)
            print("ğŸ“‹ Discovery + Agent ç»“æœ:")
            print("="*50)
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            if args.output_dir:
                scraper.generate_agent_report(result, args.output_dir, prompt=enhanced_prompt)
        else:
            print("âŒ Agent ä»»åŠ¡å¤±è´¥")
            sys.exit(1)
            
    elif args.agent:
        # ====================== Standard Agent Mode ======================
        # Agent æ¨¡å¼ - å¢å¼º prompt ä»¥è¯·æ±‚å›¾ç‰‡
        enhanced_prompt = args.agent
        if args.output_dir:
            # è‡ªåŠ¨æ·»åŠ å›¾ç‰‡è¯·æ±‚åˆ° prompt
            if "image" not in args.agent.lower():
                enhanced_prompt = f"{args.agent}. Also extract all image URLs from the page."
        
        urls = args.urls.split(",") if args.urls else None
        result = scraper.agent_search(enhanced_prompt, urls=urls, max_credits=args.max_credits)
        
        if result:
            print("\n" + "="*50)
            print("ğŸ“‹ Agent ç»“æœ:")
            print("="*50)
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š
            if args.output_dir:
                scraper.generate_agent_report(result, args.output_dir, prompt=enhanced_prompt)
        else:
            print("âŒ Agent æŸ¥è¯¢å¤±è´¥")
            sys.exit(1)
    else:
        # ====================== Standard Scrape Mode ======================
        # é»˜è®¤æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰ä½œå“
        scraper.scrape_all()
        scraper.save_to_json()
        scraper.generate_markdown()


if __name__ == "__main__":
    main()
"""
Report generation mixin for the aaajiao scraper.

This module provides functionality to generate various output formats:
- JSON: Save scraped artwork data to structured JSON
- Markdown: Generate human-readable portfolio documents
- Agent reports: Create detailed reports with downloaded images

Supports both basic scraper output and AI extraction results.
"""

import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List

import requests

from .constants import BASE_URL

logger = logging.getLogger(__name__)


class ReportMixin:
    """Mixin providing report generation functionality.
    
    This mixin adds methods to export scraped data in various formats.
    It handles image downloading, local path mapping, and structured
    report generation.
    
    Attributes:
        works: List of scraped artwork data from CoreScraper.
    """

    def save_to_json(self, filename: str = "aaajiao_works.json") -> None:
        """Save artwork data to JSON file.
        
        Args:
            filename: Output JSON filename. Defaults to 'aaajiao_works.json'.
                Relative paths are resolved from current directory.
        
        Note:
            Outputs UTF-8 encoded JSON with 2-space indentation.
            Uses ensure_ascii=False to preserve Chinese characters.
            
        Example:
            >>> scraper = AaajiaoScraper()
            >>> # After scraping...
            >>> scraper.save_to_json("output/works.json")
        """
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.works, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON data saved: {filename} ({len(self.works)} works)")

    def generate_markdown(self, filename: str = "aaajiao_portfolio.md") -> None:
        """Generate Markdown format portfolio document for basic scraper.
        
        Creates a human-readable Markdown file organized by year with
        artwork details, descriptions, and metadata.
        
        Args:
            filename: Output Markdown filename. Defaults to 'aaajiao_portfolio.md'.
                Relative paths are resolved from current directory.
        
        Note:
            - Artworks are sorted by year in descending order
            - Supports bilingual titles (English/Chinese)
            - Includes images and videos when available
            
        Example:
            >>> scraper = AaajiaoScraper()
            >>> scraper.get_all_work_links()
            >>> # After scraping...
            >>> scraper.generate_markdown("portfolio.md")
        """
        lines = [
            "# aaajiao ä½œå“é›† / aaajiao Portfolio\n",
            f"Source: {BASE_URL}\n",
            "Generated by aaajiao Scraper v3 (Firecrawl Edition)\n",
            "\n---\n\n",
        ]

        # Sort by year in descending order
        sorted_works = sorted(self.works, key=lambda x: x.get("year") or "0000", reverse=True)

        current_year = None
        for work in sorted_works:
            year = work.get("year", "Unknown")
            if year != current_year:
                lines.append(f"## {year}\n\n")
                current_year = year

            title = work.get("title", "Untitled")
            title_cn = work.get("title_cn", "")

            header = f"### [{title}]({work['url']})"
            if title_cn:
                header += f" / {title_cn}"
            lines.append(header + "\n\n")

            if work.get("type"):
                lines.append(f"**Type**: {work['type']}\n\n")
            if work.get("materials"):
                lines.append(f"**Materials**: {work['materials']}\n\n")
            if work.get("video_link"):
                lines.append(f"**Video**: {work['video_link']}\n\n")

            if work.get("description_cn"):
                lines.append(f"> {work['description_cn']}\n\n")

            if work.get("description_en"):
                lines.append(f"{work['description_en']}\n\n")

            lines.append("---\n")

        with open(filename, "w", encoding="utf-8") as f:
            f.write("".join(lines))

        logger.info(f"Markdown file generated: {filename}")

    def generate_agent_report(
        self,
        data: Dict[str, Any],
        output_dir: str,
        prompt: str = "",
        extraction_level: str = "custom",
    ) -> str:
        """Generate detailed report from AI extraction results with image downloads.
        
        Creates a comprehensive Markdown report with:
        - Timestamped output directory
        - Downloaded high-resolution images
        - Bilingual descriptions
        - Structured metadata table
        - Original JSON data export
        
        Args:
            data: Extraction result dictionary. Should contain 'data' key with
                list of artwork dicts, or be a single artwork dict.
            output_dir: Output directory path for report and images.
                Created if doesn't exist.
            prompt: The extraction prompt used (for metadata). Defaults to "".
            extraction_level: Extraction mode (quick/full/custom). Defaults to "custom".
        
        Returns:
            Absolute path to the generated Markdown report file.
            
        Note:
            - Downloads all high-resolution images to timestamped subdirectory
            - Maps remote URLs to local paths in Markdown
            - Preserves original data in separate JSON file
            - Limits to 6 images per artwork in report
            
        Example:
            >>> result = scraper.agent_search(prompt, urls)
            >>> report_path = scraper.generate_agent_report(
            ...     result,
            ...     "output/extraction_20250121",
            ...     prompt="Extract artwork details",
            ...     extraction_level="full"
            ... )
        """
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        logger.info(f"ğŸ“ Generating report to: {output_dir}")

        # Parse data list
        data_list = data.get("data", [data]) if isinstance(data, dict) else [data]
        if not isinstance(data_list, list):
            data_list = [data_list]

        # 1. Create image directory and download all images, build URL -> local path mapping
        images_dir = f"images_{timestamp}"
        images_path = os.path.join(output_dir, images_dir)
        os.makedirs(images_path, exist_ok=True)

        url_to_local: Dict[str, str] = {}  # URL -> relative path mapping
        img_counter = 0

        for item in data_list:
            if not isinstance(item, dict):
                continue
            images = item.get("high_res_images") or item.get("images") or []
            for img_url in images:
                if img_url in url_to_local:
                    continue  # Already downloaded
                img_counter += 1
                try:
                    # Extract file extension
                    ext = os.path.splitext(img_url.split("?")[0])[-1] or ".jpg"
                    if not ext.startswith("."):
                        ext = ".jpg"
                    local_filename = f"{img_counter:02d}{ext}"
                    local_path = os.path.join(images_path, local_filename)

                    resp = requests.get(img_url, timeout=30)
                    if resp.status_code == 200:
                        with open(local_path, "wb") as f:
                            f.write(resp.content)
                        url_to_local[img_url] = f"{images_dir}/{local_filename}"
                        logger.info(f"ğŸ“¥ Downloaded image [{img_counter}]: {local_filename}")
                except Exception as e:
                    logger.warning(f"Image download failed: {img_url[:50]}... - {e}")

        logger.info(f"âœ… Successfully downloaded {len(url_to_local)} images")

        # 2. Generate Markdown report
        report_filename = f"report_{timestamp}.md"
        report_path = os.path.join(output_dir, report_filename)

        lines: List[str] = []

        # Report header
        lines.append("# ä½œå“æå–æŠ¥å‘Š\n\n")
        lines.append(f"> **æå–æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        lines.append(f"> **æå–æ¨¡å¼:** {extraction_level.upper()}\n")
        lines.append(f"> **ä½œå“æ•°é‡:** {len(data_list)}\n")
        lines.append("\n---\n\n")

        # One section per artwork
        for i, item in enumerate(data_list, 1):
            if not isinstance(item, dict):
                continue

            title = item.get("title", f"ä½œå“ {i}")
            title_cn = item.get("title_cn", "")
            year = item.get("year", "")

            if title_cn and title_cn != title:
                lines.append(f"## {i}. {title} / {title_cn}\n\n")
            else:
                lines.append(f"## {i}. {title}\n\n")

            # Metadata table
            if year:
                lines.append(f"| å¹´ä»½ | {year} |\n")
            if item.get("category") or item.get("type"):
                lines.append(f"| ç±»å‹ | {item.get('category') or item.get('type')} |\n")
            if item.get("video_link"):
                lines.append(f"| è§†é¢‘ | [{item['video_link']}]({item['video_link']}) |\n")
            if item.get("materials"):
                lines.append(f"| ææ–™ | {item['materials']} |\n")
            lines.append("\n")

            # Descriptions
            desc_en = item.get("description_en") or item.get("description", "")
            desc_cn = item.get("description_cn", "")

            if desc_en or desc_cn:
                lines.append("### Description / æè¿°\n\n")
                if desc_en:
                    lines.append(f"**English:**\n\n{desc_en}\n\n")
                if desc_cn:
                    lines.append(f"**ä¸­æ–‡:**\n\n{desc_cn}\n\n")

            # Images (use local relative paths)
            images = item.get("high_res_images") or item.get("images") or []
            if images:
                lines.append("### å›¾ç‰‡\n\n")
                for img_url in images[:6]:  # Limit to 6 images
                    local_rel_path = url_to_local.get(img_url)
                    if local_rel_path:
                        lines.append(f"![]({local_rel_path})\n\n")
                    else:
                        lines.append(f"![]({img_url})\n\n")  # fallback to URL

            lines.append("---\n\n")

        # Write to file
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("".join(lines))

        logger.info(f"ğŸ“„ Markdown report generated: {report_path}")

        # Also save original JSON
        json_filename = f"data_{timestamp}.json"
        json_path = os.path.join(output_dir, json_filename)

        output_data = {
            "_meta": {
                "prompt": prompt,
                "extraction_level": extraction_level,
                "timestamp": datetime.now().isoformat(),
            },
            "data": data_list,
            # Pass through stats if available in data
            "cached_count": data.get("cached_count", 0) if isinstance(data, dict) else 0,
            "new_count": data.get("new_count", 0) if isinstance(data, dict) else 0,
        }

        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ JSON data saved: {json_path}")

        return report_path


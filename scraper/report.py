"""
Report generation mixin for the aaajiao scraper.

This module provides functionality to generate various output formats:
- JSON: Save scraped artwork data to structured JSON
- Markdown: Generate human-readable portfolio documents
- Agent reports: Create detailed reports with downloaded images

Supports both basic scraper output and AI extraction results.
"""

import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List

import requests

from .constants import BASE_URL

logger = logging.getLogger(__name__)


class ReportMixin:
    """Mixin providing report generation functionality.
    
    This mixin adds methods to export scraped data in various formats.
    It handles image downloading, local path mapping, and structured
    report generation.
    
    Attributes:
        works: List of scraped artwork data from CoreScraper.
    """

    def save_to_json(self, filename: str = "aaajiao_works.json") -> None:
        """Save artwork data to JSON file.
        
        Args:
            filename: Output JSON filename. Defaults to 'aaajiao_works.json'.
                Relative paths are resolved from current directory.
        
        Note:
            Outputs UTF-8 encoded JSON with 2-space indentation.
            Uses ensure_ascii=False to preserve Chinese characters.
            
        Example:
            >>> scraper = AaajiaoScraper()
            >>> # After scraping...
            >>> scraper.save_to_json("output/works.json")
        """
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(self.works, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON data saved: {filename} ({len(self.works)} works)")

    def generate_markdown(self, filename: str = "aaajiao_portfolio.md") -> None:
        """Generate Markdown format portfolio document for basic scraper.
        
        Creates a human-readable Markdown file organized by year with
        artwork details, descriptions, and metadata.
        
        Args:
            filename: Output Markdown filename. Defaults to 'aaajiao_portfolio.md'.
                Relative paths are resolved from current directory.
        
        Note:
            - Artworks are sorted by year in descending order
            - Supports bilingual titles (English/Chinese)
            - Includes images and videos when available
            
        Example:
            >>> scraper = AaajiaoScraper()
            >>> scraper.get_all_work_links()
            >>> # After scraping...
            >>> scraper.generate_markdown("portfolio.md")
        """
        lines = [
            "# aaajiao ä½œå“é›† / aaajiao Portfolio\n",
            f"Source: {BASE_URL}\n",
            "Generated by aaajiao Scraper v3 (Firecrawl Edition)\n",
            "\n---\n\n",
        ]

        # Sort by year in descending order (newest first)
        def get_sort_year(work):
            year = work.get("year") or "0000"
            # For year ranges like "2018-2022", use the end year for sorting
            if "-" in year:
                parts = year.split("-")
                return parts[-1].strip()  # Use end year (most recent)
            return year
        
        sorted_works = sorted(self.works, key=get_sort_year, reverse=True)

        current_year = None
        for work in sorted_works:
            year = work.get("year", "Unknown")
            if year != current_year:
                lines.append(f"## {year}\n\n")
                current_year = year

            title = work.get("title", "Untitled")
            title_cn = work.get("title_cn", "")

            header = f"### [{title}]({work['url']})"
            if title_cn:
                header += f" / {title_cn}"
            lines.append(header + "\n\n")

            if work.get("type"):
                lines.append(f"**Type**: {work['type']}\n\n")
            if work.get("materials"):
                lines.append(f"**Materials**: {work['materials']}\n\n")
            if work.get("size"):
                lines.append(f"**Size**: {work['size']}\n\n")
            if work.get("duration"):
                lines.append(f"**Duration**: {work['duration']}\n\n")
            if work.get("video_link"):
                lines.append(f"**Video**: {work['video_link']}\n\n")

            if work.get("description_cn"):
                lines.append(f"> {work['description_cn']}\n\n")

            if work.get("description_en"):
                lines.append(f"{work['description_en']}\n\n")

            lines.append("---\n")

        with open(filename, "w", encoding="utf-8") as f:
            f.write("".join(lines))

        logger.info(f"Markdown file generated: {filename}")

    def generate_agent_report(
        self,
        data: Dict[str, Any],
        output_dir: str,
        prompt: str = "",
        extraction_level: str = "custom",
        output_mode: str = "merged",  # "merged" | "split"
    ) -> str:
        """Generate detailed report from AI extraction results with image downloads.
        
        Creates a comprehensive Markdown report with:
        - Timestamped output directory
        - Downloaded high-resolution images
        - Bilingual descriptions
        - Structured metadata table
        - Original JSON data export
        
        Args:
            data: Extraction result dictionary. Should contain 'data' key with
                list of artwork dicts, or be a single artwork dict.
            output_dir: Output directory path for report and images.
                Created if doesn't exist.
            prompt: The extraction prompt used (for metadata). Defaults to "".
            extraction_level: Extraction mode (quick/full/custom). Defaults to "custom".
            output_mode: Output format - 'merged' (one combined MD) or 'split' 
                (one MD per artwork). Defaults to "merged".
        
        Returns:
            Absolute path to the generated Markdown report file (or directory for split mode).
            
        Note:
            - Downloads all high-resolution images to timestamped subdirectory
            - Maps remote URLs to local paths in Markdown
            - Preserves original data in separate JSON file
            - Limits to 6 images per artwork in report
            
        Example:
            >>> result = scraper.agent_search(prompt, urls)
            >>> report_path = scraper.generate_agent_report(
            ...     result,
            ...     "output/extraction_20250121",
            ...     prompt="Extract artwork details",
            ...     extraction_level="full",
            ...     output_mode="split"
            ... )
        """
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        logger.info(f"ğŸ“ Generating report to: {output_dir} (mode: {output_mode})")

        # Parse data list
        data_list = data.get("data", [data]) if isinstance(data, dict) else [data]
        if not isinstance(data_list, list):
            data_list = [data_list]

        # === SPLIT MODE: One MD file per artwork ===
        if output_mode == "split":
            reports_generated = []
            
            for i, item in enumerate(data_list, 1):
                if not isinstance(item, dict):
                    continue
                
                title = item.get("title", f"artwork_{i}")
                # Sanitize title for filename
                safe_title = "".join(c if c.isalnum() or c in "-_ " else "_" for c in title)[:50]
                item_dir = os.path.join(output_dir, f"{i:02d}_{safe_title}")
                os.makedirs(item_dir, exist_ok=True)
                
                # Download images for this item
                images_dir = "images"
                images_path = os.path.join(item_dir, images_dir)
                os.makedirs(images_path, exist_ok=True)
                
                url_to_local: Dict[str, str] = {}
                images = item.get("high_res_images") or item.get("images") or []
                for j, img_url in enumerate(images, 1):
                    if img_url in url_to_local:
                        continue
                    try:
                        ext = os.path.splitext(img_url.split("?")[0])[-1] or ".jpg"
                        if not ext.startswith("."):
                            ext = ".jpg"
                        local_filename = f"{j:02d}{ext}"
                        local_path = os.path.join(images_path, local_filename)
                        
                        resp = requests.get(img_url, timeout=30)
                        if resp.status_code == 200:
                            with open(local_path, "wb") as f:
                                f.write(resp.content)
                            url_to_local[img_url] = f"{images_dir}/{local_filename}"
                            logger.info(f"ğŸ“¥ [{title[:20]}...] Downloaded: {local_filename}")
                    except Exception as e:
                        logger.warning(f"Image download failed: {img_url[:50]}... - {e}")
                
                # Generate individual MD file
                report_path = os.path.join(item_dir, "report.md")
                lines = self._generate_single_item_report(item, i, url_to_local, extraction_level)
                
                with open(report_path, "w", encoding="utf-8") as f:
                    f.write("".join(lines))
                
                # Save item JSON
                json_path = os.path.join(item_dir, "data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(item, f, ensure_ascii=False, indent=2)
                
                reports_generated.append(report_path)
                logger.info(f"ğŸ“„ Generated: {report_path}")
            
            # Create index file
            index_path = os.path.join(output_dir, f"index_{timestamp}.md")
            index_lines = [
                "# Artwork Extraction Report / ä½œå“æå–æŠ¥å‘Š (Split Mode)\n\n",
                f"> **Extracted At / æå–æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
                f"> **Mode / æå–æ¨¡å¼:** {extraction_level.upper()}\n",
                f"> **Total / ä½œå“æ•°é‡:** {len(data_list)}\n\n",
                "## Contents / ç›®å½•\n\n",
            ]
            for i, item in enumerate(data_list, 1):
                if isinstance(item, dict):
                    title = item.get("title", f"ä½œå“ {i}")
                    safe_title = "".join(c if c.isalnum() or c in "-_ " else "_" for c in title)[:50]
                    index_lines.append(f"{i}. [{title}](./{i:02d}_{safe_title}/report.md)\n")
            
            with open(index_path, "w", encoding="utf-8") as f:
                f.write("".join(index_lines))
            
            logger.info(f"âœ… Split mode complete: {len(reports_generated)} files generated")
            return index_path

        # === MERGED MODE: One combined MD file (original behavior) ===
        # 1. Create image directory and download all images
        images_dir = f"images_{timestamp}"
        images_path = os.path.join(output_dir, images_dir)
        os.makedirs(images_path, exist_ok=True)

        url_to_local: Dict[str, str] = {}
        img_counter = 0

        for item in data_list:
            if not isinstance(item, dict):
                continue
            images = item.get("high_res_images") or item.get("images") or []
            for img_url in images:
                if img_url in url_to_local:
                    continue
                img_counter += 1
                try:
                    ext = os.path.splitext(img_url.split("?")[0])[-1] or ".jpg"
                    if not ext.startswith("."):
                        ext = ".jpg"
                    local_filename = f"{img_counter:02d}{ext}"
                    local_path = os.path.join(images_path, local_filename)

                    resp = requests.get(img_url, timeout=30)
                    if resp.status_code == 200:
                        with open(local_path, "wb") as f:
                            f.write(resp.content)
                        url_to_local[img_url] = f"{images_dir}/{local_filename}"
                        logger.info(f"ğŸ“¥ Downloaded image [{img_counter}]: {local_filename}")
                except Exception as e:
                    logger.warning(f"Image download failed: {img_url[:50]}... - {e}")

        logger.info(f"âœ… Successfully downloaded {len(url_to_local)} images")

        # 2. Generate Markdown report
        report_filename = f"report_{timestamp}.md"
        report_path = os.path.join(output_dir, report_filename)

        lines: List[str] = []

        # Report header
        lines.append("# Artwork Extraction Report / ä½œå“æå–æŠ¥å‘Š\n\n")
        lines.append(f"> **Extracted At / æå–æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        lines.append(f"> **Mode / æå–æ¨¡å¼:** {extraction_level.upper()}\n")
        lines.append(f"> **Total / ä½œå“æ•°é‡:** {len(data_list)}\n")
        lines.append("\n---\n\n")

        # One section per artwork
        for i, item in enumerate(data_list, 1):
            if not isinstance(item, dict):
                continue
            lines.extend(self._generate_single_item_report(item, i, url_to_local, extraction_level))

        # Write to file
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("".join(lines))

        logger.info(f"ğŸ“„ Markdown report generated: {report_path}")

        # Also save original JSON
        json_filename = f"data_{timestamp}.json"
        json_path = os.path.join(output_dir, json_filename)

        output_data = {
            "_meta": {
                "prompt": prompt,
                "extraction_level": extraction_level,
                "output_mode": output_mode,
                "timestamp": datetime.now().isoformat(),
            },
            "data": data_list,
            "cached_count": data.get("cached_count", 0) if isinstance(data, dict) else 0,
            "new_count": data.get("new_count", 0) if isinstance(data, dict) else 0,
        }

        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ JSON data saved: {json_path}")

        return report_path

    def _generate_single_item_report(
        self, 
        item: Dict[str, Any], 
        index: int, 
        url_to_local: Dict[str, str],
        extraction_level: str
    ) -> List[str]:
        """Generate report lines for a single artwork item.
        
        Args:
            item: Artwork data dictionary.
            index: Item index (1-based).
            url_to_local: Mapping of remote image URLs to local paths.
            extraction_level: The extraction mode used.
            
        Returns:
            List of Markdown lines for this item.
        """
        lines: List[str] = []
        
        title = item.get("title", f"ä½œå“ {index}")
        title_cn = item.get("title_cn", "")
        year = item.get("year", "")

        # Check for error items
        if item.get("error"):
            lines.append(f"## {index}. âŒ Extraction Failed / æå–å¤±è´¥: {title}\n\n")
            lines.append(f"> **Error / é”™è¯¯:** {item.get('error')}\n")
            lines.append(f"> **URL / é“¾æ¥:** [{item.get('url')}]({item.get('url')})\n\n")
            lines.append("---\n\n")
            return lines

        if title_cn and title_cn != title:
            lines.append(f"## {index}. {title} / {title_cn}\n\n")
        else:
            lines.append(f"## {index}. {title}\n\n")

        # Metadata table
        if year:
            lines.append(f"| Year / å¹´ä»½ | {year} |\n")
        if item.get("category") or item.get("type"):
            lines.append(f"| Type / ç±»å‹ | {item.get('category') or item.get('type')} |\n")
        if item.get("video_link"):
            lines.append(f"| Video / è§†é¢‘ | [{item['video_link']}]({item['video_link']}) |\n")
        if item.get("materials"):
            lines.append(f"| Materials / ææ–™ | {item['materials']} |\n")
        if item.get("size"):
            lines.append(f"| Size / å°ºå¯¸ | {item['size']} |\n")
        if item.get("duration"):
            lines.append(f"| Duration / æ—¶é•¿ | {item['duration']} |\n")
        lines.append("\n")

        # Descriptions
        desc_en = item.get("description_en") or item.get("description", "")
        desc_cn = item.get("description_cn", "")

        if desc_en or desc_cn:
            lines.append("### Description / æè¿°\n\n")
            if desc_en:
                lines.append(f"**English:**\n\n{desc_en}\n\n")
            if desc_cn:
                lines.append(f"**ä¸­æ–‡:**\n\n{desc_cn}\n\n")

        # Images (use local relative paths)
        images = item.get("high_res_images") or item.get("images") or []
        if images:
            lines.append("### Images / å›¾ç‰‡\n\n")
            for img_url in images[:6]:  # Limit to 6 images
                local_rel_path = url_to_local.get(img_url)
                if local_rel_path:
                    if not local_rel_path.startswith("./"):
                        local_rel_path = f"./{local_rel_path}"
                    lines.append(f"![]({local_rel_path})\n\n")
                else:
                    lines.append(f"![]({img_url})\n\n")

        lines.append("---\n\n")
        return lines


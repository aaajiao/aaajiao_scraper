
import logging
from bs4 import BeautifulSoup
from typing import List, Dict

from .constants import BASE_URL, SITEMAP_URL

logger = logging.getLogger(__name__)

class BasicScraperMixin:
    """Basic extraction functionality via Sitemap & HTML"""
    
    def get_all_work_links(self, incremental: bool = False) -> List[str]:
        """
        ä» Sitemap è·å–æ‰€æœ‰ä½œå“é“¾æ¥
        """
        logger.info(f"æ­£åœ¨è¯»å– Sitemap: {SITEMAP_URL}")
        try:
            response = self.session.get(SITEMAP_URL, timeout=self.TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è§£æ URL å’Œ lastmod
            current_sitemap = {}  # {url: lastmod}
            raw_urls = soup.find_all('url')
            logger.info(f"Sitemap raw url tags found: {len(raw_urls)}")
            
            for url_tag in raw_urls:
                loc = url_tag.find('loc')
                lastmod = url_tag.find('lastmod')
                if loc:
                    url = loc.get_text().strip()
                    if self._is_valid_work_link(url):
                        current_sitemap[url] = lastmod.get_text().strip() if lastmod else ""

            logger.info(f"Sitemap ä¸­æ‰¾åˆ° {len(current_sitemap)} ä¸ªæœ‰æ•ˆä½œå“é“¾æ¥ (Filtered from {len(raw_urls)})")
            
            if not incremental:
                # å…¨é‡æ¨¡å¼ï¼šä¿å­˜ç¼“å­˜åè¿”å›æ‰€æœ‰é“¾æ¥
                self._save_sitemap_cache(current_sitemap)
                return sorted(list(current_sitemap.keys()))
            
            # å¢é‡æ¨¡å¼ï¼šæ¯”è¾ƒç¼“å­˜
            cached_sitemap = self._load_sitemap_cache()
            changed_urls = []
            
            for url, lastmod in current_sitemap.items():
                if url not in cached_sitemap:
                    # æ–°å¢ URL
                    changed_urls.append(url)
                    logger.info(f"ğŸ†• æ–°å¢: {url}")
                elif lastmod and lastmod != cached_sitemap.get(url, ""):
                    # lastmod å˜åŒ–
                    changed_urls.append(url)
                    logger.info(f"ğŸ”„ æ›´æ–°: {url} ({cached_sitemap.get(url)} â†’ {lastmod})")
            
            if changed_urls:
                logger.info(f"ğŸ“Š å¢é‡æ£€æµ‹: {len(changed_urls)} ä¸ªæ›´æ–°/æ–°å¢")
            else:
                logger.info("âœ… æ²¡æœ‰æ£€æµ‹åˆ°æ›´æ–°")
                
            # ä¿å­˜æ–°ç¼“å­˜
            self._save_sitemap_cache(current_sitemap)
            
            return sorted(changed_urls)
            
        except Exception as e:
            logger.error(f"Sitemap è¯»å–å¤±è´¥: {e}")
            return self._fallback_scan_main_page()
            
    def _fallback_scan_main_page(self) -> List[str]:
        """å¤‡ç”¨æ–¹æ¡ˆï¼šä»ä¸»é¡µæ‰«æé“¾æ¥"""
        logger.info("å°è¯•æ‰«æä¸»é¡µé“¾æ¥ (å¤‡ç”¨æ–¹æ¡ˆ)...")
        try:
            resp = self.session.get(BASE_URL, timeout=self.TIMEOUT)
            soup = BeautifulSoup(resp.content, 'html.parser')
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = href if href.startswith('http') else f"{BASE_URL.rstrip('/')}/{href.lstrip('/')}"
                if self._is_valid_work_link(full_url):
                    links.append(full_url)
            return sorted(list(set(links)))
        except Exception as e:
            logger.error(f"ä¸»é¡µæ‰«æå¤±è´¥: {e}")
            return []

    def _is_valid_work_link(self, url: str) -> bool:
        """è¿‡æ»¤éä½œå“é“¾æ¥"""
        if not url.startswith(BASE_URL):
            return False
            
        path = url.replace(BASE_URL, '')
        
        # æ’é™¤åˆ—è¡¨
        excludes = [
            '/', '/rss', '/feed', '/filter', '/aaajiao', 
            '/contact', '/cv', '/about', '/index', '/sitemap'
        ]
        
        if path in ['/', '']: return False
        
        for ex in excludes:
            if ex in path and len(path) < 20: # simple heuristic
                if path == ex or path.startswith(ex + '/'):
                    return False
        
        # æ’é™¤åŒ…å« 'tag' çš„é“¾æ¥
        if '/tag/' in path:
            return False
            
        return True

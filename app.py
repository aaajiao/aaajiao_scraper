"""
aaajiao ä½œå“é›†æŠ“å–å·¥å…· - Streamlit GUI

ç®€åŒ–çš„å•é¡µç•Œé¢ï¼Œç”¨äºä» eventstructure.com æŠ“å–ä½œå“æ•°æ®ã€‚
åŠŸèƒ½ï¼š
- ä¸€é”®æŠ“å–ï¼Œä¸¤å±‚æ··åˆæå–ç­–ç•¥ (v6.3.0)
- è‡ªåŠ¨è¿‡æ»¤å±•è§ˆå’Œç”»å†Œ
- å›¾ç‰‡æ•´åˆå·¥å…·
"""

import json
import os
import time

import pandas as pd
import streamlit as st

from scraper import AaajiaoScraper, deduplicate_works, is_artwork

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="aaajiao æŠ“å–å·¥å…·",
    page_icon="ğŸ¨",
    layout="wide"
)

# æ ‡é¢˜
st.title("ğŸ¨ aaajiao ä½œå“é›†æŠ“å–å·¥å…·")
st.markdown("è‡ªåŠ¨ä» eventstructure.com æŠ“å–ä½œå“è¯¦æƒ…")


# ============ è¾…åŠ©å‡½æ•° ============

def load_existing_works() -> list:
    """ä» JSON æ–‡ä»¶åŠ è½½å·²æœ‰ä½œå“ã€‚"""
    try:
        with open("aaajiao_works.json", "r", encoding="utf-8") as f:
            works = json.load(f)
            # è¿‡æ»¤æ‰å±•è§ˆï¼ˆä»¥é˜²æ—§æ•°æ®åŒ…å«ï¼‰
            return [w for w in works if is_artwork(w)]
    except FileNotFoundError:
        return []


def get_stats(works: list) -> dict:
    """è®¡ç®—ä½œå“ç»Ÿè®¡æ•°æ®ã€‚"""
    total = len(works)
    has_size = sum(1 for w in works if w.get('size'))
    has_duration = sum(1 for w in works if w.get('duration'))
    has_year = sum(1 for w in works if w.get('year'))

    return {
        "total": total,
        "has_size": has_size,
        "has_duration": has_duration,
        "has_year": has_year,
        "size_pct": (has_size / total * 100) if total > 0 else 0,
    }


def normalize_url(url: str) -> str:
    """è§„èŒƒåŒ– URL ç”¨äºåŒ¹é…ã€‚"""
    if not url:
        return ""
    url = url.strip().rstrip("/")
    return url


def merge_work_with_full_data(work: dict, full_works: list) -> dict:
    """å°†ä½œå“æ•°æ®ä¸ aaajiao_works.json ä¸­çš„å®Œæ•´å…ƒæ•°æ®åˆå¹¶ã€‚

    ä½¿ç”¨ URL ä½œä¸ºåŒ¹é…é”®ï¼Œä¿ç•™ç¼“å­˜ä¸­çš„å›¾ç‰‡æ•°æ®ï¼Œè¡¥å…… JSON ä¸­çš„å…ƒæ•°æ®ã€‚
    """
    url = normalize_url(work.get("url", ""))
    if not url:
        return work

    for full_work in full_works:
        if normalize_url(full_work.get("url", "")) == url:
            merged = work.copy()
            metadata_fields = [
                'type', 'materials', 'size', 'duration',
                'description_en', 'description_cn', 'video_link', 'tags',
                'title', 'title_cn', 'year'
            ]
            for key in metadata_fields:
                if full_work.get(key):
                    merged[key] = full_work[key]
            if len(full_work.get("images", [])) > len(merged.get("images", [])):
                merged["images"] = full_work["images"]
                merged["high_res_images"] = full_work.get("high_res_images", full_work["images"])
            return merged

    return work


def generate_rich_work_markdown(work: dict, include_local_images: bool = False) -> str:
    """ç”ŸæˆåŒ…å«å®Œæ•´å…ƒæ•°æ®çš„ä½œå“ Markdownã€‚

    Args:
        work: ä½œå“å­—å…¸
        include_local_images: True ä½¿ç”¨æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼ŒFalse ä½¿ç”¨ç½‘ç»œ URL
    """
    lines = []

    title = work.get("title", "æ— æ ‡é¢˜")
    title_cn = work.get("title_cn", "")
    year = work.get("year", "")

    if title_cn and title_cn != title:
        lines.append(f"## {title} / {title_cn}\n\n")
    else:
        lines.append(f"## {title}\n\n")

    if year:
        lines.append(f"**Year**: {year}\n\n")
    if work.get("type"):
        lines.append(f"**Type**: {work['type']}\n\n")
    if work.get("materials"):
        lines.append(f"**Materials**: {work['materials']}\n\n")
    if work.get("size"):
        lines.append(f"**Size**: {work['size']}\n\n")
    if work.get("duration"):
        lines.append(f"**Duration**: {work['duration']}\n\n")
    if work.get("video_link"):
        lines.append(f"**Video**: {work['video_link']}\n\n")
    if work.get("url"):
        lines.append(f"**URL**: {work['url']}\n\n")

    if work.get("description_cn"):
        lines.append(f"**ä¸­æ–‡æè¿°**: {work['description_cn']}\n\n")
    if work.get("description_en"):
        lines.append(f"**Description**: {work['description_en']}\n\n")

    if include_local_images and work.get("local_images"):
        images = work.get("local_images", [])
        if images:
            lines.append("### å›¾ç‰‡\n\n")
            for img_path in images:  # æ˜¾ç¤ºå…¨éƒ¨æœ¬åœ°å›¾ç‰‡
                rel_path = os.path.basename(img_path)
                lines.append(f'<a href="{rel_path}" target="_blank"><img src="{rel_path}" width="400" alt="{title}"></a>\n\n')
    else:
        images = work.get("images", []) or work.get("high_res_images", [])
        if images:
            lines.append("### å›¾ç‰‡\n\n")
            for img in images:  # æ˜¾ç¤ºå…¨éƒ¨å›¾ç‰‡
                lines.append(f'<a href="{img}" target="_blank"><img src="{img}" width="400"></a>\n\n')

    lines.append("---\n\n")
    return "".join(lines)


# ============ åˆå§‹åŒ– Session State ============

if 'works' not in st.session_state:
    st.session_state.works = load_existing_works()
if 'running' not in st.session_state:
    st.session_state.running = False
if 'logs' not in st.session_state:
    st.session_state.logs = []


# ============ ä¸»ç•Œé¢ ============

# --- çŠ¶æ€åŒºåŸŸ ---
st.subheader("ğŸ“¦ å½“å‰çŠ¶æ€")

works = st.session_state.works
stats = get_stats(works)

col1, col2, col3, col4, col5 = st.columns(5)
with col1:
    st.metric("ä½œå“æ€»æ•°", stats["total"])
with col2:
    st.metric("æœ‰å°ºå¯¸", f"{stats['has_size']} ({stats['size_pct']:.0f}%)")
with col3:
    st.metric("æœ‰æ—¶é•¿", stats["has_duration"])
with col4:
    st.metric("æœ‰å¹´ä»½", stats["has_year"])
with col5:
    # æ˜¾ç¤º API credits
    if 'api_credits' not in st.session_state:
        st.session_state.api_credits = None
    if st.button("ğŸ”„", key="refresh_credits", help="åˆ·æ–° API ä½™é¢"):
        try:
            scraper = AaajiaoScraper(use_cache=False)
            st.session_state.api_credits = scraper.get_credit_usage()
        except Exception:
            st.session_state.api_credits = None
    if st.session_state.api_credits:
        credits = st.session_state.api_credits
        remaining = credits.get('remaining_credits', 0)
        total = credits.get('plan_credits', 0)
        st.metric("API Credits", f"{remaining:,}", delta=f"/{total:,}")

st.divider()

# --- ä¸»æ“ä½œåŒºåŸŸ ---
st.subheader("ğŸš€ ä¸€é”®æŠ“å–")
st.markdown("""
**å·¥ä½œæµç¨‹ï¼š** è·å– sitemap â†’ æå–æ•°æ®ï¼ˆä¸¤å±‚æ··åˆç­–ç•¥ï¼‰â†’ è¿‡æ»¤å±•è§ˆ â†’ ä¿å­˜

- **ç¬¬1å±‚ï¼š** æœ¬åœ° BeautifulSoup è§£æï¼ˆ0 creditsï¼‰
- **ç¬¬2å±‚ï¼š** Firecrawl Extract v2ï¼ˆ~5 credits/é¡µï¼Œæ¯” v1 ä¾¿å®œ 10 å€ï¼‰

*v6.3.0 æ–°æ¶æ„ï¼šä½¿ç”¨ Firecrawl v2 APIï¼Œæ™ºèƒ½åˆå¹¶ä¸¤å±‚ç»“æœï¼Œå®Œæ•´åº¦å¯è¾¾ 90%+*
""")

# é«˜çº§é€‰é¡¹ï¼ˆé»˜è®¤æŠ˜å ï¼‰
with st.expander("âš™ï¸ é«˜çº§é€‰é¡¹"):
    col_opt1, col_opt2 = st.columns(2)
    with col_opt1:
        incremental = st.checkbox(
            "å¢é‡æ›´æ–°",
            value=True,
            help="ä»…è·å–æ–°å¢/æ›´æ–°çš„é¡µé¢ï¼ˆåŸºäº sitemap lastmodï¼‰"
        )
    with col_opt2:
        max_workers = st.slider(
            "å¹¶å‘æ•°",
            min_value=1,
            max_value=8,
            value=4,
            help="å¹¶è¡Œæå–çš„å·¥ä½œçº¿ç¨‹æ•°"
        )

# è¿è¡ŒæŒ‰é’®
if st.button(
    "ğŸš€ å¼€å§‹æŠ“å–",
    disabled=st.session_state.running,
    type="primary",
    use_container_width=True
):
    st.session_state.running = True
    st.session_state.logs = []

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_area = st.empty()

    def progress_callback(msg: str, pct: float):
        st.session_state.logs.append(msg)
        progress_bar.progress(pct)
        status_text.text(msg)
        log_area.code("\n".join(st.session_state.logs[-10:]))

    try:
        scraper = AaajiaoScraper()
        result = scraper.run_full_pipeline(
            incremental=incremental,
            max_workers=max_workers,
            progress_callback=progress_callback,
        )

        st.session_state.works = result["works"]
        stats = result["stats"]

        st.success(
            f"âœ… å®Œæˆï¼æå–äº† {stats['extracted']} ä¸ªä½œå“ï¼Œ"
            f"è·³è¿‡äº† {stats['skipped_exhibitions']} ä¸ªå±•è§ˆï¼Œ"
            f"å…±ä¿å­˜ {stats['total']} ä¸ªä½œå“ã€‚"
        )
        st.balloons()

    except Exception as e:
        st.error(f"é”™è¯¯ï¼š{str(e)}")
    finally:
        st.session_state.running = False

st.divider()

# --- è¾“å‡ºåŒºåŸŸ ---
st.subheader("ğŸ“¥ è¾“å‡ºæ–‡ä»¶")

col_dl1, col_dl2 = st.columns(2)

with col_dl1:
    try:
        with open("aaajiao_works.json", "rb") as f:
            st.download_button(
                label="ğŸ“„ ä¸‹è½½ JSON",
                data=f,
                file_name="aaajiao_works.json",
                mime="application/json",
                use_container_width=True
            )
    except FileNotFoundError:
        st.info("JSON æ–‡ä»¶å°šæœªç”Ÿæˆ")

with col_dl2:
    try:
        with open("aaajiao_portfolio.md", "rb") as f:
            st.download_button(
                label="ğŸ“ ä¸‹è½½ Markdown",
                data=f,
                file_name="aaajiao_portfolio.md",
                mime="text/markdown",
                use_container_width=True
            )
    except FileNotFoundError:
        st.info("Markdown æ–‡ä»¶å°šæœªç”Ÿæˆ")

# --- æ•°æ®é¢„è§ˆåŒºåŸŸ ---
with st.expander("ğŸ“‹ æ•°æ®é¢„è§ˆ", expanded=bool(works)):
    if works:
        df = pd.DataFrame(works)

        # === ç±»å‹ç­›é€‰å™¨ ===
        # å½’ä¸€åŒ–ç±»å‹ç”¨äºåˆ†ç»„ï¼ˆå»é™¤å¤§å°å†™ã€ç©ºæ ¼å·®å¼‚ï¼‰
        def normalize_type_for_filter(t: str) -> str:
            if not t:
                return "(ç©º)"
            t = t.lower().strip()
            # ç®€å•å½’ä¸€åŒ–ï¼šæå–ä¸»è¦ç±»å‹
            if "installation" in t:
                if "video" in t:
                    return "Video Installation"
                elif "sound" in t:
                    return "Sound Installation"
                elif "interactive" in t:
                    return "Interactive Installation"
                return "Installation"
            elif "video" in t:
                return "Video"
            elif "website" in t or "ç½‘ç«™" in t:
                return "Website"
            elif "performance" in t:
                return "Performance"
            elif "sculpture" in t or "é›•å¡‘" in t:
                return "Sculpture"
            elif "print" in t or "printing" in t or "å°åˆ·" in t or "æ‰“å°" in t:
                return "Print"
            elif "software" in t or "app" in t:
                return "Software/App"
            elif "photo" in t:
                return "Photography"
            return t.title()[:30]  # å…¶ä»–ç±»å‹æˆªæ–­

        # åˆ›å»ºå½’ä¸€åŒ–ç±»å‹åˆ—
        df['_normalized_type'] = df['type'].apply(lambda x: normalize_type_for_filter(x or ''))

        # è·å–æ‰€æœ‰å½’ä¸€åŒ–ç±»å‹å¹¶ç»Ÿè®¡
        type_counts = df['_normalized_type'].value_counts().to_dict()
        type_options = ["å…¨éƒ¨"] + [f"{t} ({c})" for t, c in sorted(type_counts.items(), key=lambda x: -x[1])]

        col_filter1, col_filter2 = st.columns([1, 2])
        with col_filter1:
            selected_type_display = st.selectbox("æŒ‰ç±»å‹ç­›é€‰", type_options)

        # è§£æé€‰æ‹©çš„ç±»å‹
        if selected_type_display == "å…¨éƒ¨":
            filtered_df = df
        else:
            selected_type = selected_type_display.rsplit(" (", 1)[0]
            filtered_df = df[df['_normalized_type'] == selected_type]

        # === åˆ—é€‰æ‹©å™¨ ===
        # å®šä¹‰æ‰€æœ‰å¯ç”¨åˆ—åŠå…¶æ˜¾ç¤ºåç§°
        all_columns = {
            'title': 'æ ‡é¢˜',
            'title_cn': 'ä¸­æ–‡æ ‡é¢˜',
            'year': 'å¹´ä»½',
            'type': 'ç±»å‹',
            'materials': 'ææ–™',
            'size': 'å°ºå¯¸',
            'duration': 'æ—¶é•¿',
            'credits': 'è‡´è°¢',
            'description_cn': 'ä¸­æ–‡æè¿°',
            'description_en': 'è‹±æ–‡æè¿°',
            'video_link': 'è§†é¢‘é“¾æ¥',
            'url': 'é“¾æ¥'
        }
        # é»˜è®¤æ˜¾ç¤ºçš„åˆ—
        default_cols = ['title', 'title_cn', 'year', 'type', 'materials', 'size', 'duration']
        available_cols = [c for c in all_columns.keys() if c in df.columns]

        with col_filter2:
            selected_cols = st.multiselect(
                "é€‰æ‹©æ˜¾ç¤ºçš„åˆ—",
                options=available_cols,
                default=[c for c in default_cols if c in available_cols],
                format_func=lambda x: all_columns.get(x, x)
            )

        if selected_cols:
            # è¿‡æ»¤å¹¶é‡å‘½ååˆ—ä¸ºä¸­æ–‡æ˜¾ç¤º
            display_df = filtered_df[selected_cols].copy()
            display_df.columns = [all_columns.get(c, c) for c in selected_cols]
            st.dataframe(display_df.head(100), use_container_width=True)
            st.caption(f"æ˜¾ç¤º {min(100, len(filtered_df))}/{len(filtered_df)} ä¸ªä½œå“ï¼ˆå…± {len(works)} ä¸ªï¼‰")
        else:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€åˆ—")
    else:
        st.info("æš‚æ— æ•°æ®ã€‚ç‚¹å‡»ã€Œå¼€å§‹æŠ“å–ã€å¼€å§‹ã€‚")

st.divider()

# ============ å›¾ç‰‡å·¥å…·åŒºåŸŸ ============

st.subheader("ğŸ–¼ï¸ å›¾ç‰‡å·¥å…·")

# ä¼˜å…ˆä» aaajiao_works.json åŠ è½½ï¼Œå›é€€åˆ°ç¼“å­˜
works_for_images = load_existing_works()
if not works_for_images:
    # å›é€€ï¼šå°è¯•ä» .cache/ è¯»å–
    scraper_preview = AaajiaoScraper()
    works_for_images = scraper_preview.get_all_cached_works()
    works_for_images = [w for w in works_for_images if is_artwork(w)]

if works_for_images:
    st.success(f"ğŸ“¦ æ‰¾åˆ° {len(works_for_images)} ä¸ªä½œå“")

    # --- åŠŸèƒ½ 1ï¼šå›¾ç‰‡æ•´åˆ ---
    with st.expander("ğŸ–¼ï¸ å›¾ç‰‡æ•´åˆï¼ˆä¸‹è½½åˆ°æœ¬åœ°ï¼‰"):
        st.markdown("""
        ä»ç¼“å­˜çš„ä½œå“ä¸­æå–å¹¶ä¸‹è½½å›¾ç‰‡ã€‚
        - ä½¿ç”¨ HTML è§£æï¼ˆæ—  API æˆæœ¬ï¼‰
        - ä¸‹è½½åˆ° `output/images/`
        """)

        col_img1, col_img2 = st.columns(2)
        with col_img1:
            download_images = st.checkbox("ä¸‹è½½å›¾ç‰‡", value=True)
        with col_img2:
            img_limit = st.slider(
                "å¤„ç†ä½œå“æ•°",
                min_value=1,
                max_value=len(works_for_images),
                value=min(50, len(works_for_images))
            )

        merge_full_metadata = st.checkbox(
            "åˆå¹¶å®Œæ•´å…ƒæ•°æ®",
            value=False,
            help="ä» aaajiao_works.json åˆå¹¶å®Œæ•´ä½œå“ä¿¡æ¯ï¼ˆç±»å‹ã€ææ–™ã€å°ºå¯¸ã€æè¿°ç­‰ï¼‰",
            key="local_merge_checkbox"
        )

        if st.button("ğŸ–¼ï¸ å¼€å§‹å›¾ç‰‡æ•´åˆ", key="enrich_btn"):
            progress = st.progress(0)
            status = st.empty()

            full_works = []
            if merge_full_metadata:
                full_works = load_existing_works()
                if not full_works:
                    st.warning("âš ï¸ æœªæ‰¾åˆ° aaajiao_works.jsonï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®")

            scraper = AaajiaoScraper()
            works_to_process = works_for_images[:img_limit]
            enriched_works = []

            for i, work in enumerate(works_to_process):
                title = work.get("title", "æœªçŸ¥")[:30]
                status.text(f"[{i+1}/{len(works_to_process)}] {title}...")

                try:
                    enriched = scraper.enrich_work_with_images(work, output_dir="output")
                    if merge_full_metadata and full_works:
                        enriched = merge_work_with_full_data(enriched, full_works)
                    enriched_works.append(enriched)
                except Exception as e:
                    st.warning(f"å¤±è´¥ï¼š{title} - {e}")
                    enriched_works.append(work)

                progress.progress((i + 1) / len(works_to_process))

            # ç”ŸæˆæŠ¥å‘Š
            if merge_full_metadata:
                report_lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆå®Œæ•´å…ƒæ•°æ® + å›¾ç‰‡ï¼‰\n",
                    f"*ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}*\n\n"
                ]
                for work in enriched_works:
                    report_lines.append(generate_rich_work_markdown(work, include_local_images=True))
            else:
                report_lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆå«å›¾ç‰‡ï¼‰\n",
                    f"*ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}*\n\n"
                ]
                for work in enriched_works:
                    title = work.get("title", "æ— æ ‡é¢˜")
                    year = work.get("year", "")
                    local_images = work.get("local_images", [])

                    report_lines.append(f"## {title}\n")
                    report_lines.append(f"**å¹´ä»½ï¼š** {year}\n\n")

                    if local_images:
                        report_lines.append("### å›¾ç‰‡\n\n")
                        for img_path in local_images:  # æ˜¾ç¤ºå…¨éƒ¨æœ¬åœ°å›¾ç‰‡
                            rel_path = os.path.basename(img_path)
                            report_lines.append(f'<a href="{rel_path}" target="_blank"><img src="{rel_path}" width="400"></a>\n\n')

                    report_lines.append("---\n\n")

            report_content = "".join(report_lines)

            os.makedirs("output", exist_ok=True)
            with open("output/portfolio_with_images.md", "w", encoding="utf-8") as f:
                f.write(report_content)

            # ä¿å­˜åˆ° reports æ–‡ä»¶å¤¹ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            os.makedirs("reports", exist_ok=True)
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            report_filename = f"portfolio_images_{timestamp}.md"
            report_path = os.path.join("reports", report_filename)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            st.success(f"âœ… å›¾ç‰‡æ•´åˆå®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ° `{report_path}`")
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æŠ¥å‘Š",
                data=report_content,
                file_name="aaajiao_portfolio_images.md",
                mime="text/markdown"
            )

    # --- åŠŸèƒ½ 2ï¼šç½‘ç»œå›¾ç‰‡æŠ¥å‘Š ---
    with st.expander("ğŸŒ ç½‘ç»œå›¾ç‰‡æŠ¥å‘Šï¼ˆä¸ä¸‹è½½ï¼‰"):
        st.markdown("ç”Ÿæˆè½»é‡æŠ¥å‘Šï¼Œä½¿ç”¨åœ¨çº¿å›¾ç‰‡é“¾æ¥ã€‚")

        web_merge_full_metadata = st.checkbox(
            "åˆå¹¶å®Œæ•´å…ƒæ•°æ®",
            value=False,
            help="ä» aaajiao_works.json åˆå¹¶å®Œæ•´ä½œå“ä¿¡æ¯ï¼ˆç±»å‹ã€ææ–™ã€å°ºå¯¸ã€æè¿°ç­‰ï¼‰",
            key="web_merge_checkbox"
        )

        if st.button("ğŸ“„ ç”Ÿæˆç½‘ç»œæŠ¥å‘Š", key="web_report_btn"):
            progress = st.progress(0)
            status = st.empty()

            full_works = []
            if web_merge_full_metadata:
                full_works = load_existing_works()
                if not full_works:
                    st.warning("âš ï¸ æœªæ‰¾åˆ° aaajiao_works.jsonï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®")

            # æŒ‰å¹´ä»½æ’åº
            def get_sort_year(w):
                y = w.get("year", "0000")
                if "-" in str(y):
                    return str(y).split("-")[-1]
                return str(y)

            sorted_works = sorted(works_for_images, key=get_sort_year, reverse=True)

            if web_merge_full_metadata:
                lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆå®Œæ•´å…ƒæ•°æ®ï¼‰\n",
                    f"> ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}\n",
                    "> **æ³¨æ„**ï¼šå›¾ç‰‡ä¸º eventstructure.com çš„ç›´é“¾\n\n",
                    "---\n\n"
                ]
            else:
                lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆç½‘ç»œå›¾ç‰‡ï¼‰\n",
                    f"> ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}\n",
                    "> **æ³¨æ„**ï¼šå›¾ç‰‡ä¸º eventstructure.com çš„ç›´é“¾\n\n",
                    "---\n\n"
                ]

            scraper = AaajiaoScraper()

            for i, work in enumerate(sorted_works):
                status.text(f"å¤„ç†ä¸­ {i+1}/{len(sorted_works)}...")
                progress.progress((i + 1) / len(sorted_works))

                if web_merge_full_metadata and full_works:
                    work = merge_work_with_full_data(work, full_works)

                # è·å–å›¾ç‰‡ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                imgs = work.get("images", []) or work.get("high_res_images", [])
                if not imgs and work.get("url"):
                    try:
                        imgs = scraper.extract_images_from_page(work.get("url"))
                        work["images"] = imgs
                    except Exception:
                        pass

                if web_merge_full_metadata:
                    lines.append(generate_rich_work_markdown(work, include_local_images=False))
                else:
                    title = work.get("title", "æ— æ ‡é¢˜")
                    year = work.get("year", "")
                    url = work.get("url", "")

                    lines.append(f"## {year} - {title}\n\n")
                    lines.append(f"**é“¾æ¥ï¼š** [{url}]({url})\n\n")

                    if imgs:
                        lines.append("### å›¾ç‰‡\n\n")
                        for img in imgs[:5]:
                            lines.append(f"![]({img})\n\n")

                    lines.append("---\n\n")

            report = "".join(lines)

            # ä¿å­˜åˆ° reports æ–‡ä»¶å¤¹ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            os.makedirs("reports", exist_ok=True)
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            report_filename = f"web_report_{timestamp}.md"
            report_path = os.path.join("reports", report_filename)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report)

            st.success(f"âœ… å·²ä¸º {len(sorted_works)} ä¸ªä½œå“ç”ŸæˆæŠ¥å‘Šï¼ä¿å­˜åˆ° `{report_path}`")
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ç½‘ç»œæŠ¥å‘Š",
                data=report,
                file_name="aaajiao_web_images_report.md",
                mime="text/markdown"
            )

else:
    st.warning("âš ï¸ æœªæ‰¾åˆ°å·²ç¼“å­˜ä½œå“ã€‚è¯·å…ˆè¿è¡Œã€Œå¼€å§‹æŠ“å–ã€ã€‚")


# ============ ä¾§è¾¹æ  ============

with st.sidebar:
    st.markdown("### æ§åˆ¶å°")
    st.markdown("---")
    st.markdown("**ä¸¤å±‚æ··åˆç­–ç•¥ (v6.3.0)ï¼š**")
    st.markdown("- ç¬¬1å±‚ï¼š0 creditsï¼ˆBS4 æœ¬åœ°ï¼‰")
    st.markdown("- ç¬¬2å±‚ï¼š~5 creditsï¼ˆExtract v2ï¼‰")
    st.markdown("- æ™ºèƒ½åˆå¹¶ï¼š90%+ å®Œæ•´åº¦")
    st.markdown("---")
    st.markdown("**è¿‡æ»¤è§„åˆ™ï¼š**")
    st.markdown("- âœ… ä»…ä½œå“")
    st.markdown("- âŒ æ’é™¤å±•è§ˆ")
    st.markdown("- âŒ æ’é™¤ç”»å†Œ")
    st.markdown("---")

    if st.button("ğŸ”„ é‡æ–°åŠ è½½æ•°æ®"):
        st.session_state.works = load_existing_works()
        st.rerun()

    if st.button("âŒ é€€å‡ºåº”ç”¨"):
        st.warning("æ­£åœ¨é€€å‡º...")
        time.sleep(1)
        os._exit(0)

"""
aaajiao ä½œå“é›†æŠ“å–å·¥å…· - Streamlit GUI

ç®€åŒ–çš„å•é¡µç•Œé¢ï¼Œç”¨äºä» eventstructure.com æŠ“å–ä½œå“æ•°æ®ã€‚
åŠŸèƒ½ï¼š
- ä¸€é”®æŠ“å–ï¼Œä¸‰å±‚æˆæœ¬ä¼˜åŒ–ç­–ç•¥
- è‡ªåŠ¨è¿‡æ»¤å±•è§ˆå’Œç”»å†Œ
- å›¾ç‰‡æ•´åˆå·¥å…·
"""

import json
import os
import time

import pandas as pd
import streamlit as st

from scraper import AaajiaoScraper, deduplicate_works, is_artwork

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="aaajiao æŠ“å–å·¥å…·",
    page_icon="ğŸ¨",
    layout="wide"
)

# æ ‡é¢˜
st.title("ğŸ¨ aaajiao ä½œå“é›†æŠ“å–å·¥å…·")
st.markdown("è‡ªåŠ¨ä» eventstructure.com æŠ“å–ä½œå“è¯¦æƒ…")


# ============ è¾…åŠ©å‡½æ•° ============

def load_existing_works() -> list:
    """ä» JSON æ–‡ä»¶åŠ è½½å·²æœ‰ä½œå“ã€‚"""
    try:
        with open("aaajiao_works.json", "r", encoding="utf-8") as f:
            works = json.load(f)
            # è¿‡æ»¤æ‰å±•è§ˆï¼ˆä»¥é˜²æ—§æ•°æ®åŒ…å«ï¼‰
            return [w for w in works if is_artwork(w)]
    except FileNotFoundError:
        return []


def get_stats(works: list) -> dict:
    """è®¡ç®—ä½œå“ç»Ÿè®¡æ•°æ®ã€‚"""
    total = len(works)
    has_size = sum(1 for w in works if w.get('size'))
    has_duration = sum(1 for w in works if w.get('duration'))
    has_year = sum(1 for w in works if w.get('year'))

    return {
        "total": total,
        "has_size": has_size,
        "has_duration": has_duration,
        "has_year": has_year,
        "size_pct": (has_size / total * 100) if total > 0 else 0,
    }


def normalize_url(url: str) -> str:
    """è§„èŒƒåŒ– URL ç”¨äºåŒ¹é…ã€‚"""
    if not url:
        return ""
    url = url.strip().rstrip("/")
    return url


def merge_work_with_full_data(work: dict, full_works: list) -> dict:
    """å°†ä½œå“æ•°æ®ä¸ aaajiao_works.json ä¸­çš„å®Œæ•´å…ƒæ•°æ®åˆå¹¶ã€‚

    ä½¿ç”¨ URL ä½œä¸ºåŒ¹é…é”®ï¼Œä¿ç•™ç¼“å­˜ä¸­çš„å›¾ç‰‡æ•°æ®ï¼Œè¡¥å…… JSON ä¸­çš„å…ƒæ•°æ®ã€‚
    """
    url = normalize_url(work.get("url", ""))
    if not url:
        return work

    for full_work in full_works:
        if normalize_url(full_work.get("url", "")) == url:
            merged = work.copy()
            metadata_fields = [
                'type', 'materials', 'size', 'duration',
                'description_en', 'description_cn', 'video_link', 'tags',
                'title', 'title_cn', 'year'
            ]
            for key in metadata_fields:
                if full_work.get(key):
                    merged[key] = full_work[key]
            if len(full_work.get("images", [])) > len(merged.get("images", [])):
                merged["images"] = full_work["images"]
                merged["high_res_images"] = full_work.get("high_res_images", full_work["images"])
            return merged

    return work


def generate_rich_work_markdown(work: dict, include_local_images: bool = False) -> str:
    """ç”ŸæˆåŒ…å«å®Œæ•´å…ƒæ•°æ®çš„ä½œå“ Markdownã€‚

    Args:
        work: ä½œå“å­—å…¸
        include_local_images: True ä½¿ç”¨æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼ŒFalse ä½¿ç”¨ç½‘ç»œ URL
    """
    lines = []

    title = work.get("title", "æ— æ ‡é¢˜")
    title_cn = work.get("title_cn", "")
    year = work.get("year", "")

    if title_cn and title_cn != title:
        lines.append(f"## {title} / {title_cn}\n\n")
    else:
        lines.append(f"## {title}\n\n")

    lines.append("| å­—æ®µ | å†…å®¹ |\n")
    lines.append("|------|------|\n")

    if year:
        lines.append(f"| å¹´ä»½ | {year} |\n")
    if work.get("type"):
        lines.append(f"| ç±»å‹ | {work['type']} |\n")
    if work.get("materials"):
        lines.append(f"| ææ–™ | {work['materials']} |\n")
    if work.get("size"):
        lines.append(f"| å°ºå¯¸ | {work['size']} |\n")
    if work.get("duration"):
        lines.append(f"| æ—¶é•¿ | {work['duration']} |\n")
    if work.get("video_link"):
        lines.append(f"| è§†é¢‘ | [{work['video_link']}]({work['video_link']}) |\n")
    if work.get("url"):
        lines.append(f"| é“¾æ¥ | [{work['url']}]({work['url']}) |\n")

    lines.append("\n")

    if work.get("description_cn"):
        lines.append(f"### ä¸­æ–‡æè¿°\n\n> {work['description_cn']}\n\n")
    if work.get("description_en"):
        lines.append(f"### English Description\n\n{work['description_en']}\n\n")

    if include_local_images and work.get("local_images"):
        images = work.get("local_images", [])
        if images:
            lines.append("### å›¾ç‰‡\n\n")
            for img_path in images[:10]:
                rel_path = os.path.basename(img_path)
                lines.append(f"![{title}]({rel_path})\n\n")
    else:
        images = work.get("images", []) or work.get("high_res_images", [])
        if images:
            lines.append("### å›¾ç‰‡\n\n")
            for img in images[:5]:
                lines.append(f"![]({img})\n\n")

    lines.append("---\n\n")
    return "".join(lines)


# ============ åˆå§‹åŒ– Session State ============

if 'works' not in st.session_state:
    st.session_state.works = load_existing_works()
if 'running' not in st.session_state:
    st.session_state.running = False
if 'logs' not in st.session_state:
    st.session_state.logs = []


# ============ ä¸»ç•Œé¢ ============

# --- çŠ¶æ€åŒºåŸŸ ---
st.subheader("ğŸ“¦ å½“å‰çŠ¶æ€")

works = st.session_state.works
stats = get_stats(works)

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("ä½œå“æ€»æ•°", stats["total"])
with col2:
    st.metric("æœ‰å°ºå¯¸", f"{stats['has_size']} ({stats['size_pct']:.0f}%)")
with col3:
    st.metric("æœ‰æ—¶é•¿", stats["has_duration"])
with col4:
    st.metric("æœ‰å¹´ä»½", stats["has_year"])

st.divider()

# --- ä¸»æ“ä½œåŒºåŸŸ ---
st.subheader("ğŸš€ ä¸€é”®æŠ“å–")
st.markdown("""
**å·¥ä½œæµç¨‹ï¼š** è·å– sitemap â†’ æå–æ•°æ®ï¼ˆä¸‰å±‚ä¼˜åŒ–ï¼‰â†’ è¿‡æ»¤å±•è§ˆ â†’ ä¿å­˜

- **ç¬¬1å±‚ï¼š** æœ¬åœ° HTML è§£æï¼ˆ0 creditsï¼‰
- **ç¬¬2å±‚ï¼š** Markdown æŠ“å– + æ­£åˆ™ï¼ˆ1 creditï¼‰
- **ç¬¬3å±‚ï¼š** LLM æå–ï¼ˆ2 creditsï¼‰- ä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨
""")

# é«˜çº§é€‰é¡¹ï¼ˆé»˜è®¤æŠ˜å ï¼‰
with st.expander("âš™ï¸ é«˜çº§é€‰é¡¹"):
    col_opt1, col_opt2 = st.columns(2)
    with col_opt1:
        incremental = st.checkbox(
            "å¢é‡æ›´æ–°",
            value=True,
            help="ä»…è·å–æ–°å¢/æ›´æ–°çš„é¡µé¢ï¼ˆåŸºäº sitemap lastmodï¼‰"
        )
    with col_opt2:
        max_workers = st.slider(
            "å¹¶å‘æ•°",
            min_value=1,
            max_value=8,
            value=4,
            help="å¹¶è¡Œæå–çš„å·¥ä½œçº¿ç¨‹æ•°"
        )

# è¿è¡ŒæŒ‰é’®
if st.button(
    "ğŸš€ å¼€å§‹æŠ“å–",
    disabled=st.session_state.running,
    type="primary",
    use_container_width=True
):
    st.session_state.running = True
    st.session_state.logs = []

    progress_bar = st.progress(0)
    status_text = st.empty()
    log_area = st.empty()

    def progress_callback(msg: str, pct: float):
        st.session_state.logs.append(msg)
        progress_bar.progress(pct)
        status_text.text(msg)
        log_area.code("\n".join(st.session_state.logs[-10:]))

    try:
        scraper = AaajiaoScraper()
        result = scraper.run_full_pipeline(
            incremental=incremental,
            max_workers=max_workers,
            progress_callback=progress_callback,
        )

        st.session_state.works = result["works"]
        stats = result["stats"]

        st.success(
            f"âœ… å®Œæˆï¼æå–äº† {stats['extracted']} ä¸ªä½œå“ï¼Œ"
            f"è·³è¿‡äº† {stats['skipped_exhibitions']} ä¸ªå±•è§ˆï¼Œ"
            f"å…±ä¿å­˜ {stats['total']} ä¸ªä½œå“ã€‚"
        )
        st.balloons()

    except Exception as e:
        st.error(f"é”™è¯¯ï¼š{str(e)}")
    finally:
        st.session_state.running = False

st.divider()

# --- è¾“å‡ºåŒºåŸŸ ---
st.subheader("ğŸ“¥ è¾“å‡ºæ–‡ä»¶")

col_dl1, col_dl2 = st.columns(2)

with col_dl1:
    try:
        with open("aaajiao_works.json", "rb") as f:
            st.download_button(
                label="ğŸ“„ ä¸‹è½½ JSON",
                data=f,
                file_name="aaajiao_works.json",
                mime="application/json",
                use_container_width=True
            )
    except FileNotFoundError:
        st.info("JSON æ–‡ä»¶å°šæœªç”Ÿæˆ")

with col_dl2:
    try:
        with open("aaajiao_portfolio.md", "rb") as f:
            st.download_button(
                label="ğŸ“ ä¸‹è½½ Markdown",
                data=f,
                file_name="aaajiao_portfolio.md",
                mime="text/markdown",
                use_container_width=True
            )
    except FileNotFoundError:
        st.info("Markdown æ–‡ä»¶å°šæœªç”Ÿæˆ")

# --- æ•°æ®é¢„è§ˆåŒºåŸŸ ---
with st.expander("ğŸ“‹ æ•°æ®é¢„è§ˆ", expanded=bool(works)):
    if works:
        df = pd.DataFrame(works)
        # å®šä¹‰æ‰€æœ‰å¯ç”¨åˆ—åŠå…¶æ˜¾ç¤ºåç§°
        all_columns = {
            'title': 'æ ‡é¢˜',
            'title_cn': 'ä¸­æ–‡æ ‡é¢˜',
            'year': 'å¹´ä»½',
            'type': 'ç±»å‹',
            'materials': 'ææ–™',
            'size': 'å°ºå¯¸',
            'duration': 'æ—¶é•¿',
            'description_cn': 'ä¸­æ–‡æè¿°',
            'description_en': 'è‹±æ–‡æè¿°',
            'video_link': 'è§†é¢‘é“¾æ¥',
            'tags': 'æ ‡ç­¾',
            'url': 'é“¾æ¥'
        }
        # é»˜è®¤æ˜¾ç¤ºçš„åˆ—
        default_cols = ['title', 'title_cn', 'year', 'type', 'materials', 'size', 'duration']
        available_cols = [c for c in all_columns.keys() if c in df.columns]

        # åˆ—é€‰æ‹©å™¨
        selected_cols = st.multiselect(
            "é€‰æ‹©æ˜¾ç¤ºçš„åˆ—",
            options=available_cols,
            default=[c for c in default_cols if c in available_cols],
            format_func=lambda x: all_columns.get(x, x)
        )

        if selected_cols:
            # é‡å‘½ååˆ—ä¸ºä¸­æ–‡æ˜¾ç¤º
            display_df = df[selected_cols].copy()
            display_df.columns = [all_columns.get(c, c) for c in selected_cols]
            st.dataframe(display_df.head(100), use_container_width=True)
            st.caption(f"æ˜¾ç¤º {min(100, len(works))}/{len(works)} ä¸ªä½œå“")
        else:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€åˆ—")
    else:
        st.info("æš‚æ— æ•°æ®ã€‚ç‚¹å‡»ã€Œå¼€å§‹æŠ“å–ã€å¼€å§‹ã€‚")

st.divider()

# ============ å›¾ç‰‡å·¥å…·åŒºåŸŸ ============

st.subheader("ğŸ–¼ï¸ å›¾ç‰‡å·¥å…·")

# åŠ è½½ç¼“å­˜çš„ä½œå“ç”¨äºå›¾ç‰‡å·¥å…·
scraper_preview = AaajiaoScraper()
cached_works = scraper_preview.get_all_cached_works()
# ä»…è¿‡æ»¤ä¸ºä½œå“
cached_works = [w for w in cached_works if is_artwork(w)]

if cached_works:
    st.success(f"ğŸ“¦ æ‰¾åˆ° {len(cached_works)} ä¸ªå·²ç¼“å­˜ä½œå“")

    # --- åŠŸèƒ½ 1ï¼šå›¾ç‰‡æ•´åˆ ---
    with st.expander("ğŸ–¼ï¸ å›¾ç‰‡æ•´åˆï¼ˆä¸‹è½½åˆ°æœ¬åœ°ï¼‰"):
        st.markdown("""
        ä»ç¼“å­˜çš„ä½œå“ä¸­æå–å¹¶ä¸‹è½½å›¾ç‰‡ã€‚
        - ä½¿ç”¨ HTML è§£æï¼ˆæ—  API æˆæœ¬ï¼‰
        - ä¸‹è½½åˆ° `output/images/`
        """)

        col_img1, col_img2 = st.columns(2)
        with col_img1:
            download_images = st.checkbox("ä¸‹è½½å›¾ç‰‡", value=True)
        with col_img2:
            img_limit = st.slider(
                "å¤„ç†ä½œå“æ•°",
                min_value=1,
                max_value=len(cached_works),
                value=min(50, len(cached_works))
            )

        merge_full_metadata = st.checkbox(
            "åˆå¹¶å®Œæ•´å…ƒæ•°æ®",
            value=False,
            help="ä» aaajiao_works.json åˆå¹¶å®Œæ•´ä½œå“ä¿¡æ¯ï¼ˆç±»å‹ã€ææ–™ã€å°ºå¯¸ã€æè¿°ç­‰ï¼‰",
            key="local_merge_checkbox"
        )

        if st.button("ğŸ–¼ï¸ å¼€å§‹å›¾ç‰‡æ•´åˆ", key="enrich_btn"):
            progress = st.progress(0)
            status = st.empty()

            full_works = []
            if merge_full_metadata:
                full_works = load_existing_works()
                if not full_works:
                    st.warning("âš ï¸ æœªæ‰¾åˆ° aaajiao_works.jsonï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®")

            scraper = AaajiaoScraper()
            works_to_process = cached_works[:img_limit]
            enriched_works = []

            for i, work in enumerate(works_to_process):
                title = work.get("title", "æœªçŸ¥")[:30]
                status.text(f"[{i+1}/{len(works_to_process)}] {title}...")

                try:
                    enriched = scraper.enrich_work_with_images(work, output_dir="output")
                    if merge_full_metadata and full_works:
                        enriched = merge_work_with_full_data(enriched, full_works)
                    enriched_works.append(enriched)
                except Exception as e:
                    st.warning(f"å¤±è´¥ï¼š{title} - {e}")
                    enriched_works.append(work)

                progress.progress((i + 1) / len(works_to_process))

            # ç”ŸæˆæŠ¥å‘Š
            if merge_full_metadata:
                report_lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆå®Œæ•´å…ƒæ•°æ® + å›¾ç‰‡ï¼‰\n",
                    f"*ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}*\n\n"
                ]
                for work in enriched_works:
                    report_lines.append(generate_rich_work_markdown(work, include_local_images=True))
            else:
                report_lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆå«å›¾ç‰‡ï¼‰\n",
                    f"*ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}*\n\n"
                ]
                for work in enriched_works:
                    title = work.get("title", "æ— æ ‡é¢˜")
                    year = work.get("year", "")
                    local_images = work.get("local_images", [])

                    report_lines.append(f"## {title}\n")
                    report_lines.append(f"**å¹´ä»½ï¼š** {year}\n\n")

                    if local_images:
                        report_lines.append("### å›¾ç‰‡\n\n")
                        for img_path in local_images[:10]:
                            rel_path = os.path.basename(img_path)
                            report_lines.append(f"![å›¾ç‰‡]({rel_path})\n\n")

                    report_lines.append("---\n\n")

            report_content = "".join(report_lines)

            os.makedirs("output", exist_ok=True)
            with open("output/portfolio_with_images.md", "w", encoding="utf-8") as f:
                f.write(report_content)

            st.success("âœ… å›¾ç‰‡æ•´åˆå®Œæˆï¼")
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æŠ¥å‘Š",
                data=report_content,
                file_name="aaajiao_portfolio_images.md",
                mime="text/markdown"
            )

    # --- åŠŸèƒ½ 2ï¼šç½‘ç»œå›¾ç‰‡æŠ¥å‘Š ---
    with st.expander("ğŸŒ ç½‘ç»œå›¾ç‰‡æŠ¥å‘Šï¼ˆä¸ä¸‹è½½ï¼‰"):
        st.markdown("ç”Ÿæˆè½»é‡æŠ¥å‘Šï¼Œä½¿ç”¨åœ¨çº¿å›¾ç‰‡é“¾æ¥ã€‚")

        web_merge_full_metadata = st.checkbox(
            "åˆå¹¶å®Œæ•´å…ƒæ•°æ®",
            value=False,
            help="ä» aaajiao_works.json åˆå¹¶å®Œæ•´ä½œå“ä¿¡æ¯ï¼ˆç±»å‹ã€ææ–™ã€å°ºå¯¸ã€æè¿°ç­‰ï¼‰",
            key="web_merge_checkbox"
        )

        if st.button("ğŸ“„ ç”Ÿæˆç½‘ç»œæŠ¥å‘Š", key="web_report_btn"):
            progress = st.progress(0)
            status = st.empty()

            full_works = []
            if web_merge_full_metadata:
                full_works = load_existing_works()
                if not full_works:
                    st.warning("âš ï¸ æœªæ‰¾åˆ° aaajiao_works.jsonï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®")

            # æŒ‰å¹´ä»½æ’åº
            def get_sort_year(w):
                y = w.get("year", "0000")
                if "-" in str(y):
                    return str(y).split("-")[-1]
                return str(y)

            sorted_works = sorted(cached_works, key=get_sort_year, reverse=True)

            if web_merge_full_metadata:
                lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆå®Œæ•´å…ƒæ•°æ®ï¼‰\n",
                    f"> ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}\n",
                    "> **æ³¨æ„**ï¼šå›¾ç‰‡ä¸º eventstructure.com çš„ç›´é“¾\n\n",
                    "---\n\n"
                ]
            else:
                lines = [
                    "# aaajiao ä½œå“é›†ï¼ˆç½‘ç»œå›¾ç‰‡ï¼‰\n",
                    f"> ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M')}\n",
                    "> **æ³¨æ„**ï¼šå›¾ç‰‡ä¸º eventstructure.com çš„ç›´é“¾\n\n",
                    "---\n\n"
                ]

            scraper = AaajiaoScraper()

            for i, work in enumerate(sorted_works):
                status.text(f"å¤„ç†ä¸­ {i+1}/{len(sorted_works)}...")
                progress.progress((i + 1) / len(sorted_works))

                if web_merge_full_metadata and full_works:
                    work = merge_work_with_full_data(work, full_works)

                # è·å–å›¾ç‰‡ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                imgs = work.get("images", []) or work.get("high_res_images", [])
                if not imgs and work.get("url"):
                    try:
                        imgs = scraper.extract_images_from_page(work.get("url"))
                        work["images"] = imgs
                    except Exception:
                        pass

                if web_merge_full_metadata:
                    lines.append(generate_rich_work_markdown(work, include_local_images=False))
                else:
                    title = work.get("title", "æ— æ ‡é¢˜")
                    year = work.get("year", "")
                    url = work.get("url", "")

                    lines.append(f"## {year} - {title}\n\n")
                    lines.append(f"**é“¾æ¥ï¼š** [{url}]({url})\n\n")

                    if imgs:
                        lines.append("### å›¾ç‰‡\n\n")
                        for img in imgs[:5]:
                            lines.append(f"![]({img})\n\n")

                    lines.append("---\n\n")

            report = "".join(lines)

            st.success(f"âœ… å·²ä¸º {len(sorted_works)} ä¸ªä½œå“ç”ŸæˆæŠ¥å‘Šï¼")
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ç½‘ç»œæŠ¥å‘Š",
                data=report,
                file_name="aaajiao_web_images_report.md",
                mime="text/markdown"
            )

else:
    st.warning("âš ï¸ æœªæ‰¾åˆ°å·²ç¼“å­˜ä½œå“ã€‚è¯·å…ˆè¿è¡Œã€Œå¼€å§‹æŠ“å–ã€ã€‚")


# ============ ä¾§è¾¹æ  ============

with st.sidebar:
    st.markdown("### æ§åˆ¶å°")
    st.markdown("---")
    st.markdown("**æˆæœ¬ä¼˜åŒ–ï¼š**")
    st.markdown("- ç¬¬1å±‚ï¼š0 creditsï¼ˆæœ¬åœ°ï¼‰")
    st.markdown("- ç¬¬2å±‚ï¼š1 creditï¼ˆmarkdownï¼‰")
    st.markdown("- ç¬¬3å±‚ï¼š2 creditsï¼ˆLLMï¼‰")
    st.markdown("---")
    st.markdown("**è¿‡æ»¤è§„åˆ™ï¼š**")
    st.markdown("- âœ… ä»…ä½œå“")
    st.markdown("- âŒ æ’é™¤å±•è§ˆ")
    st.markdown("- âŒ æ’é™¤ç”»å†Œ")
    st.markdown("---")

    if st.button("ğŸ”„ é‡æ–°åŠ è½½æ•°æ®"):
        st.session_state.works = load_existing_works()
        st.rerun()

    if st.button("âŒ é€€å‡ºåº”ç”¨"):
        st.warning("æ­£åœ¨é€€å‡º...")
        time.sleep(1)
        os._exit(0)

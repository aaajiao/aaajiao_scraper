import streamlit as st
import time
import pandas as pd
import json
import os
from aaajiao_scraper import AaajiaoScraper
import concurrent.futures

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="aaajiao Scraper",
    page_icon="ğŸ¨",
    layout="wide"
)

# æ ‡é¢˜
st.title("ğŸ¨ aaajiao ä½œå“é›†æŠ“å–å·¥å…·")
st.markdown("æ­¤å·¥å…·å¯ä»¥ä» eventstructure.com è‡ªåŠ¨æŠ“å–ä½œå“ä¿¡æ¯å¹¶ç”Ÿæˆæ–‡æ¡£ã€‚")

# åˆå§‹åŒ– session state
if 'works' not in st.session_state:
    st.session_state.works = []
if 'scraping' not in st.session_state:
    st.session_state.scraping = False
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []
if 'agent_result' not in st.session_state:
    st.session_state.agent_result = None

def run_scraper():
    st.session_state.scraping = True
    st.session_state.works = []
    st.session_state.log_messages = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        # 1. è·å–é“¾æ¥
        status_text.text("æ­£åœ¨è·å–ä½œå“åˆ—è¡¨...")
        st.session_state.log_messages.append("æ­£åœ¨æ‰«æä¸»é¡µè·å–é“¾æ¥...")
        links = scraper.get_all_work_links()
        total_links = len(links)
        st.session_state.log_messages.append(f"æ‰¾åˆ° {total_links} ä¸ªä½œå“é“¾æ¥")
        
        # 2. å¹¶å‘æŠ“å–
        if total_links > 0:
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                future_to_url = {executor.submit(scraper.extract_work_details, url): url for url in links}
                
                completed_count = 0
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    completed_count += 1
                    
                    try:
                        data = future.result()
                        if data:
                            st.session_state.works.append(data)
                            msg = f"[{completed_count}/{total_links}] æˆåŠŸ: {data.get('title', 'Unknown')}"
                        else:
                            msg = f"[{completed_count}/{total_links}] å¤±è´¥: {url}"
                            
                        st.session_state.log_messages.append(msg)
                        
                        # æ›´æ–°UI
                        progress = completed_count / total_links
                        progress_bar.progress(progress)
                        status_text.text(f"æ­£åœ¨æŠ“å–: {completed_count}/{total_links}")
                        
                        # ä»…æ˜¾ç¤ºæœ€è¿‘5æ¡æ—¥å¿—ä»¥å…åˆ·å±
                        log_area.code("\n".join(st.session_state.log_messages[-5:]))
                        
                    except Exception as e:
                        st.session_state.log_messages.append(f"é”™è¯¯: {e}")

        # 3. ä¿å­˜æ–‡ä»¶
        status_text.text("æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        # æ­¤æ—¶ works å·²ç»å¡«å……åˆ° scraper å®ä¾‹ä¸­äº†å—ï¼Ÿæ²¡æœ‰ï¼Œæˆ‘ä»¬æ‰‹åŠ¨èµ‹å€¼
        scraper.works = st.session_state.works
        
        scraper.save_to_json()
        scraper.generate_markdown()
        
        st.success(f"æŠ“å–å®Œæˆï¼å…±è·å– {len(st.session_state.works)} ä¸ªä½œå“ã€‚")
        st.balloons()
        
    except Exception as e:
        st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
    finally:
        st.session_state.scraping = False


def run_agent(prompt: str, urls: str, max_credits: int, download_images: bool = False):
    """è¿è¡Œ Agent æŸ¥è¯¢"""
    st.session_state.agent_result = None
    
    status_area = st.empty()
    result_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        status_area.info("ğŸ¤– å¯åŠ¨ Agent ä»»åŠ¡...")
        
        # è§£æ URLs
        url_list = None
        if urls.strip():
            url_list = [u.strip() for u in urls.split(",") if u.strip()]
        
        # å¦‚æœéœ€è¦ä¸‹è½½å›¾ç‰‡ï¼Œå¢å¼º prompt
        enhanced_prompt = prompt
        if download_images and "image" not in prompt.lower():
            enhanced_prompt = f"{prompt}. Also extract all image URLs from the page."
        
        # è°ƒç”¨ Agent
        result = scraper.agent_search(enhanced_prompt, urls=url_list, max_credits=max_credits)
        
        if result:
            st.session_state.agent_result = result
            status_area.success("âœ… Agent æŸ¥è¯¢å®Œæˆ!")
            result_area.json(result)
            
            # å¦‚æœå‹¾é€‰äº†ä¸‹è½½å›¾ç‰‡ï¼Œç”ŸæˆæŠ¥å‘Š
            if download_images:
                status_area.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š...")
                scraper.generate_agent_report(result, "agent_output", prompt=enhanced_prompt)
                status_area.success("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        else:
            status_area.error("âŒ Agent æŸ¥è¯¢å¤±è´¥")
            
    except Exception as e:
        status_area.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")


# ============ ä¸»ç•Œé¢ï¼šä½¿ç”¨ Tabs ============

tab1, tab2 = st.tabs(["ğŸ“‹ æ‰¹é‡æŠ“å–", "ğŸ¤– Agent æŸ¥è¯¢"])

# ============ Tab 1: æ‰¹é‡æŠ“å– ============
with tab1:
    st.markdown("ä» Sitemap è·å–æ‰€æœ‰ä½œå“é“¾æ¥ï¼Œé€ä¸€æŠ“å–è¯¦ç»†ä¿¡æ¯ã€‚")
    
    if st.button("ğŸš€ å¼€å§‹æŠ“å–", disabled=st.session_state.scraping, type="primary", key="scrape_btn"):
        run_scraper()

    # ç»“æœå±•ç¤ºåŒºåŸŸ
    if st.session_state.works:
        st.divider()
        st.subheader("ğŸ“Š æŠ“å–ç»“æœé¢„è§ˆ")
        
        # è½¬ä¸º DataFrame å±•ç¤º
        df = pd.DataFrame(st.session_state.works)
        # é€‰å–ä¸»è¦åˆ—å±•ç¤º
        display_cols = ['title', 'title_cn', 'year', 'type', 'url']
        cols_to_show = [c for c in display_cols if c in df.columns]
        st.dataframe(df[cols_to_show], use_container_width=True)
        
        st.divider()
        st.subheader("ğŸ“¥ ä¸‹è½½æ–‡ä»¶")
        
        c1, c2 = st.columns(2)
        with c1:
            # è¯»å–ç”Ÿæˆçš„æ–‡ä»¶ä¾›ä¸‹è½½
            try:
                with open("aaajiao_works.json", "rb") as f:
                    st.download_button(
                        label="ä¸‹è½½ JSON æ•°æ®",
                        data=f,
                        file_name="aaajiao_works.json",
                        mime="application/json"
                    )
            except FileNotFoundError:
                st.warning("JSON æ–‡ä»¶å°šæœªç”Ÿæˆ")
                
        with c2:
            try:
                with open("aaajiao_portfolio.md", "rb") as f:
                    st.download_button(
                        label="ä¸‹è½½ Markdown æ–‡æ¡£",
                        data=f,
                        file_name="aaajiao_portfolio.md",
                        mime="text/markdown"
                    )
            except FileNotFoundError:
                st.warning("Markdown æ–‡ä»¶å°šæœªç”Ÿæˆ")

    elif not st.session_state.scraping:
        st.info("ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹è¿è¡Œã€‚")


# ============ Tab 2: Agent æŸ¥è¯¢ ============
with tab2:
    st.markdown("""
    ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ æƒ³è¦çš„ä¿¡æ¯ï¼ŒFirecrawl Agent ä¼šè‡ªåŠ¨æœç´¢å¹¶æå–æ•°æ®ã€‚
    
    **ç¤ºä¾‹æŸ¥è¯¢ï¼š**
    - "Find all video installations by aaajiao"
    - "Get complete information including all images"
    - "Summarize the artwork and list exhibition history"
    """)
    
    # è¾“å…¥åŒºåŸŸ
    prompt = st.text_area(
        "æŸ¥è¯¢æè¿° (Prompt)",
        placeholder="ä¾‹å¦‚: Get complete information about this artwork including all images",
        height=100
    )
    
    urls = st.text_input(
        "æŒ‡å®š URLï¼ˆå¯é€‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰",
        placeholder="https://eventstructure.com/Absurd-Reality-Check"
    )
    
    col1, col2 = st.columns(2)
    with col1:
        max_credits = st.slider("æœ€å¤§ Credits æ¶ˆè€—", min_value=10, max_value=100, value=50)
    with col2:
        download_images = st.checkbox("ğŸ“¥ ä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š", value=True)
    
    if st.button("ğŸ” å¼€å§‹æŸ¥è¯¢", type="primary", key="agent_btn", disabled=not prompt.strip()):
        run_agent(prompt, urls, max_credits, download_images)
    
    # æ˜¾ç¤ºä¸Šæ¬¡ç»“æœ
    if st.session_state.agent_result:
        st.divider()
        st.subheader("ğŸ“‹ æŸ¥è¯¢ç»“æœ")
        
        c1, c2 = st.columns(2)
        with c1:
            # æä¾›ä¸‹è½½æŒ‰é’®
            result_json = json.dumps(st.session_state.agent_result, ensure_ascii=False, indent=2)
            st.download_button(
                label="ä¸‹è½½ç»“æœ JSON",
                data=result_json,
                file_name="agent_result.json",
                mime="application/json"
            )
        
        with c2:
            # å¦‚æœæœ‰ç”ŸæˆæŠ¥å‘Šï¼Œæä¾›ä¸‹è½½
            report_path = "agent_output/artwork_report.md"
            if os.path.exists(report_path):
                with open(report_path, "rb") as f:
                    st.download_button(
                        label="ä¸‹è½½ Markdown æŠ¥å‘Š",
                        data=f,
                        file_name="artwork_report.md",
                        mime="text/markdown"
                    )
        
        # æ˜¾ç¤ºä¸‹è½½çš„å›¾ç‰‡
        images_dir = "agent_output/images"
        if os.path.exists(images_dir):
            images = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))]
            if images:
                st.subheader("ğŸ–¼ï¸ ä¸‹è½½çš„å›¾ç‰‡")
                cols = st.columns(min(len(images), 3))
                for i, img in enumerate(sorted(images)[:6]):
                    with cols[i % 3]:
                        st.image(os.path.join(images_dir, img), caption=img, use_container_width=True)


# ä¾§è¾¹æ ï¼šé€€å‡ºåŠŸèƒ½
with st.sidebar:
    st.markdown("### æ§åˆ¶å°")
    st.markdown("---")
    st.markdown("**æ¨¡å¼è¯´æ˜ï¼š**")
    st.markdown("- **æ‰¹é‡æŠ“å–**ï¼šæŠ“å–æ‰€æœ‰ä½œå“")
    st.markdown("- **Agent æŸ¥è¯¢**ï¼šè‡ªç„¶è¯­è¨€æŸ¥è¯¢")
    st.markdown("---")
    if st.button("âŒ é€€å‡ºç¨‹åº"):
        st.warning("ç¨‹åºæ­£åœ¨é€€å‡º...æ‚¨å¯ä»¥å…³é—­æ­¤æµè§ˆå™¨æ ‡ç­¾é¡µäº†ã€‚")
        # ç»™ä¸€ç‚¹æ—¶é—´è®©ä¸Šé¢çš„æç¤ºæ¸²æŸ“å‡ºæ¥
        time.sleep(1)
        os._exit(0)


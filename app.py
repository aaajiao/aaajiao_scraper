import streamlit as st
import time
import pandas as pd
import json
import os
from aaajiao_scraper import AaajiaoScraper
import concurrent.futures

# Page Config
st.set_page_config(
    page_title="aaajiao Scraper",
    page_icon="ğŸ¨",
    layout="wide"
)

# Title
st.title("ğŸ¨ aaajiao Portfolio Scraper / ä½œå“é›†æŠ“å–å·¥å…·")
st.markdown("Automated tool to scrape artwork details from eventstructure.com / è‡ªåŠ¨æŠ“å–å¹¶ç”Ÿæˆæ–‡æ¡£å·¥å…·")

# Initialize session state
if 'works' not in st.session_state:
    st.session_state.works = []
if 'scraping' not in st.session_state:
    st.session_state.scraping = False
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []
if 'agent_result' not in st.session_state:
    st.session_state.agent_result = None
if 'discovery_found_urls' not in st.session_state:
    st.session_state.discovery_found_urls = []
if 'discovery_urls' not in st.session_state:
    st.session_state.discovery_urls = []

def run_scraper(incremental: bool = False):
    st.session_state.scraping = True
    st.session_state.log_messages = []
    
    # Reset or keep works based on incremental
    if not incremental:
        st.session_state.works = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        # 1. Get Links
        status_text.text("Fetching sitemap.xml...")
        links = scraper.get_all_work_links(incremental=incremental)
        total_links = len(links)
        
        if total_links == 0 and incremental:
             st.session_state.log_messages.append("No changes detected. / æ²¡æœ‰æ£€æµ‹åˆ°æ›´æ–°ã€‚")
             st.info("âœ… No new artworks found / æ²¡æœ‰å‘ç°æ–°ä½œå“")
             status_text.text("Done.")
             st.session_state.scraping = False
             return

        st.session_state.log_messages.append(f"Found {total_links} new/updated links / æ‰¾åˆ° {total_links} ä¸ªéœ€æ›´æ–°é“¾æ¥")
        
        # 2. Concurrent Scrape
        if total_links > 0:
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                future_to_url = {executor.submit(scraper.extract_work_details, url): url for url in links}
                
                completed_count = 0
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    completed_count += 1
                    
                    try:
                        data = future.result()
                        if data:
                            st.session_state.works.append(data)
                            msg = f"[{completed_count}/{total_links}] Success: {data.get('title', 'Unknown')}"
                        else:
                            msg = f"[{completed_count}/{total_links}] Failed: {url}"
                            
                        st.session_state.log_messages.append(msg)
                        
                        # Update UI
                        progress = completed_count / total_links
                        progress_bar.progress(progress)
                        status_text.text(f"Scraping: {completed_count}/{total_links} / æ­£åœ¨æŠ“å–...")
                        
                        # Show logs
                        log_area.code("\n".join(st.session_state.log_messages[-5:]))
                        
                    except Exception as e:
                        st.session_state.log_messages.append(f"Error: {e}")
                    
                    # Auto-save every 5 items
                    if completed_count % 5 == 0:
                        scraper.works = st.session_state.works
                        scraper.save_to_json()
                        st.session_state.log_messages.append(f"ğŸ’¾ Auto-saved {len(st.session_state.works)} items")

        # 3. Save Files
        status_text.text("Saving files... / æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        scraper.works = st.session_state.works
        
        scraper.save_to_json()
        scraper.generate_markdown()
        
        st.success(f"Completed! Scraped {len(st.session_state.works)} artworks. / æŠ“å–å®Œæˆï¼å…±è·å– {len(st.session_state.works)} ä¸ªä½œå“ã€‚")
        st.balloons()
        
    except Exception as e:
        st.error(f"Error occurred: {str(e)} / å‘ç”Ÿé”™è¯¯")
    finally:
        st.session_state.scraping = False


def run_agent(prompt: str, urls: str, max_credits: int, download_images: bool = False):
    """Run Agent Search"""
    st.session_state.agent_result = None
    
    status_area = st.empty()
    result_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        status_area.info("ğŸ¤– Starting Agent Task... / å¯åŠ¨ Agent ä»»åŠ¡...")
        
        # Parse URLs if list of strings, or keep if already list
        url_list = urls
        if isinstance(urls, str) and urls.strip():
            url_list = [u.strip() for u in urls.split(",") if u.strip()]
        
        # Enhanced prompt for images
        enhanced_prompt = prompt
        if download_images and "image" not in prompt.lower():
            enhanced_prompt = f"{prompt}. IMPORTANT: For images, extract the 'src_o' attribute which contains the high-resolution URL. Do not mistakenly extract thumbnails from the sidebar gallery."
        
        # Call Agent
        result = scraper.agent_search(enhanced_prompt, urls=url_list, max_credits=max_credits)
        
        if result:
            st.session_state.agent_result = result
            status_area.success("âœ… Agent Task Completed! / Agent æŸ¥è¯¢å®Œæˆ!")
            result_area.json(result)
            
            # Generate Report
            if download_images:
                status_area.info("ğŸ“¥ Downloading images & generating report... / æ­£åœ¨ä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š...")
                scraper.generate_agent_report(result, "agent_output", prompt=enhanced_prompt)
                status_area.success("âœ… Report Generated! / æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        else:
            status_area.error("âŒ Agent Task Failed / Agent æŸ¥è¯¢å¤±è´¥")
            
    except Exception as e:
        status_area.error(f"Error: {str(e)}")


# ============ Main Interface with Tabs ============

tab1, tab2, tab3 = st.tabs(["ğŸ—ï¸ Basic Scraper / åŸºç¡€çˆ¬è™«", "âš¡ï¸ Quick Extract / å¿«é€Ÿæå–", "ğŸš€ Batch Discovery / æ‰¹é‡å‘ç°"])

# ============ Tab 1: Basic Scraper (Original) ============
with tab1:
    col_u1, col_u2 = st.columns([1, 1])
    with col_u1:
        st.markdown("Click button below to scrape all artworks defined in `sitemap.xml` / ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æŠ“å–æ‰€æœ‰ä½œå“")
    
    with col_u2:
        incremental = st.checkbox("Incremental Update / å¢é‡æ›´æ–° (åªæŠ“å–æ–°é¡µé¢)", value=False, help="Based on sitemap 'lastmod' / åŸºäº sitemap çš„ lastmod æ£€æµ‹")
    
    if st.button("ğŸš€ Start Scraping / å¼€å§‹æŠ“å–", disabled=st.session_state.scraping, type="primary", key="scrape_btn"):
        run_scraper(incremental=incremental)

    # Results Area
    if st.session_state.works:
        st.divider()
        st.subheader("ğŸ“Š Preview / ç»“æœé¢„è§ˆ")
        
        df = pd.DataFrame(st.session_state.works)
        display_cols = ['title', 'title_cn', 'year', 'type', 'url']
        cols_to_show = [c for c in display_cols if c in df.columns]
        st.dataframe(df[cols_to_show], use_container_width=True)
        
        st.divider()
        st.subheader("ğŸ“¥ Download / ä¸‹è½½æ–‡ä»¶")
        
        c1, c2 = st.columns(2)
        with c1:
            try:
                with open("aaajiao_works.json", "rb") as f:
                    st.download_button(
                        label="Download JSON / ä¸‹è½½ JSON æ•°æ®",
                        data=f,
                        file_name="aaajiao_works.json",
                        mime="application/json"
                    )
            except FileNotFoundError:
                st.warning("JSON file not found")
                
        with c2:
            try:
                with open("aaajiao_portfolio.md", "rb") as f:
                    st.download_button(
                        label="Download Markdown / ä¸‹è½½ Markdown æ–‡æ¡£",
                        data=f,
                        file_name="aaajiao_portfolio.md",
                        mime="text/markdown"
                    )
            except FileNotFoundError:
                st.warning("Markdown file not found")

    elif not st.session_state.scraping:
        st.info("Click the button above to start. / ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹è¿è¡Œã€‚")


# ============ Tab 2: Quick Extract / AI Search (The Agent) ============
with tab2:
    st.markdown("""
    **ä¸¤ç§æ¨¡å¼ / Two Modes**:
    - **ğŸ¯ å•é¡µæå–**: å¡«å†™ URL â†’ ä½¿ç”¨ `Extract API` (~50 credits) â†’ å¿«é€Ÿæå–æŒ‡å®šé¡µé¢
    - **ğŸ¤– å¼€æ”¾æœç´¢**: ä¸å¡« URL â†’ ä½¿ç”¨ `Agent API` (é«˜æˆæœ¬) â†’ AI è‡ªä¸»æµè§ˆå’Œæœç´¢
    
    > ğŸ’¡ **æç¤º**: å¦‚æœä½ çŸ¥é“è¦æå–å“ªä¸ªé¡µé¢ï¼Œè¯·å¡«å†™ URLï¼Œè¿™æ ·æ›´ä¾¿å®œã€æ›´å¿«ï¼
    """)
    
    # Standardized Prompt
    default_prompt = "Extract all text content from the page (title, description, metadata, full text). Also extract the URL of the first visible image (or main artwork image) and map it to the field 'image'. IMPORTANT: If the image has a 'src_o' attribute, extract that URL for high resolution."

    # Input Area
    prompt = st.text_area(
        "Prompt / æå–æŒ‡ä»¤",
        value=default_prompt,
        height=120,
        help="æè¿°ä½ æƒ³è¦æå–çš„å†…å®¹"
    )
    
    urls = st.text_input(
        "Specific URL (Optional) / æŒ‡å®š URL (å¯é€‰)",
        placeholder="https://eventstructure.com/Absurd-Reality-Check",
        help="Paste a single URL here in Quick Mode. / åœ¨æ­¤ç²˜è´´å•ä¸ª URLã€‚",
        key="quick_url_input"
    )
    
    # Determine mode based on input
    has_url = bool(urls and urls.strip())
    
    col1, col2 = st.columns(2)
    with col1:
        if has_url:
            st.info("ğŸ¯ **Mode: Single Page Extraction**\n(Cost: ~50-80 credits per page)")
            # In URL mode, slider sets the COUNT of pages (if multiple comma-separated)
            max_credits = st.slider("Limit (Pages) / æ•°é‡é™åˆ¶ (é¡µæ•°)", min_value=1, max_value=10, value=1, help="Number of URLs to process.")
        else:
            st.info("ğŸ¤– **Mode: Open AI Research**\n(Cost: Variable)")
            # In Agent mode, slider sets the Credit Budget
            max_credits = st.slider("Max Budget (Credits) / é¢„ç®—ä¸Šé™ (ç§¯åˆ†)", min_value=10, max_value=200, value=50, help="Max credits the agent can spend.")
            
    with col2:
        download_images = st.checkbox("ğŸ“¥ Download Images & Report / ä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š", value=True)
    
    if st.button("ğŸ” Start / å¼€å§‹æ‰§è¡Œ", type="primary", key="agent_btn", disabled=not prompt.strip()):
        # Handle single URL as list
        url_list = urls.split(",") if urls else None
        if url_list:
             url_list = [u.strip() for u in url_list if u.strip()]
             
        # Debug feedback
        if has_url:
            st.toast(f"Processing {len(url_list)} URL(s)...", icon="ğŸš€")
        else:
            st.toast("Starting Open Agent Search...", icon="ğŸ¤–")
            
        run_agent(prompt, url_list, max_credits, download_images)

    # Show Results
    if st.session_state.agent_result:
        st.divider()
        st.subheader("ğŸ“‹ Results / æŸ¥è¯¢ç»“æœ")
        
        c1, c2 = st.columns(2)
        with c1:
            result_json = json.dumps(st.session_state.agent_result, ensure_ascii=False, indent=2)
            st.download_button(
                label="Download JSON / ä¸‹è½½ç»“æœ JSON",
                data=result_json,
                file_name="agent_result.json",
                mime="application/json"
            )
        
        with c2:
            report_path = "agent_output/artwork_report.md"
            if os.path.exists(report_path):
                with open(report_path, "rb") as f:
                    st.download_button(
                        label="Download Report / ä¸‹è½½ Markdown æŠ¥å‘Š",
                        data=f,
                        file_name="artwork_report.md",
                        mime="text/markdown"
                    )
        
        # Show Images
        images_dir = "agent_output/images"
        if os.path.exists(images_dir):
            images = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))]
            if images:
                st.subheader("ğŸ–¼ï¸ Downloaded Images / ä¸‹è½½çš„å›¾ç‰‡")
                cols = st.columns(min(len(images), 3))
                for i, img in enumerate(sorted(images)[:6]):
                    with cols[i % 3]:
                        st.image(os.path.join(images_dir, img), caption=img, use_container_width=True)


# ============ Tab 3: Batch Discovery (The Factory) ============
with tab3:
    st.markdown("""
    **Solve Infinite/Horizontal Scroll Issues / è§£å†³æ»šåŠ¨åŠ è½½é—®é¢˜**:
    1. **Scan / æ‰«æ**: Auto-scroll page to discover links.
    2. **Filter / ç­›é€‰**: Select artworks to extract.
    3. **Extract / æå–**: Batch process with selected mode.
    """)
    
    # Session State Init
    if 'discovery_urls' not in st.session_state:
        st.session_state.discovery_urls = []
    
    # --- Step 1: Scan ---
    st.subheader("1. Scan Page / æ‰«æé¡µé¢")
    
    col_url, col_mode = st.columns([3, 1])
    with col_url:
        discovery_url = st.text_input("Target URL / ç›®æ ‡ç½‘å€", value="https://eventstructure.com")
    with col_mode:
        scroll_mode = st.selectbox(
            "Scroll Strategy", 
            ["auto", "horizontal", "vertical"],
            index=0,
            help="Auto: Hybrid\nHorizontal: Gallery\nVertical: Standard"
        )
    
    if st.button("ğŸ”­ Start Scanning / å¼€å§‹æ‰«æ", type="primary"):
        with st.spinner(f"Scanning ({scroll_mode} mode)..."):
            scraper = AaajiaoScraper()
            found = scraper.discover_urls_with_scroll(discovery_url, scroll_mode=scroll_mode)
            st.session_state.discovery_urls = found
            
            if found:
                st.success(f"âœ… Found {len(found)} links / å‘ç° {len(found)} ä¸ªé“¾æ¥")
            else:
                st.error("âŒ No links found / æœªå‘ç°é“¾æ¥")

    # --- Step 2 & 3: Select & Extract (æ˜¾ç¤ºåœ¨æ‰«æç»“æœä¹‹å) ---
    if st.session_state.discovery_urls:
        st.divider()
        st.subheader("2. Filter & Extract / ç­›é€‰ä¸æå–")
        
        # Callback for Select All
        def toggle_all():
            new_state = st.session_state.select_all_chk
            for url in st.session_state.discovery_urls:
                st.session_state[f"chk_{url}"] = new_state

        # Select All Checkbox
        st.checkbox("Select All / å…¨é€‰", value=False, key="select_all_chk", on_change=toggle_all)
        
        # Link List
        selected_urls = []
        with st.expander("View Links / æŸ¥çœ‹é“¾æ¥åˆ—è¡¨", expanded=True):
            for url in st.session_state.discovery_urls:
                key = f"chk_{url}"
                if key not in st.session_state:
                    st.session_state[key] = False
                
                if st.checkbox(url, key=key):
                    selected_urls.append(url)
        
        st.write(f"Selected / å·²é€‰æ‹©: **{len(selected_urls)}** items")
        
        # --- æå–æ¨¡å¼é€‰æ‹©ï¼ˆæ”¾åœ¨é€‰æ‹©é“¾æ¥ä¹‹åï¼‰---
        st.markdown("---")
        st.markdown("**Extraction Mode / æå–æ¨¡å¼**")
        
        mode_col, config_col = st.columns([1, 1])
        with mode_col:
            extraction_level = st.radio(
                "Select Mode",
                ["quick", "full", "images_only", "custom"],
                format_func=lambda x: {
                    "quick": "âš¡ Quick (~20 credits)",
                    "full": "ğŸ“‹ Full (~50 credits)",
                    "images_only": "ğŸ–¼ï¸ Images (~30 credits)",
                    "custom": "ğŸ”§ Custom"
                }[x],
                horizontal=True,
                key="disc_level"
            )
            
            if extraction_level == "custom":
                disc_prompt = st.text_area("Custom Prompt", value="Extract all text content and high-res images (src_o attribute).", height=80, key="disc_custom_prompt")
            else:
                disc_prompt = ""
                mode_info = {"quick": "æ ‡é¢˜ã€å¹´ä»½ã€ç±»å‹", "full": "å®Œæ•´æè¿°+é«˜æ¸…å›¾", "images_only": "ä»…å›¾ç‰‡URL", "custom": ""}
                st.caption(f"ğŸ“Œ {mode_info.get(extraction_level, '')}")
        
        with config_col:
            # ç¼“å­˜ç»Ÿè®¡
            scraper_check = AaajiaoScraper()
            prompt_for_cache = disc_prompt if extraction_level == "custom" else scraper_check.PROMPT_TEMPLATES.get(extraction_level, "")
            cached_count = sum(1 for url in selected_urls if scraper_check._load_extract_cache(url, prompt_for_cache))
            uncached_count = len(selected_urls) - cached_count
            
            if cached_count > 0:
                st.success(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cached_count}/{len(selected_urls)}")
            
            cost_per_url = {"quick": 20, "full": 50, "images_only": 30, "custom": 50}.get(extraction_level, 50)
            est_cost = uncached_count * cost_per_url
            st.markdown(f"**é¢„è®¡æ¶ˆè€—:** `{est_cost} credits`")
            
            disc_credits = st.slider("Batch Limit", 1, max(50, len(selected_urls)), len(selected_urls), key="disc_slider")
            disc_download = st.checkbox("Download Images / ä¸‹è½½å›¾ç‰‡", value=True, key="disc_img")
            
        if st.button("ğŸ¤– Batch Extract / å¼€å§‹æ‰¹é‡æå–", disabled=len(selected_urls)==0, type="primary"):
            status_box = st.empty()
            with status_box.container():
                st.info("ğŸš€ Submitting Agent Task... / æ­£åœ¨æäº¤ Agent ä»»åŠ¡...")
                
                final_prompt = disc_prompt
                # å¯¹äºé custom æ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ¿
                if extraction_level != "custom":
                    final_prompt = ""  # agent_search ä¼šè‡ªåŠ¨ä½¿ç”¨æ¨¡æ¿
                elif disc_download and "image" not in disc_prompt.lower():
                    final_prompt += ". Also extract all image URLs."
                
                scraper = AaajiaoScraper()
                result = scraper.agent_search(
                    final_prompt, 
                    urls=selected_urls, 
                    max_credits=disc_credits,
                    extraction_level=extraction_level
                )
                
                if result:
                    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
                    cached = result.get("cached_count", 0)
                    new = result.get("new_count", len(result.get("data", [])) - cached)
                    if result.get("from_cache"):
                        st.success(f"âœ… å…¨éƒ¨ä»ç¼“å­˜è·å–ï¼èŠ‚çœ API è°ƒç”¨")
                    else:
                        st.success(f"âœ… æå–å®Œæˆï¼(ç¼“å­˜: {cached}, æ–°å¢: {new})")
                    
                    # === ç»„åˆè§†å›¾ ===
                    data_list = result.get("data", [])
                    if data_list:
                        # 1. è¡¨æ ¼æ¦‚è§ˆ
                        st.subheader("ğŸ“Š ç»“æœæ¦‚è§ˆ")
                        table_data = []
                        for item in data_list:
                            table_data.append({
                                "æ ‡é¢˜": item.get("title", "N/A"),
                                "å¹´ä»½": item.get("year", "N/A"),
                                "ç±»å‹": item.get("type", "N/A"),
                                "å›¾ç‰‡æ•°": len(item.get("high_res_images", item.get("images", [])) or [])
                            })
                        st.dataframe(table_data, use_container_width=True)
                        
                        # 2. è¯¦ç»†é¢„è§ˆï¼ˆå¯å±•å¼€ï¼‰
                        st.subheader("ğŸ–¼ï¸ è¯¦ç»†ä¿¡æ¯")
                        for i, item in enumerate(data_list):
                            title = item.get("title", f"Item {i+1}")
                            year = item.get("year", "")
                            with st.expander(f"**{title}** ({year})" if year else f"**{title}**"):
                                # æè¿°
                                desc = item.get("description_cn") or item.get("description_en") or item.get("description", "")
                                if desc:
                                    st.markdown(desc[:500] + ("..." if len(desc) > 500 else ""))
                                
                                # å›¾ç‰‡ç¼©ç•¥å›¾
                                images = item.get("high_res_images") or item.get("images") or []
                                if images:
                                    img_cols = st.columns(min(4, len(images)))
                                    for j, img_url in enumerate(images[:4]):
                                        try:
                                            img_cols[j].image(img_url, width=120)
                                        except:
                                            img_cols[j].markdown(f"[å›¾ç‰‡{j+1}]({img_url})")
                                
                                # è§†é¢‘é“¾æ¥
                                video = item.get("video_link")
                                if video:
                                    st.markdown(f"ğŸ¬ **è§†é¢‘:** [{video}]({video})")
                        
                        # 3. JSON ä¸‹è½½ï¼ˆæŠ˜å ï¼‰
                        with st.expander("ğŸ“¥ æŸ¥çœ‹åŸå§‹ JSON"):
                            st.json(result)
                    
                    if disc_download:
                        scraper.generate_agent_report(result, "agent_discovery_output", prompt=final_prompt, extraction_level=extraction_level)
                        st.info("ğŸ“„ Report generated at: `agent_discovery_output/`")
                else:
                    st.error("âŒ Task Failed / ä»»åŠ¡å¤±è´¥")


# Sidebar
with st.sidebar:
    st.markdown("### Console / æ§åˆ¶å°")
    st.markdown("---")
    st.markdown("**Modes / æ¨¡å¼è¯´æ˜ï¼š**")
    st.markdown("- **Basic**: Scrape Sitemap / æŠ“å–ç«™ç‚¹åœ°å›¾")
    st.markdown("- **Quick**: Single URL or AI / å¿«é€Ÿæå–")
    st.markdown("- **Batch**: Discovery -> Extract / æ‰¹é‡å‘ç°")
    st.markdown("---")
    if st.button("âŒ Exit App / é€€å‡ºç¨‹åº"):
        st.warning("Exiting... / ç¨‹åºé€€å‡º...")
        time.sleep(1)
        os._exit(0)

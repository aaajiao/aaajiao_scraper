import streamlit as st
import time
import pandas as pd
import json
import os
import re
import requests
from scraper import AaajiaoScraper
import concurrent.futures

# Page Config
st.set_page_config(
    page_title="aaajiao Scraper",
    page_icon="ğŸ¨",
    layout="wide"
)

# Title
st.title("ğŸ¨ aaajiao Portfolio Scraper / ä½œå“é›†æŠ“å–å·¥å…·")
st.markdown("Automated tool to scrape artwork details from eventstructure.com / è‡ªåŠ¨æŠ“å–å¹¶ç”Ÿæˆæ–‡æ¡£å·¥å…·")

# Initialize session state
if 'works' not in st.session_state:
    st.session_state.works = []
if 'scraping' not in st.session_state:
    st.session_state.scraping = False
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []

def run_scraper(incremental: bool = False):
    st.session_state.scraping = True
    st.session_state.log_messages = []
    
    # Reset or keep works based on incremental
    if not incremental:
        st.session_state.works = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        # 1. Get Links
        status_text.text("Fetching sitemap.xml...")
        links = scraper.get_all_work_links(incremental=incremental)
        total_links = len(links)
        
        if total_links == 0 and incremental:
             st.session_state.log_messages.append("No changes detected. / æ²¡æœ‰æ£€æµ‹åˆ°æ›´æ–°ã€‚")
             st.info("âœ… No new artworks found / æ²¡æœ‰å‘ç°æ–°ä½œå“")
             status_text.text("Done.")
             
             # Load existing cached data into session state so UI can display it
             try:
                 with open("aaajiao_works.json", "r", encoding="utf-8") as f:
                     st.session_state.works = json.load(f)
                     st.session_state.log_messages.append(f"ğŸ“¦ Loaded {len(st.session_state.works)} cached works")
             except FileNotFoundError:
                 pass
             
             st.session_state.scraping = False
             return

        st.session_state.log_messages.append(f"Found {total_links} new/updated links / æ‰¾åˆ° {total_links} ä¸ªéœ€æ›´æ–°é“¾æ¥")
        
        # 2. Concurrent Scrape
        if total_links > 0:
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                future_to_url = {executor.submit(scraper.extract_work_details, url): url for url in links}
                
                completed_count = 0
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    completed_count += 1
                    
                    try:
                        data = future.result()
                        if data:
                            st.session_state.works.append(data)
                            msg = f"[{completed_count}/{total_links}] Success: {data.get('title', 'Unknown')}"
                        else:
                            msg = f"[{completed_count}/{total_links}] Failed: {url}"
                            
                        st.session_state.log_messages.append(msg)
                        
                        # Update UI
                        progress = completed_count / total_links
                        progress_bar.progress(progress)
                        status_text.text(f"Scraping: {completed_count}/{total_links} / æ­£åœ¨æŠ“å–...")
                        
                        # Show logs
                        log_area.code("\n".join(st.session_state.log_messages[-5:]))
                        
                    except Exception as e:
                        st.session_state.log_messages.append(f"Error: {e}")
                    
                    # Auto-save every 5 items (with deduplication)
                    if completed_count % 5 == 0:
                        # Deduplicate by URL before saving
                        seen_urls = set()
                        unique_works = []
                        for w in st.session_state.works:
                            url = w.get('url', '')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                unique_works.append(w)
                        scraper.works = unique_works
                        scraper.save_to_json()
                        st.session_state.log_messages.append(f"ğŸ’¾ Auto-saved {len(unique_works)} items")

        # 3. Save Files (with final deduplication)
        status_text.text("Saving files... / æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        
        # Final deduplication by URL
        seen_urls = set()
        unique_works = []
        for w in st.session_state.works:
            url = w.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_works.append(w)
        
        st.session_state.works = unique_works  # Update session state with deduplicated list
        scraper.works = unique_works
        
        scraper.save_to_json()
        scraper.generate_markdown()
        
        st.success(f"Completed! Scraped {len(unique_works)} artworks. / æŠ“å–å®Œæˆï¼å…±è·å– {len(unique_works)} ä¸ªä½œå“ã€‚")
        st.balloons()
        
    except Exception as e:
        st.error(f"Error occurred: {str(e)} / å‘ç”Ÿé”™è¯¯")
    finally:
        st.session_state.scraping = False


# ============ Main Interface with Tabs ============

tab1, tab2 = st.tabs([
    "ğŸ—ï¸ Basic Scraper / åŸºç¡€çˆ¬è™«", 
    "ğŸ”„ Batch Update / æ‰¹é‡æ›´æ–°"
])

# ============ Tab 1: Basic Scraper (Original) ============
with tab1:
    col_u1, col_u2 = st.columns([1, 1])
    with col_u1:
        st.markdown("Click button below to scrape all artworks defined in `sitemap.xml` / ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æŠ“å–æ‰€æœ‰ä½œå“")
    
    with col_u2:
        incremental = st.checkbox("Incremental Update / å¢é‡æ›´æ–° (åªæŠ“å–æ–°é¡µé¢)", value=False, help="Based on sitemap 'lastmod' / åŸºäº sitemap çš„ lastmod æ£€æµ‹")
    
    if st.button("ğŸš€ Start Scraping / å¼€å§‹æŠ“å–", disabled=st.session_state.scraping, type="primary", key="scrape_btn"):
        run_scraper(incremental=incremental)

    # Results Area
    if st.session_state.works:
        st.divider()
        st.subheader("ğŸ“Š Preview / ç»“æœé¢„è§ˆ")
        
        df = pd.DataFrame(st.session_state.works)
        display_cols = ['title', 'title_cn', 'year', 'type', 'size', 'duration', 'url']
        cols_to_show = [c for c in display_cols if c in df.columns]
        st.dataframe(df[cols_to_show], use_container_width=True)
        
        st.divider()
        st.subheader("ğŸ“¥ Download / ä¸‹è½½æ–‡ä»¶")
        
        c1, c2 = st.columns(2)
        with c1:
            try:
                with open("aaajiao_works.json", "rb") as f:
                    st.download_button(
                        label="Download JSON / ä¸‹è½½ JSON æ•°æ®",
                        data=f,
                        file_name="aaajiao_works.json",
                        mime="application/json"
                    )
            except FileNotFoundError:
                st.warning("JSON file not found")
                
        with c2:
            try:
                with open("aaajiao_portfolio.md", "rb") as f:
                    st.download_button(
                        label="Download Markdown / ä¸‹è½½ Markdown æ–‡æ¡£",
                        data=f,
                        file_name="aaajiao_portfolio.md",
                        mime="text/markdown"
                    )
            except FileNotFoundError:
                st.warning("Markdown file not found")

    elif not st.session_state.scraping:
        st.info("Click the button above to start. / ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹è¿è¡Œã€‚")
    
    # ============ Image Enrichment Section ============

    st.divider()
    
    # Load cached works count
    scraper_preview = AaajiaoScraper()
    cached_works = scraper_preview.get_all_cached_works()
    
    if cached_works:
        st.success(f"ğŸ“¦ Found {len(cached_works)} cached works / å‘ç° {len(cached_works)} ä¸ªå·²ç¼“å­˜ä½œå“")
        st.subheader("ğŸ–¼ï¸ Image Enrichment / å›¾ç‰‡æ•´åˆ")
        
        st.markdown("""
        **ä»å·²ç¼“å­˜çš„ä½œå“æ•°æ®ä¸­æå–å›¾ç‰‡ (æ— éœ€ API)**
        - ä½¿ç”¨ HTML è§£ææå–æ¯ä¸ªä½œå“çš„é«˜æ¸…å›¾ç‰‡
        - å¯é€‰æ‹©ä¸‹è½½åˆ°æœ¬åœ°
        - ç”ŸæˆåŒ…å«å›¾ç‰‡çš„å®Œæ•´æŠ¥å‘Š
        """)
        
        # --- Feature 1: Image Enrichment (Download & Patch) ---
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            download_images_option = st.checkbox("ğŸ“¥ Download Images / ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°", value=True, key="enrich_download")
        with col_opt2:
            limit_works = st.slider("å¤„ç†æ•°é‡é™åˆ¶", min_value=1, max_value=len(cached_works), value=min(50, len(cached_works)), key="enrich_limit")
        
        if st.button("ğŸ–¼ï¸ Start Image Enrichment (Local) / å¼€å§‹å›¾ç‰‡æ•´åˆ", type="primary", key="enrich_btn"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            scraper = AaajiaoScraper()
            works_to_process = cached_works[:limit_works]
            enriched_works = []
            all_images = [] # Track for stats
            
            output_dir = "output/images" if download_images_option else "output"
            
            for i, work in enumerate(works_to_process):
                title = work.get("title", "Unknown")[:30]
                status_text.text(f"[{i+1}/{len(works_to_process)}] Processing: {title}...")
                
                try:
                    # Enrich works
                    enriched_work = scraper.enrich_work_with_images(
                        work, 
                        output_dir="output" 
                    )
                    enriched_works.append(enriched_work)
                    
                    if enriched_work.get("local_images"):
                         all_images.extend(enriched_work["local_images"])
                         
                except Exception as e:
                    st.warning(f"Failed: {title} - {e}")
                    enriched_works.append(work)
                
                progress_bar.progress((i + 1) / len(works_to_process))
            
            status_text.text("Generating report...")
            
            # Generate Markdown report
            report_lines = ["# aaajiao Portfolio with Images\n", f"*Generated: {time.strftime('%Y-%m-%d %H:%M')}*\n\n"]
            
            for work in enriched_works:
                title = work.get("title", "Untitled")
                title_cn = work.get("title_cn", "")
                year = work.get("year", "")
                url = work.get("url", "")
                desc_en = work.get("description_en", "")
                desc_cn = work.get("description_cn", "")
                images = work.get("images", [])
                local_images = work.get("local_images", [])
                
                report_lines.append(f"## {title}")
                if title_cn:
                    report_lines.append(f" / {title_cn}")
                report_lines.append(f"\n\n**Year:** {year}\n")
                report_lines.append(f"**URL:** [{url}]({url})\n\n")
                
                if desc_en:
                    report_lines.append(f"{desc_en}\n\n")
                if desc_cn:
                    report_lines.append(f"{desc_cn}\n\n")
                
                # Images section
                if images or local_images:
                    report_lines.append("### Images\n\n")
                    
                    imgs_to_show = images
                    use_local = bool(local_images)
                    
                    if use_local:
                        for img_path in local_images[:10]:
                             if "images/" in img_path:
                                rel_path = "images/" + img_path.split("images/", 1)[1]
                             else:
                                rel_path = os.path.basename(img_path)
                             report_lines.append(f"![Image]({rel_path})\n\n")     
                    else:
                        for img_url in images[:10]:
                            report_lines.append(f"![Image]({img_url})\n\n")
                
                report_lines.append("---\n\n")
            
            report_content = "".join(report_lines)
            
            # Save report
            os.makedirs("output", exist_ok=True)
            report_path = "output/portfolio_with_images.md"
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)
            
            st.success("âœ… Image Enrichment Complete! / å›¾ç‰‡æ•´åˆå®Œæˆ!")
            
            if download_images_option and all_images:
                 st.info(f"ğŸ“ Images saved to: `output/images/`")

            st.download_button(
                label="ğŸ“¥ Download Enriched Portfolio (With Local Images) / ä¸‹è½½å®Œæ•´å›¾æ–‡æŠ¥å‘Š (å«æœ¬åœ°å›¾)",
                data=report_content,
                file_name="aaajiao_portfolio_images.md",
                mime="text/markdown"
            )

        # --- Feature 2: Web Image Report (Lightweight) ---
        st.divider()
        st.subheader("ğŸŒ Web-Image Report / ç½‘ç»œå›¾ç‰‡æŠ¥å‘Š")
        st.markdown("ç”Ÿæˆä¸€ä»½ä»…åŒ…å«**åœ¨çº¿å›¾ç‰‡é“¾æ¥**çš„è½»é‡çº§æŠ¥å‘Šï¼Œæ— éœ€ä¸‹è½½å›¾ç‰‡ï¼Œä¾¿äºåˆ†äº«ã€‚")
        
        if st.button("ğŸ“„ Generate Web Report / ç”ŸæˆæŠ¥å‘Š", key="gen_web_report"):
            # Use cached_works directly since we are inside the if block
            works = cached_works
            
            # Sort
            def get_sort_year(w):
                y = w.get("year", "0000")
                if "-" in y: return y.split("-")[-1]
                return y
            
            works.sort(key=get_sort_year, reverse=True)
            
            lines = [
                "# aaajiao Portfolio (Web Images)\n", 
                f"> Generated: {time.strftime('%Y-%m-%d %H:%M')}\n",
                "> **Note**: Images are direct links to eventstructure.com\n\n",
                "---\n\n"
            ]
            
            progress = st.progress(0)
            status = st.empty()
            
            for i, work in enumerate(works):
                status.text(f"Processing {i+1}/{len(works)}...")
                progress.progress((i+1)/len(works))
                
                title = work.get("title", "Untitled")
                lines.append(f"## {work.get('year', '')} - {title}")
                if work.get('title_cn'):
                    lines.append(f" / {work['title_cn']}")
                lines.append("\n\n")
                
                lines.append(f"**URL:** [{work.get('url')}]({work.get('url')})\n\n")
                
                if work.get("description_cn"):
                    lines.append(f"> {work['description_cn']}\n\n")
                if work.get("description_en"):
                    lines.append(f"{work['description_en']}\n\n")
                    
                # Images logic
                imgs = work.get("images", [])
                if not imgs: imgs = work.get("high_res_images", [])
                
                # Fetch if missing
                if not imgs and work.get("url"):
                    try:
                        scraper_temp = AaajiaoScraper() # Need instance for method
                        imgs = scraper_temp.extract_images_from_page(work['url'])
                    except:
                        pass
                
                if imgs:
                    lines.append("### Images\n\n")
                    for img in imgs:
                         lines.append(f"![]({img})\n\n")
                
                lines.append("---\n")
            
            st.success(f"âœ… Generated report for {len(works)} works!")
            st.download_button(
                label="ğŸ“¥ Download Web Report / ä¸‹è½½ç½‘ç»œç‰ˆæŠ¥å‘Š",
                data="".join(lines),
                file_name="aaajiao_web_images_report.md",
                mime="text/markdown"
            )

    else:
        st.warning("âš ï¸ No cached works found. Run 'Start Scraping' first to cache artwork data.")


# ============ Tab 2: Batch Update (Size & Duration) ============
with tab2:
    st.markdown("""
    **æ‰¹é‡æ›´æ–°ä½œå“çš„å°ºå¯¸å’Œæ—¶é•¿ä¿¡æ¯ / Batch Update Size & Duration**
    
    ä½¿ç”¨ä½æˆæœ¬çš„ Firecrawl scrape æ¨¡å¼ï¼ˆçº¦ 1 Credit/é¡µï¼‰è·å–æ¸²æŸ“åçš„é¡µé¢å†…å®¹ï¼Œ
    ç„¶åæœ¬åœ°è§£ææå–å°ºå¯¸ï¼ˆsizeï¼‰å’Œæ—¶é•¿ï¼ˆdurationï¼‰ä¿¡æ¯ã€‚
    
    > ğŸ’¡ æ¯” AI Extract ä¾¿å®œ **50 å€**ï¼ï¼ˆ1 Credit vs 50 Creditsï¼‰
    """)
    
    # Load current data
    try:
        with open("aaajiao_works.json", "r", encoding="utf-8") as f:
            all_works = json.load(f)
    except FileNotFoundError:
        all_works = []
    
    if not all_works:
        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä½œå“æ•°æ®ï¼Œè¯·å…ˆåœ¨ Tab 1 è¿è¡ŒåŸºç¡€çˆ¬è™«")
    else:
        # Stats
        total = len(all_works)
        has_size = sum(1 for w in all_works if w.get('size'))
        has_duration = sum(1 for w in all_works if w.get('duration'))
        missing_size = total - has_size
        missing_duration = total - has_duration
        
        # Video works
        video_types = ['video', 'Video', 'video installation', 'Video Installation']
        video_works = [w for w in all_works if any(vt.lower() in (w.get('type', '') or '').lower() for vt in video_types)]
        video_with_duration = sum(1 for w in video_works if w.get('duration'))
        
        st.subheader("ğŸ“Š æ•°æ®ç»Ÿè®¡ / Data Statistics")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ€»ä½œå“æ•°", total)
        with col2:
            st.metric("æœ‰å°ºå¯¸ä¿¡æ¯", f"{has_size} ({has_size*100/total:.0f}%)", delta=f"-{missing_size} ç¼ºå¤±")
        with col3:
            st.metric("æœ‰æ—¶é•¿ä¿¡æ¯", f"{has_duration}", delta=f"è§†é¢‘ä½œå“: {len(video_works)}")
        
        st.divider()
        
        # ---- Feature 1: Batch Update ----
        st.subheader("ğŸ”„ æ‰¹é‡æ›´æ–° / Batch Update")
        st.markdown("ä½¿ç”¨ Firecrawl scrape è·å–æ¸²æŸ“åçš„é¡µé¢å†…å®¹ï¼Œæå–å°ºå¯¸å’Œæ—¶é•¿ä¿¡æ¯")
        
        # Options
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            update_limit = st.slider(
                "å¤„ç†æ•°é‡ / Limit", 
                min_value=1, 
                max_value=min(200, missing_size + missing_duration), 
                value=min(50, missing_size + missing_duration),
                help="æ¯ä¸ªä½œå“æ¶ˆè€—çº¦ 1 Credit"
            )
        with col_opt2:
            st.info(f"ğŸ’° é¢„è®¡æ¶ˆè€—: ~{update_limit} Credits")
        
        # Helper functions
        def load_api_key():
            try:
                with open('.env', 'r') as f:
                    for line in f:
                        if line.startswith('FIRECRAWL_API_KEY'):
                            return line.split('=')[1].strip()
            except:
                pass
            return os.getenv("FIRECRAWL_API_KEY", "")
        
        def scrape_markdown(url, api_key):
            payload = {"url": url, "formats": ["markdown"]}
            headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
            try:
                resp = requests.post("https://api.firecrawl.dev/v2/scrape", json=payload, headers=headers, timeout=30)
                if resp.status_code == 200:
                    return resp.json().get("data", {}).get("markdown", "")
                elif resp.status_code == 429:
                    time.sleep(3)
                    return scrape_markdown(url, api_key)
            except:
                pass
            return None
        
        def parse_size_duration(md):
            result = {"size": "", "duration": ""}
            if not md:
                return result
            
            lines = md[:2000].split('\n')
            
            # Size patterns
            for line in lines:
                line = line.strip()
                if result["size"]:
                    break
                for pattern in [
                    r'size\s+(\d+\s*[Ã—xX]\s*\d+(?:\s*[Ã—xX]\s*\d+)?\s*(?:cm|mm|m)?)',
                    r'(Dimension[s]?\s+variable\s*/\s*å°ºå¯¸å¯å˜)',
                    r'(Dimension[s]?\s+variable)',
                    r'^(å°ºå¯¸å¯å˜)$',
                ]:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        result["size"] = match.group(1).strip()
                        break
            
            # Duration patterns
            for line in lines:
                line = line.strip()
                if result["duration"]:
                    break
                for pattern in [
                    r"^(\d+['â€²]\d+['â€²''\"]*)\s*$",
                    r"^(\d+['â€²''\"]+)\s*$",
                    r"video\s+(\d+['â€²''\"]+)",
                    r"^(\d+:\d+(?::\d+)?)\s*$",
                ]:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        result["duration"] = match.group(1).strip()
                        break
            
            return result
        
        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡æ›´æ–° / Start Batch Update", type="primary", key="batch_update_btn"):
            api_key = load_api_key()
            if not api_key:
                st.error("âŒ æœªæ‰¾åˆ° FIRECRAWL_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
            else:
                # Filter works that need updating
                to_update = [w for w in all_works if not w.get('size') or not w.get('duration')][:update_limit]
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                log_area = st.empty()
                
                url_to_work = {w['url']: w for w in all_works}
                updated = 0
                logs = []
                
                for i, work in enumerate(to_update):
                    url = work.get('url')
                    title = work.get('title', 'Unknown')[:25]
                    
                    status_text.text(f"[{i+1}/{len(to_update)}] å¤„ç†: {title}...")
                    
                    md = scrape_markdown(url, api_key)
                    if md:
                        extracted = parse_size_duration(md)
                        changes = []
                        
                        if extracted['size'] and not work.get('size'):
                            url_to_work[url]['size'] = extracted['size']
                            changes.append(f"size='{extracted['size']}'")
                        
                        if extracted['duration'] and not work.get('duration'):
                            url_to_work[url]['duration'] = extracted['duration']
                            changes.append(f"duration='{extracted['duration']}'")
                        
                        if changes:
                            updated += 1
                            logs.append(f"âœ… {title}: {', '.join(changes)}")
                        else:
                            logs.append(f"âšª {title}: æ— æ–°æ•°æ®")
                    else:
                        logs.append(f"âŒ {title}: æŠ“å–å¤±è´¥")
                    
                    progress_bar.progress((i + 1) / len(to_update))
                    log_area.code("\n".join(logs[-8:]))
                    time.sleep(0.3)
                
                # Save
                with open("aaajiao_works.json", "w", encoding="utf-8") as f:
                    json.dump(all_works, f, ensure_ascii=False, indent=2)
                
                st.success(f"âœ… å®Œæˆï¼æ›´æ–°äº† {updated}/{len(to_update)} ä¸ªä½œå“")
                st.balloons()
                
                # Regenerate markdown
                scraper = AaajiaoScraper()
                scraper.works = all_works
                scraper.generate_markdown()
                st.info("ğŸ“„ Markdown æŠ¥å‘Šå·²é‡æ–°ç”Ÿæˆ")
        
        st.divider()
        
        # ---- Feature 2: Data Cleaning ----
        st.subheader("ğŸ§¹ æ•°æ®æ¸…æ´— / Data Cleaning")
        st.markdown("ä» `materials` å­—æ®µä¸­åˆ†ç¦»å‡ºæ··æ‚çš„å°ºå¯¸å’Œæ—¶é•¿ä¿¡æ¯")
        
        # Preview
        mixed_materials = [w for w in all_works if w.get('materials') and 
                          any(kw in w.get('materials', '').lower() for kw in ['dimension', 'size', 'cm', 'Ã—', 'variable', 'å°ºå¯¸'])]
        
        if mixed_materials:
            st.warning(f"âš ï¸ å‘ç° {len(mixed_materials)} ä¸ªä½œå“çš„ materials å­—æ®µå¯èƒ½åŒ…å«å°ºå¯¸ä¿¡æ¯")
            
            with st.expander("æŸ¥çœ‹å¯èƒ½éœ€è¦æ¸…æ´—çš„æ•°æ®"):
                for w in mixed_materials[:10]:
                    st.markdown(f"- **{w.get('title', 'Unknown')[:30]}**: `{w.get('materials', '')[:60]}...`")
            
            if st.button("ğŸ§¹ è¿è¡Œæ•°æ®æ¸…æ´— / Run Cleaning", key="clean_btn"):
                cleaned = 0
                for work in all_works:
                    old_materials = work.get('materials', '')
                    if not old_materials:
                        continue
                    
                    # Check if pure size
                    if re.match(r'^Dimension[s]?\s+variable\s*/?\s*å°ºå¯¸å¯å˜$', old_materials, re.IGNORECASE):
                        work['materials'] = ''
                        work['size'] = old_materials
                        cleaned += 1
                    elif re.match(r'^Dimension[s]?\s+variable$', old_materials, re.IGNORECASE):
                        work['materials'] = ''
                        work['size'] = old_materials
                        cleaned += 1
                    elif re.match(r'^å°ºå¯¸å¯å˜$', old_materials):
                        work['materials'] = ''
                        work['size'] = old_materials
                        cleaned += 1
                
                # Save
                with open("aaajiao_works.json", "w", encoding="utf-8") as f:
                    json.dump(all_works, f, ensure_ascii=False, indent=2)
                
                st.success(f"âœ… æ¸…æ´—å®Œæˆï¼ä¿®æ”¹äº† {cleaned} ä¸ªä½œå“")
        else:
            st.success("âœ… æ•°æ®å·²æ¸…æ´ï¼Œæ— éœ€æ¸…æ´—")
        
        st.divider()
        
        # ---- Preview Updated Data ----
        st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ / Data Preview")
        
        filter_option = st.radio(
            "ç­›é€‰ / Filter",
            ["å…¨éƒ¨", "æœ‰å°ºå¯¸", "æœ‰æ—¶é•¿", "ç¼ºå¤±å°ºå¯¸", "è§†é¢‘ä½œå“"],
            horizontal=True
        )
        
        filtered = all_works
        if filter_option == "æœ‰å°ºå¯¸":
            filtered = [w for w in all_works if w.get('size')]
        elif filter_option == "æœ‰æ—¶é•¿":
            filtered = [w for w in all_works if w.get('duration')]
        elif filter_option == "ç¼ºå¤±å°ºå¯¸":
            filtered = [w for w in all_works if not w.get('size')]
        elif filter_option == "è§†é¢‘ä½œå“":
            filtered = video_works
        
        if filtered:
            df = pd.DataFrame(filtered)
            display_cols = ['title', 'year', 'type', 'size', 'duration', 'materials']
            cols_to_show = [c for c in display_cols if c in df.columns]
            st.dataframe(df[cols_to_show].head(50), use_container_width=True)
            st.caption(f"æ˜¾ç¤º {min(50, len(filtered))}/{len(filtered)} æ¡")


# Sidebar
with st.sidebar:
    st.markdown("### Console / æ§åˆ¶å°")
    st.markdown("---")
    st.markdown("**Modes / æ¨¡å¼è¯´æ˜ï¼š**")
    st.markdown("- **Basic**: Scrape Sitemap / æŠ“å–ç«™ç‚¹åœ°å›¾")
    st.markdown("- **Update**: Size & Duration / æ›´æ–°å°ºå¯¸æ—¶é•¿")
    st.markdown("---")
    if st.button("âŒ Exit App / é€€å‡ºç¨‹åº"):
        st.warning("Exiting... / ç¨‹åºé€€å‡º...")
        time.sleep(1)
        os._exit(0)


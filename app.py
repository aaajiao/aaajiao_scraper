import streamlit as st
import time
import pandas as pd
import json
import os
from aaajiao_scraper import AaajiaoScraper
import concurrent.futures

# Page Config
st.set_page_config(
    page_title="aaajiao Scraper",
    page_icon="ğŸ¨",
    layout="wide"
)

# Title
st.title("ğŸ¨ aaajiao Portfolio Scraper / ä½œå“é›†æŠ“å–å·¥å…·")
st.markdown("Automated tool to scrape artwork details from eventstructure.com / è‡ªåŠ¨æŠ“å–å¹¶ç”Ÿæˆæ–‡æ¡£å·¥å…·")

# Initialize session state
if 'works' not in st.session_state:
    st.session_state.works = []
if 'scraping' not in st.session_state:
    st.session_state.scraping = False
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []
if 'agent_result' not in st.session_state:
    st.session_state.agent_result = None
if 'discovery_found_urls' not in st.session_state:
    st.session_state.discovery_found_urls = []
if 'discovery_urls' not in st.session_state:
    st.session_state.discovery_urls = []

def run_scraper():
    st.session_state.scraping = True
    st.session_state.works = []
    st.session_state.log_messages = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        # 1. Get Links
        status_text.text("Scanning homepage for links... / æ­£åœ¨è·å–ä½œå“åˆ—è¡¨...")
        st.session_state.log_messages.append("Scanning homepage... / æ­£åœ¨æ‰«æä¸»é¡µ...")
        links = scraper.get_all_work_links()
        total_links = len(links)
        st.session_state.log_messages.append(f"Found {total_links} artwork links / æ‰¾åˆ° {total_links} ä¸ªä½œå“é“¾æ¥")
        
        # 2. Concurrent Scrape
        if total_links > 0:
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                future_to_url = {executor.submit(scraper.extract_work_details, url): url for url in links}
                
                completed_count = 0
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    completed_count += 1
                    
                    try:
                        data = future.result()
                        if data:
                            st.session_state.works.append(data)
                            msg = f"[{completed_count}/{total_links}] Success: {data.get('title', 'Unknown')}"
                        else:
                            msg = f"[{completed_count}/{total_links}] Failed: {url}"
                            
                        st.session_state.log_messages.append(msg)
                        
                        # Update UI
                        progress = completed_count / total_links
                        progress_bar.progress(progress)
                        status_text.text(f"Scraping: {completed_count}/{total_links} / æ­£åœ¨æŠ“å–...")
                        
                        # Show logs
                        log_area.code("\n".join(st.session_state.log_messages[-5:]))
                        
                    except Exception as e:
                        st.session_state.log_messages.append(f"Error: {e}")

        # 3. Save Files
        status_text.text("Saving files... / æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
        scraper.works = st.session_state.works
        
        scraper.save_to_json()
        scraper.generate_markdown()
        
        st.success(f"Completed! Scraped {len(st.session_state.works)} artworks. / æŠ“å–å®Œæˆï¼å…±è·å– {len(st.session_state.works)} ä¸ªä½œå“ã€‚")
        st.balloons()
        
    except Exception as e:
        st.error(f"Error occurred: {str(e)} / å‘ç”Ÿé”™è¯¯")
    finally:
        st.session_state.scraping = False


def run_agent(prompt: str, urls: str, max_credits: int, download_images: bool = False):
    """Run Agent Search"""
    st.session_state.agent_result = None
    
    status_area = st.empty()
    result_area = st.empty()
    
    try:
        scraper = AaajiaoScraper()
        
        status_area.info("ğŸ¤– Starting Agent Task... / å¯åŠ¨ Agent ä»»åŠ¡...")
        
        # Parse URLs
        url_list = None
        if urls.strip():
            url_list = [u.strip() for u in urls.split(",") if u.strip()]
        
        # Enhanced prompt
        enhanced_prompt = prompt
        if download_images and "image" not in prompt.lower():
            enhanced_prompt = f"{prompt}. Also extract all image URLs from the page."
        
        # Call Agent
        result = scraper.agent_search(enhanced_prompt, urls=url_list, max_credits=max_credits)
        
        if result:
            st.session_state.agent_result = result
            status_area.success("âœ… Agent Task Completed! / Agent æŸ¥è¯¢å®Œæˆ!")
            result_area.json(result)
            
            # Generate Report
            if download_images:
                status_area.info("ğŸ“¥ Downloading images & generating report... / æ­£åœ¨ä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š...")
                scraper.generate_agent_report(result, "agent_output", prompt=enhanced_prompt)
                status_area.success("âœ… Report Generated! / æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        else:
            status_area.error("âŒ Agent Task Failed / Agent æŸ¥è¯¢å¤±è´¥")
            
    except Exception as e:
        status_area.error(f"Error: {str(e)}")


# ============ Main Interface with Tabs ============

tab1, tab2, tab3 = st.tabs(["ğŸ“‹ Batch Scrape / æ‰¹é‡æŠ“å–", "ğŸ¤– Agent Query / Agent æŸ¥è¯¢", "ğŸš€ Smart Discovery / æ™ºèƒ½å‘ç°"])

# ============ Tab 1: Batch Scrape ============
with tab1:
    st.markdown("Scrape all artwork details from Sitemap links. / ä» Sitemap è·å–æ‰€æœ‰ä½œå“é“¾æ¥å¹¶æŠ“å–ã€‚")
    
    if st.button("ğŸš€ Start Scraping / å¼€å§‹æŠ“å–", disabled=st.session_state.scraping, type="primary", key="scrape_btn"):
        run_scraper()

    # Results Area
    if st.session_state.works:
        st.divider()
        st.subheader("ğŸ“Š Preview / ç»“æœé¢„è§ˆ")
        
        df = pd.DataFrame(st.session_state.works)
        display_cols = ['title', 'title_cn', 'year', 'type', 'url']
        cols_to_show = [c for c in display_cols if c in df.columns]
        st.dataframe(df[cols_to_show], use_container_width=True)
        
        st.divider()
        st.subheader("ğŸ“¥ Download / ä¸‹è½½æ–‡ä»¶")
        
        c1, c2 = st.columns(2)
        with c1:
            try:
                with open("aaajiao_works.json", "rb") as f:
                    st.download_button(
                        label="Download JSON / ä¸‹è½½ JSON æ•°æ®",
                        data=f,
                        file_name="aaajiao_works.json",
                        mime="application/json"
                    )
            except FileNotFoundError:
                st.warning("JSON file not found")
                
        with c2:
            try:
                with open("aaajiao_portfolio.md", "rb") as f:
                    st.download_button(
                        label="Download Markdown / ä¸‹è½½ Markdown æ–‡æ¡£",
                        data=f,
                        file_name="aaajiao_portfolio.md",
                        mime="text/markdown"
                    )
            except FileNotFoundError:
                st.warning("Markdown file not found")

    elif not st.session_state.scraping:
        st.info("Click the button above to start. / ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹è¿è¡Œã€‚")


# ============ Tab 2: Agent Query ============
with tab2:
    st.markdown("""
    Use natural language to query Firecrawl Agent. / ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ æƒ³è¦çš„ä¿¡æ¯ã€‚
    
    **Example / ç¤ºä¾‹:**
    - "Find all video installations by aaajiao"
    - "Get complete information including all images"
    """)
    
    # Input Area
    prompt = st.text_area(
        "Query Prompt / æŸ¥è¯¢æè¿°",
        placeholder="e.g.: Get complete information about this artwork including all images",
        height=100
    )
    
    urls = st.text_input(
        "Specific URLs (Optional) / æŒ‡å®š URL (å¯é€‰)",
        placeholder="https://eventstructure.com/Absurd-Reality-Check"
    )
    
    col1, col2 = st.columns(2)
    with col1:
        max_credits = st.slider("Max Credits", min_value=10, max_value=100, value=50)
    with col2:
        download_images = st.checkbox("ğŸ“¥ Download Images & Report / ä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š", value=True)
    
    if st.button("ğŸ” Start Query / å¼€å§‹æŸ¥è¯¢", type="primary", key="agent_btn", disabled=not prompt.strip()):
        run_agent(prompt, urls, max_credits, download_images)
    
    # Show Results
    if st.session_state.agent_result:
        st.divider()
        st.subheader("ğŸ“‹ Results / æŸ¥è¯¢ç»“æœ")
        
        c1, c2 = st.columns(2)
        with c1:
            result_json = json.dumps(st.session_state.agent_result, ensure_ascii=False, indent=2)
            st.download_button(
                label="Download JSON / ä¸‹è½½ç»“æœ JSON",
                data=result_json,
                file_name="agent_result.json",
                mime="application/json"
            )
        
        with c2:
            report_path = "agent_output/artwork_report.md"
            if os.path.exists(report_path):
                with open(report_path, "rb") as f:
                    st.download_button(
                        label="Download Report / ä¸‹è½½ Markdown æŠ¥å‘Š",
                        data=f,
                        file_name="artwork_report.md",
                        mime="text/markdown"
                    )
        
        # Show Images
        images_dir = "agent_output/images"
        if os.path.exists(images_dir):
            images = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))]
            if images:
                st.subheader("ğŸ–¼ï¸ Downloaded Images / ä¸‹è½½çš„å›¾ç‰‡")
                cols = st.columns(min(len(images), 3))
                for i, img in enumerate(sorted(images)[:6]):
                    with cols[i % 3]:
                        st.image(os.path.join(images_dir, img), caption=img, use_container_width=True)


# ============ Tab 3: Smart Discovery ============
with tab3:
    st.markdown("""
    **Solve Infinite/Horizontal Scroll Issues / è§£å†³æ»šåŠ¨åŠ è½½é—®é¢˜**:
    1. **Scan / æ‰«æ**: Auto-scroll page to discover links.
    2. **filter / ç­›é€‰**: Select artworks to extract.
    3. **Extract / æå–**: Batch process with Agent.
    """)
    
    # Session State Init
    if 'discovery_urls' not in st.session_state:
        st.session_state.discovery_urls = []
        
    # --- Step 1: Scan ---
    st.subheader("1. Scan Page / æ‰«æé¡µé¢")
    
    col_url, col_mode = st.columns([3, 1])
    with col_url:
        discovery_url = st.text_input("Target URL / ç›®æ ‡ç½‘å€", value="https://eventstructure.com")
    with col_mode:
        scroll_mode = st.selectbox(
            "Scroll Strategy / æ»šåŠ¨ç­–ç•¥", 
            ["auto", "horizontal", "vertical"],
            index=0,
            help="Auto: Hybrid / æ··åˆ\nHorizontal: Gallery / ç”»å»Š\nVertical: Standard / å‚ç›´"
        )
    
    if st.button("ğŸ”­ Start Scanning / å¼€å§‹æ‰«æå‘ç°é“¾æ¥", type="primary"):
        with st.spinner(f"Scanning ({scroll_mode} mode)... / æ­£åœ¨æ‰«æ..."):
            scraper = AaajiaoScraper()
            found = scraper.discover_urls_with_scroll(discovery_url, scroll_mode=scroll_mode)
            st.session_state.discovery_urls = found
            st.session_state.discovery_selected_urls = [] # Reset selection
            
            if found:
                st.success(f"âœ… Scanning Complete! Found {len(found)} links / æ‰«æå®Œæˆï¼å‘ç° {len(found)} ä¸ªé“¾æ¥")
            else:
                st.error("âŒ No links found / æœªå‘ç°é“¾æ¥")

    # --- Step 2 & 3: Select & Extract ---
    if st.session_state.discovery_urls:
        st.divider()
        st.subheader("2. Filter & Extract / ç­›é€‰ä¸æå–")
        
        # Callback for Select All
        def toggle_all():
            new_state = st.session_state.select_all_chk
            for url in st.session_state.discovery_urls:
                st.session_state[f"chk_{url}"] = new_state

        # Select All Checkbox
        st.checkbox("Select All / å…¨é€‰", value=False, key="select_all_chk", on_change=toggle_all)
        
        # Link List
        selected_urls = []
        with st.expander("View Links / æŸ¥çœ‹é“¾æ¥åˆ—è¡¨", expanded=True):
            for url in st.session_state.discovery_urls:
                key = f"chk_{url}"
                if key not in st.session_state:
                    st.session_state[key] = False
                
                if st.checkbox(url, key=key):
                    selected_urls.append(url)
        
        st.write(f"Selected / å·²é€‰æ‹©: **{len(selected_urls)}** items")
        
        # Agent Config
        c1, c2 = st.columns(2)
        with c1:
            disc_prompt = st.text_area("Agent Prompt", value="Extract title, year, materials and description", height=70)
        with c2:
            disc_credits = st.slider("Max Credits (Total / æ€»è®¡)", 10, 500, 100, key="disc_slider")
            disc_download = st.checkbox("Download Images / ä¸‹è½½å›¾ç‰‡", value=True, key="disc_img")
            
        if st.button("ğŸ¤– Batch Extract / å¼€å§‹æ‰¹é‡æå–", disabled=len(selected_urls)==0, type="primary"):
            status_box = st.empty()
            with status_box.container():
                st.info("ğŸš€ Submitting Agent Task... / æ­£åœ¨æäº¤ Agent ä»»åŠ¡...")
                
                final_prompt = disc_prompt
                if disc_download and "image" not in disc_prompt.lower():
                    final_prompt += ". Also extract all image URLs."
                
                scraper = AaajiaoScraper()
                result = scraper.agent_search(final_prompt, urls=selected_urls, max_credits=disc_credits)
                
                if result:
                    st.success("âœ… Extraction Completed! / æå–å®Œæˆ!")
                    st.json(result)
                    
                    if disc_download:
                        scraper.generate_agent_report(result, "agent_discovery_output", prompt=final_prompt)
                        st.info("Report generated at: `agent_discovery_output/` / æŠ¥å‘Šå·²ç”Ÿæˆ")
                else:
                    st.error("âŒ Task Failed / ä»»åŠ¡å¤±è´¥")


# Sidebar
with st.sidebar:
    st.markdown("### Console / æ§åˆ¶å°")
    st.markdown("---")
    st.markdown("**Modes / æ¨¡å¼è¯´æ˜ï¼š**")
    st.markdown("- **Batch / æ‰¹é‡**: Scrape all / æŠ“å–æ‰€æœ‰")
    st.markdown("- **Agent**: AI Query / AI æŸ¥è¯¢")
    st.markdown("- **Discovery / æ™ºèƒ½å‘ç°**: Smart Scroll / æ™ºèƒ½æ»šåŠ¨")
    st.markdown("---")
    if st.button("âŒ Exit App / é€€å‡ºç¨‹åº"):
        st.warning("Exiting... / ç¨‹åºé€€å‡º...")
        time.sleep(1)
        os._exit(0)

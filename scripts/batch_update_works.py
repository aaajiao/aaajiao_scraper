#!/usr/bin/env python3
"""
æ‰¹é‡æ›´æ–°ä½œå“æ•°æ® - ä½¿ç”¨ä¸¤å±‚æ··åˆæå–ç­–ç•¥

è¿™ä¸ªè„šæœ¬ä½¿ç”¨ AaajiaoScraper çš„ extract_work_details_v2 æ–¹æ³•è¿›è¡Œæ‰¹é‡æå–ï¼Œ
é‡‡ç”¨ Layer 1 (BS4) + Layer 2 (Schema Extract) æ··åˆç­–ç•¥ã€‚

Usage:
    python scripts/batch_update_works.py --dry-run          # é¢„è§ˆæ¨¡å¼
    python scripts/batch_update_works.py --limit 10         # åªå¤„ç†å‰ 10 ä¸ª
    python scripts/batch_update_works.py                    # å¤„ç†æ‰€æœ‰ä½œå“
"""
import json
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scraper import AaajiaoScraper


def batch_update(
    input_file: str,
    output_file: str,
    limit: int = None,
    dry_run: bool = False,
    force: bool = False,
) -> int:
    """æ‰¹é‡æ›´æ–°ä½œå“æ•°æ®

    Args:
        input_file: è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
        limit: é™åˆ¶å¤„ç†æ•°é‡
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹
        force: å¼ºåˆ¶é‡æ–°æå–æ‰€æœ‰ä½œå“ï¼ˆå¿½ç•¥å·²æœ‰å­—æ®µï¼‰

    Returns:
        æ›´æ–°çš„ä½œå“æ•°é‡
    """
    # åŠ è½½ç°æœ‰æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        works = json.load(f)

    # åˆå§‹åŒ– scraper
    scraper = AaajiaoScraper(use_cache=True)

    # ç­›é€‰éœ€è¦æ›´æ–°çš„ä½œå“ï¼ˆç¼ºå¤±å…³é”®å­—æ®µçš„ï¼‰
    required_fields = ['size', 'duration', 'materials', 'description_en', 'credits']

    if force:
        to_update = works
    else:
        to_update = [
            w for w in works
            if sum(1 for f in required_fields if not w.get(f)) >= 2
        ]

    if limit:
        to_update = to_update[:limit]

    print(f"ğŸ“Š æ€»ä½œå“æ•°: {len(works)}")
    print(f"ğŸ“‹ éœ€è¦æ›´æ–°: {len(to_update)}")
    print(f"ğŸ’° é¢„è®¡æ¶ˆè€—: ~{len(to_update) * 30} Credits (æŒ‰ ~30 credits/page ä¼°ç®—)")
    print()

    if dry_run:
        print("[DRY RUN] ä»¥ä¸‹ä½œå“å°†è¢«æ›´æ–°:")
        for w in to_update[:10]:
            missing = [f for f in required_fields if not w.get(f)]
            print(f"  - {w.get('title', 'Unknown')[:40]}")
            print(f"    ç¼ºå¤±å­—æ®µ: {', '.join(missing)}")
        if len(to_update) > 10:
            print(f"  ... è¿˜æœ‰ {len(to_update) - 10} ä¸ª")
        return 0

    # åˆ›å»º URL åˆ° work çš„æ˜ å°„
    url_to_work = {w['url']: w for w in works}

    updated = 0
    errors = 0

    for i, work in enumerate(to_update, 1):
        url = work.get('url')
        title = work.get('title', 'Unknown')[:30]

        print(f"[{i}/{len(to_update)}] {title}...")

        # ä½¿ç”¨æ–°çš„ä¸¤å±‚æ··åˆç­–ç•¥æå–
        extracted = scraper.extract_work_details_v2(url)

        if extracted:
            # æ›´æ–°ä½œå“æ•°æ®
            changes = []
            for field in required_fields:
                if extracted.get(field) and not work.get(field):
                    url_to_work[url][field] = extracted[field]
                    value_preview = str(extracted[field])[:30]
                    changes.append(f"{field}='{value_preview}'")

            # åŒæ—¶æ›´æ–°å…¶ä»–å¯èƒ½æ”¹è¿›çš„å­—æ®µ
            for field in ['title_cn', 'description_cn', 'type']:
                if extracted.get(field) and not work.get(field):
                    url_to_work[url][field] = extracted[field]
                    changes.append(f"{field}")

            if changes:
                print(f"    âœ… æ›´æ–°: {', '.join(changes[:4])}")
                if len(changes) > 4:
                    print(f"       + {len(changes) - 4} æ›´å¤šå­—æ®µ")
                updated += 1
            else:
                print(f"    âšª æ— æ–°æ•°æ®")
        else:
            print(f"    âŒ æå–å¤±è´¥")
            errors += 1

        # æ¯ 20 ä¸ªä¿å­˜ä¸€æ¬¡è¿›åº¦
        if i % 20 == 0:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(works, f, ensure_ascii=False, indent=2)
            print(f"    ğŸ’¾ è¿›åº¦å·²ä¿å­˜ ({i}/{len(to_update)})")

    # æœ€ç»ˆä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(works, f, ensure_ascii=False, indent=2)

    print()
    print(f"âœ… å®Œæˆ! æ›´æ–°: {updated}, é”™è¯¯: {errors}")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")

    return updated


def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡æ›´æ–°ä½œå“ä¿¡æ¯ï¼ˆä¸¤å±‚æ··åˆç­–ç•¥ï¼‰')
    parser.add_argument('-i', '--input', default='aaajiao_works.json', help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('-o', '--output', default='aaajiao_works.json', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†æ•°é‡')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶æ›´æ–°æ‰€æœ‰ä½œå“')

    args = parser.parse_args()

    batch_update(
        args.input,
        args.output,
        limit=args.limit,
        dry_run=args.dry_run,
        force=args.force,
    )


if __name__ == "__main__":
    main()

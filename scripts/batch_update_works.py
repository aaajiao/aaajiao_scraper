#!/usr/bin/env python3
"""
æ‰¹é‡æ›´æ–°ä½œå“æ•°æ® - ä½¿ç”¨ Firecrawl scrape + markdown æ¨¡å¼

è¿™ä¸ªè„šæœ¬ä½¿ç”¨ä½æˆæœ¬çš„ scrape æ¨¡å¼ï¼ˆçº¦ 1 Credit/é¡µï¼‰è·å–æ¸²æŸ“åçš„ markdownï¼Œ
ç„¶åæœ¬åœ°è§£ææå–å°ºå¯¸ã€æ—¶é•¿ç­‰ä¿¡æ¯ã€‚

Usage:
    python scripts/batch_update_works.py --dry-run          # é¢„è§ˆæ¨¡å¼
    python scripts/batch_update_works.py --limit 10         # åªå¤„ç†å‰ 10 ä¸ª
    python scripts/batch_update_works.py                    # å¤„ç†æ‰€æœ‰ä½œå“
"""
import json
import re
import time
import argparse
import requests
from typing import Dict, Any, Optional, Tuple
from datetime import datetime


def load_api_key() -> str:
    """ä» .env æ–‡ä»¶åŠ è½½ API key"""
    with open('.env', 'r') as f:
        for line in f:
            if line.startswith('FIRECRAWL_API_KEY'):
                return line.split('=')[1].strip()
    raise ValueError("FIRECRAWL_API_KEY not found in .env")


def scrape_markdown(url: str, api_key: str) -> Optional[str]:
    """ä½¿ç”¨ Firecrawl scrape è·å–æ¸²æŸ“åçš„ markdownï¼ˆçº¦ 1 Creditï¼‰"""
    payload = {
        "url": url,
        "formats": ["markdown"],
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    try:
        resp = requests.post(
            "https://api.firecrawl.dev/v2/scrape",
            json=payload,
            headers=headers,
            timeout=30
        )
        
        if resp.status_code == 200:
            data = resp.json()
            return data.get("data", {}).get("markdown", "")
        elif resp.status_code == 429:
            print(f"    âš ï¸ Rate limited, waiting 5s...")
            time.sleep(5)
            return scrape_markdown(url, api_key)  # é‡è¯•
        else:
            print(f"    âŒ Error {resp.status_code}: {resp.text[:100]}")
            return None
    except Exception as e:
        print(f"    âŒ Exception: {e}")
        return None


def parse_work_from_markdown(md: str, url: str) -> Dict[str, str]:
    """ä» markdown ä¸­è§£æä½œå“çš„å°ºå¯¸ã€æ—¶é•¿ç­‰ä¿¡æ¯"""
    result = {
        "size": "",
        "duration": "",
        "type": "",
        "materials": "",
    }
    
    if not md:
        return result
    
    # ç®€åŒ–ï¼šç›´æ¥ä»æ•´ä¸ª markdown æ–‡æœ¬ä¸­æå–ä¿¡æ¯
    # å› ä¸ºæ¯ä¸ªé¡µé¢çš„ markdown å¼€å¤´å°±æ˜¯å½“å‰ä½œå“çš„ä¿¡æ¯
    
    # åªå–å‰ 2000 å­—ç¬¦ï¼ˆé€šå¸¸åŒ…å«ä¸»è¦ä½œå“ä¿¡æ¯ï¼‰
    text = md[:2000]
    lines = text.split('\n')
    
    # è§£æå°ºå¯¸ - åŒ¹é…å„ç§æ ¼å¼
    size_patterns = [
        r'size\s+(\d+\s*[Ã—xX]\s*\d+(?:\s*[Ã—xX]\s*\d+)?\s*(?:cm|mm|m)?)',  # size 280cm Ã— 102cm
        r'^(\d+\s*[Ã—xX]\s*\d+\s*[Ã—xX]\s*\d+\s*(?:cm|mm)?)$',  # 280 Ã— 102 Ã— 30 cm (ç‹¬ç«‹è¡Œ)
        r'(Dimension[s]?\s+variable\s*/\s*å°ºå¯¸å¯å˜)',  # å®Œæ•´åŒè¯­
        r'(Dimension[s]?\s+variable)',  # è‹±æ–‡
        r'^(å°ºå¯¸å¯å˜)$',  # ä¸­æ–‡ç‹¬ç«‹è¡Œ
    ]
    
    for line in lines:
        line = line.strip()
        if result["size"]:
            break
        for pattern in size_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                result["size"] = match.group(1).strip()
                break
    
    # è§£ææ—¶é•¿ - è§†é¢‘ä½œå“
    duration_patterns = [
        r"^(\d+['â€²]\d+['â€²''\"]*)\s*$",   # 6'34 æˆ– 12'00'' (ç‹¬ç«‹è¡Œ)
        r"^(\d+['â€²''\"]+)\s*$",           # 43'' (ç‹¬ç«‹è¡Œ)
        r"video\s+(\d+['â€²''\"]+)",        # video 43''
        r"^(\d+:\d+(?::\d+)?)\s*$",       # 12:00 (ç‹¬ç«‹è¡Œ)
    ]
    
    for line in lines:
        line = line.strip()
        if result["duration"]:
            break
        for pattern in duration_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                result["duration"] = match.group(1).strip()
                break
    
    # è§£æç±»å‹ï¼ˆé€šå¸¸åœ¨å¼€å¤´ 15 è¡Œå†…ï¼‰
    type_keywords = [
        'video installation', 'installation', 'video', 'website', 
        'software', 'performance', 'exhibition', 'single channel video',
        'è£…ç½®', 'å½•åƒè£…ç½®', 'å½•åƒ', 'ç½‘ç«™'
    ]
    
    for line in lines[:15]:
        line_lower = line.strip().lower()
        for kw in type_keywords:
            if line_lower == kw or line_lower.startswith(kw + ' ') or line_lower.startswith(kw + '/'):
                result["type"] = line.strip()
                break
        if result["type"]:
            break
    
    return result


def batch_update(
    input_file: str,
    output_file: str,
    limit: int = None,
    dry_run: bool = False
) -> int:
    """æ‰¹é‡æ›´æ–°ä½œå“æ•°æ®"""
    
    # åŠ è½½ç°æœ‰æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        works = json.load(f)
    
    api_key = load_api_key()
    
    # ç­›é€‰éœ€è¦æ›´æ–°çš„ä½œå“ï¼ˆæ²¡æœ‰ size å’Œ duration çš„ï¼‰
    to_update = [
        w for w in works 
        if not w.get('size') or not w.get('duration')
    ]
    
    if limit:
        to_update = to_update[:limit]
    
    print(f"ğŸ“Š æ€»ä½œå“æ•°: {len(works)}")
    print(f"ğŸ“‹ éœ€è¦æ›´æ–°: {len(to_update)}")
    print(f"ğŸ’° é¢„è®¡æ¶ˆè€—: ~{len(to_update)} Credits")
    print()
    
    if dry_run:
        print("[DRY RUN] ä»¥ä¸‹ä½œå“å°†è¢«æ›´æ–°:")
        for w in to_update[:10]:
            print(f"  - {w.get('title', 'Unknown')}: {w.get('url')}")
        if len(to_update) > 10:
            print(f"  ... è¿˜æœ‰ {len(to_update) - 10} ä¸ª")
        return 0
    
    # åˆ›å»º URL åˆ° work çš„æ˜ å°„
    url_to_work = {w['url']: w for w in works}
    
    updated = 0
    errors = 0
    
    for i, work in enumerate(to_update, 1):
        url = work.get('url')
        title = work.get('title', 'Unknown')[:30]
        
        print(f"[{i}/{len(to_update)}] {title}...")
        
        # æŠ“å– markdown
        md = scrape_markdown(url, api_key)
        
        if md:
            # è§£ææå–ä¿¡æ¯
            extracted = parse_work_from_markdown(md, url)
            
            # æ›´æ–°ä½œå“æ•°æ®
            changes = []
            if extracted['size'] and not work.get('size'):
                url_to_work[url]['size'] = extracted['size']
                changes.append(f"size='{extracted['size']}'")
            
            if extracted['duration'] and not work.get('duration'):
                url_to_work[url]['duration'] = extracted['duration']
                changes.append(f"duration='{extracted['duration']}'")
            
            if extracted['type'] and not work.get('type'):
                url_to_work[url]['type'] = extracted['type']
                changes.append(f"type='{extracted['type']}'")
            
            if changes:
                print(f"    âœ… æ›´æ–°: {', '.join(changes)}")
                updated += 1
            else:
                print(f"    âšª æ— æ–°æ•°æ®")
        else:
            print(f"    âŒ æŠ“å–å¤±è´¥")
            errors += 1
        
        # é¿å… rate limit
        time.sleep(0.5)
        
        # æ¯ 20 ä¸ªä¿å­˜ä¸€æ¬¡è¿›åº¦
        if i % 20 == 0:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(works, f, ensure_ascii=False, indent=2)
            print(f"    ğŸ’¾ è¿›åº¦å·²ä¿å­˜ ({i}/{len(to_update)})")
    
    # æœ€ç»ˆä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(works, f, ensure_ascii=False, indent=2)
    
    print()
    print(f"âœ… å®Œæˆ! æ›´æ–°: {updated}, é”™è¯¯: {errors}")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
    
    return updated


def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡æ›´æ–°ä½œå“çš„å°ºå¯¸å’Œæ—¶é•¿ä¿¡æ¯')
    parser.add_argument('-i', '--input', default='aaajiao_works.json', help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('-o', '--output', default='aaajiao_works.json', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†æ•°é‡')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    
    args = parser.parse_args()
    
    batch_update(
        args.input,
        args.output,
        limit=args.limit,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()

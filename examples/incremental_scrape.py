#!/usr/bin/env python3
"""
Incremental Scrape Example - å¢é‡çˆ¬å–ç¤ºä¾‹

åªå¤„ç†æ–°å¢æˆ–ä¿®æ”¹çš„ä½œå“ï¼ŒèŠ‚çœæ—¶é—´å’ŒAPIæ¶ˆè€—
"""

from scraper import AaajiaoScraper


def main():
    """å¢é‡çˆ¬å–ç¤ºä¾‹"""
    print("ğŸ”„ å¢é‡çˆ¬å–ç¤ºä¾‹\n")
    
    # åˆå§‹åŒ–ï¼ˆå¿…é¡»å¯ç”¨ç¼“å­˜ï¼‰
    scraper = AaajiaoScraper(use_cache=True)
    
    # 1. å¢é‡æ¨¡å¼è·å–URL
    # ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè·å–æ‰€æœ‰URL
    # ä¹‹åè¿è¡Œåªä¼šè·å–æ–°å¢æˆ–ä¿®æ”¹çš„URL
    print("ğŸ” æ£€æŸ¥æ›´æ–°...")
    work_urls = scraper.get_all_work_links(incremental=True)
    
    if not work_urls:
        print("   âœ… æ²¡æœ‰æ£€æµ‹åˆ°æ–°ä½œå“æˆ–æ›´æ–°")
        print("   ğŸ’¡ æç¤ºï¼šå¦‚æœéœ€è¦é‡æ–°çˆ¬å–ï¼Œåˆ é™¤ .cache/ ç›®å½•")
        return
    
    print(f"   ğŸ†• å‘ç° {len(work_urls)} ä¸ªæ–°å¢/æ›´æ–°çš„ä½œå“\n")
    
    # 2. ä»…æå–æ›´æ–°çš„ä½œå“ï¼ˆä½¿ç”¨ä¸¤å±‚æ··åˆç­–ç•¥ï¼‰
    print("ğŸ“¥ æå–æ›´æ–°çš„ä½œå“...")
    for i, url in enumerate(work_urls, 1):
        print(f"   [{i}/{len(work_urls)}] å¤„ç†ä¸­...")

        work_data = scraper.extract_work_details_v2(url)
        if work_data:
            title = work_data.get('title', 'Unknown')
            print(f"      âœ… {title}")
        else:
            print(f"      âŒ å¤±è´¥ï¼š{url}")
    
    # 3. ä¿å­˜ç»“æœ
    if scraper.works:
        print(f"\nğŸ’¾ ä¿å­˜ {len(scraper.works)} ä¸ªä½œå“...")
        
        # ä¿å­˜ä¸ºå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"output/incremental_{timestamp}.json"
        
        scraper.save_to_json(output_file)
        print(f"   âœ… å·²ä¿å­˜åˆ° {output_file}")
    
    # 4. ä½¿ç”¨æç¤º
    print("\nğŸ’¡ å¢é‡çˆ¬å–æç¤ºï¼š")
    print("   - å®šæœŸè¿è¡Œæ­¤è„šæœ¬ï¼Œåªä¼šå¤„ç†æ–°å†…å®¹")
    print("   - ç¼“å­˜æ–‡ä»¶ä½äº .cache/ ç›®å½•")
    print("   - å¦‚éœ€å®Œå…¨é‡æ–°çˆ¬å–ï¼Œåˆ é™¤ .cache/sitemap_lastmod.json")
    
    print("\nâœ¨ å®Œæˆï¼")


if __name__ == "__main__":
    main()

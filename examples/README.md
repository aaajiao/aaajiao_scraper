# ä½¿ç”¨ç¤ºä¾‹

æœ¬ç›®å½•åŒ…å« aaajiao Portfolio Scraper çš„å„ç§ä½¿ç”¨ç¤ºä¾‹ã€‚

## ğŸ“š ç¤ºä¾‹åˆ—è¡¨

### 1. å¿«é€Ÿå¼€å§‹ - `quick_start.py`

**é€‚åˆ**: ç¬¬ä¸€æ¬¡ä½¿ç”¨  
**å­¦ä¹ å†…å®¹**: åŸºæœ¬åˆå§‹åŒ–ã€å•ä¸ªä½œå“æå–ã€ç»“æœå¯¼å‡º

```bash
python examples/quick_start.py
```

**è¾“å‡º**:
- `output/quick_start_results.json` - JSONæ ¼å¼çš„ä½œå“æ•°æ®
- `output/quick_start_portfolio.md` - Markdownæ ¼å¼çš„ä½œå“é›†

---

### 2. æ‰¹é‡æå– - `batch_extraction.py`

**é€‚åˆ**: éœ€è¦é«˜æ•ˆå¤„ç†å¤šä¸ªä½œå“  
**å­¦ä¹ å†…å®¹**: æ‰¹é‡APIä½¿ç”¨ã€ç¼“å­˜ç­–ç•¥ã€è¿›åº¦è·Ÿè¸ª

```bash
python examples/batch_extraction.py
```

**ç‰¹ç‚¹**:
- ä½¿ç”¨ Firecrawl æ‰¹é‡æå–API
- è‡ªåŠ¨åˆ©ç”¨ç¼“å­˜å‡å°‘APIæ¶ˆè€—
- æ˜¾ç¤ºæå–ç»Ÿè®¡ï¼ˆç¼“å­˜å‘½ä¸­ã€æ–°æå–æ•°é‡ï¼‰

---

### 3. å¢é‡çˆ¬å– - `incremental_scrape.py`

**é€‚åˆ**: å®šæœŸæ›´æ–°æ•°æ®  
**å­¦ä¹ å†…å®¹**: å¢é‡æ¨¡å¼ã€sitemapæ¯”è¾ƒã€åªå¤„ç†æ›´æ–°

```bash
python examples/incremental_scrape.py
```

**å·¥ä½œåŸç†**:
1. ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè·å–æ‰€æœ‰ä½œå“
2. åç»­è¿è¡Œï¼šåªè·å–æ–°å¢æˆ–ä¿®æ”¹çš„ä½œå“ï¼ˆåŸºäºsitemapçš„lastmodï¼‰
3. èŠ‚çœæ—¶é—´å’ŒAPIæ¶ˆè€—

---

## ğŸ¯ ä½¿ç”¨å‰å‡†å¤‡

1. **é…ç½®API Key**
   ```bash
   cp ../.env.example ../.env
   # ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ FIRECRAWL_API_KEY
   ```

2. **åˆ›å»ºè¾“å‡ºç›®å½•**
   ```bash
   mkdir -p output
   ```

3. **è¿è¡Œç¤ºä¾‹**
   ```bash
   python examples/quick_start.py
   ```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### èŠ‚çœAPIæ¶ˆè€—

1. **å¯ç”¨ç¼“å­˜**ï¼ˆé»˜è®¤å·²å¯ç”¨ï¼‰
   ```python
   scraper = AaajiaoScraper(use_cache=True)
   ```

2. **ä½¿ç”¨å¢é‡æ¨¡å¼**
   ```python
   work_urls = scraper.get_all_work_links(incremental=True)
   ```

3. **é€‰æ‹©åˆé€‚çš„æå–çº§åˆ«**
   - `quick` - åŸºæœ¬ä¿¡æ¯ï¼Œæ¶ˆè€—æœ€å°‘
   - `full` - å®Œæ•´ä¿¡æ¯ï¼Œæ¶ˆè€—è¾ƒå¤š
   - `images_only` - ä»…å›¾ç‰‡ï¼Œé€‚ä¸­

### é”™è¯¯å¤„ç†

```python
work_data = scraper.extract_work_details(url)
if work_data:
    # å¤„ç†æ•°æ®
    print(f"æˆåŠŸï¼š{work_data['title']}")
else:
    # æå–å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé™åˆ¶ï¼‰
    print(f"å¤±è´¥ï¼š{url}")
```

### æŸ¥çœ‹æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.INFO)  # æˆ– DEBUG æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
```

---

## ğŸ”§ è‡ªå®šä¹‰ç¤ºä¾‹

åŸºäºè¿™äº›ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥è½»æ¾åˆ›å»ºè‡ªå·±çš„è„šæœ¬ï¼š

```python
from scraper import AaajiaoScraper

# åˆå§‹åŒ–
scraper = AaajiaoScraper(use_cache=True)

# è‡ªå®šä¹‰å¤„ç†é€»è¾‘
work_urls = scraper.get_all_work_links()
for url in work_urls:
    # ä½ çš„å¤„ç†é€»è¾‘
    pass
```

---

## ğŸ“– æ›´å¤šèµ„æº

- **APIæ–‡æ¡£**: æŸ¥çœ‹ `scraper/` ç›®å½•ä¸‹å„æ¨¡å—çš„æ–‡æ¡£å­—ç¬¦ä¸²
- **æµ‹è¯•ç”¨ä¾‹**: `tests/` ç›®å½•åŒ…å«æ›´å¤šä½¿ç”¨ç¤ºä¾‹
- **ä¸»README**: `../README.md` æœ‰å®Œæ•´çš„åŠŸèƒ½è¯´æ˜
- **è´¡çŒ®æŒ‡å—**: `../CONTRIBUTING.md` äº†è§£å¼€å‘æµç¨‹

---

## â“ å¸¸è§é—®é¢˜

**Q: ç¤ºä¾‹è¿è¡Œå¤±è´¥ï¼Ÿ**  
A: ç¡®ä¿å·²é…ç½® `.env` æ–‡ä»¶ï¼Œå¹¶ä¸” API key æœ‰æ•ˆ

**Q: å¦‚ä½•å¤„ç†å¤§é‡ä½œå“ï¼Ÿ**  
A: ä½¿ç”¨ `batch_extraction.py`ï¼Œå¹¶è€ƒè™‘åˆ†æ‰¹å¤„ç†

**Q: å¦‚ä½•æ¸…é™¤ç¼“å­˜é‡æ–°çˆ¬å–ï¼Ÿ**  
A: åˆ é™¤ `.cache/` ç›®å½•

**Q: å¯ä»¥å¹¶å‘æå–å—ï¼Ÿ**  
A: å½“å‰ç‰ˆæœ¬æœ‰é€Ÿç‡é™åˆ¶ä¿æŠ¤ï¼Œæœªæ¥ç‰ˆæœ¬ä¼šæ”¯æŒasyncå¹¶å‘

#!/usr/bin/env python3
"""
Batch Extraction Example - æ‰¹é‡æå–ç¤ºä¾‹

ä½¿ç”¨ Firecrawl çš„æ‰¹é‡æå– APIï¼Œé«˜æ•ˆå¤„ç†å¤šä¸ªURL
"""

from scraper import AaajiaoScraper


def main():
    """æ‰¹é‡æå–ä½œå“ä¿¡æ¯"""
    print("ğŸš€ æ‰¹é‡æå–ç¤ºä¾‹\n")
    
    # åˆå§‹åŒ–
    scraper = AaajiaoScraper(use_cache=True)
    
    # 1. è·å–æ‰€æœ‰ä½œå“URL
    print("ğŸ“‹ è·å–ä½œå“åˆ—è¡¨...")
    work_urls = scraper.get_all_work_links(incremental=False)
    print(f"   æ‰¾åˆ° {len(work_urls)} ä¸ªä½œå“\n")
    
    # 2. æ‰¹é‡æå–ï¼ˆä½¿ç”¨ agent_searchï¼‰
    # è¿™æ¯”é€ä¸ªè°ƒç”¨ extract_work_details æ›´é«˜æ•ˆ
    print("ğŸ”„ æ‰¹é‡æå–ä¸­...")
    print("   æå–çº§åˆ«ï¼šQuickï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
    print("   å¯ç”¨ç¼“å­˜ï¼šæ˜¯\n")
    
    result = scraper.agent_search(
        prompt="æå–æ‰€æœ‰ä½œå“çš„åŸºæœ¬ä¿¡æ¯ï¼šæ ‡é¢˜ã€å¹´ä»½ã€ç±»å‹",
        urls=work_urls[:10],  # å…ˆå¤„ç†å‰10ä¸ªä½œä¸ºç¤ºä¾‹
        extraction_level="quick"
    )
    
    # 3. æŸ¥çœ‹ç»“æœ
    if result and "data" in result:
        extracted_works = result["data"]
        print(f"âœ… æˆåŠŸæå– {len(extracted_works)} ä¸ªä½œå“")
        print(f"ğŸ“Š ç¼“å­˜å‘½ä¸­ï¼š{result.get('cached_count', 0)} ä¸ª")
        print(f"ğŸ†• æ–°æå–ï¼š{result.get('new_count', 0)} ä¸ª")
        print(f"ğŸ’° API æ¶ˆè€—ï¼š{result.get('creditsUsed', 'N/A')} credits\n")
        
        # æ˜¾ç¤ºå‰3ä¸ªä½œå“
        print("ğŸ“ ç¤ºä¾‹ä½œå“ï¼š")
        for i, work in enumerate(extracted_works[:3], 1):
            print(f"   {i}. {work.get('title', 'Unknown')} ({work.get('year', 'N/A')})")
        
        # 4. ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
        scraper.works = extracted_works
        scraper.save_to_json("output/batch_extraction_results.json")
        print("   âœ… å·²ä¿å­˜åˆ° output/batch_extraction_results.json")
    else:
        print("âŒ æ‰¹é‡æå–å¤±è´¥")
    
    print("\nâœ¨ å®Œæˆï¼")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
aaajiao ä½œå“é›†çˆ¬è™« (Optimized v3 - Firecrawl Edition)
ä» https://eventstructure.com/ æŠ“å–æ‰€æœ‰ä½œå“è¯¦ç»†ä¿¡æ¯

v3 æ”¹è¿›ï¼š
1. ä½¿ç”¨ Firecrawl AI æå–ç»“æ„åŒ–æ•°æ® (ç²¾å‡†åº¦å¤§å¹…æå‡)
2. API Key å®‰å…¨ç®¡ç† (ç¯å¢ƒå˜é‡)
3. æ™ºèƒ½é€Ÿç‡æ§åˆ¶ (é¿å… Rate Limit)
4. æœ¬åœ°ç¼“å­˜ (èŠ‚çœ API è°ƒç”¨)
5. å®æ—¶è¿›åº¦æ¡ (ç”¨æˆ·å‹å¥½)
"""

import os
import sys
import time
import re
import json
import logging
import hashlib
import pickle
import concurrent.futures
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin
import argparse
from threading import Lock
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


class RateLimiter:
    """çº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶å™¨"""
    def __init__(self, calls_per_minute: int = 5):
        self.interval = 60.0 / calls_per_minute
        self.last_call = 0
        self.lock = Lock()
    
    def wait(self):
        """ç­‰å¾…ç›´åˆ°å…è®¸ä¸‹ä¸€æ¬¡è°ƒç”¨"""
        with self.lock:
            elapsed = time.time() - self.last_call
            if elapsed < self.interval:
                sleep_time = self.interval - elapsed
                logger.debug(f"Rate limit: sleeping {sleep_time:.2f}s")
                time.sleep(sleep_time)
            self.last_call = time.time()

class AaajiaoScraper:
    BASE_URL = "https://eventstructure.com"
    SITEMAP_URL = "https://eventstructure.com/sitemap.xml"
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    MAX_WORKERS = 2  # é™ä½å¹¶å‘æ•°ï¼Œé…åˆé€Ÿç‡æ§åˆ¶
    TIMEOUT = 15
    FC_TIMEOUT = 30  # Firecrawl ä¸“ç”¨è¶…æ—¶

    def __init__(self, use_cache: bool = True):
        self.session = self._create_retry_session()
        self.works: List[Dict[str, Any]] = []
        self.use_cache = use_cache
        
        # åŠ è½½ API Key
        self.firecrawl_key = self._load_api_key()
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨ (5 calls/min)
        self.rate_limiter = RateLimiter(calls_per_minute=5)
        
        logger.info(f"Scraper åˆå§‹åŒ–å®Œæˆ (ç¼“å­˜: {'å¼€å¯' if use_cache else 'å…³é—­'})")
    
    def _load_api_key(self) -> str:
        """ä»ç¯å¢ƒå˜é‡æˆ– .env æ–‡ä»¶åŠ è½½ API Key"""
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
        key = os.getenv("FIRECRAWL_API_KEY")
        
        # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•è¯»å– .env æ–‡ä»¶
        if not key:
            env_file = os.path.join(os.path.dirname(__file__), '.env')
            if os.path.exists(env_file):
                with open(env_file, 'r') as f:
                    for line in f:
                        if line.startswith('FIRECRAWL_API_KEY='):
                            key = line.split('=', 1)[1].strip()
                            break
        
        if not key:
            raise ValueError(
                "æœªæ‰¾åˆ° Firecrawl API Keyï¼\n"
                "è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export FIRECRAWL_API_KEY='your-key'\n"
                "æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶"
            )
        
        logger.info(f"API Key åŠ è½½æˆåŠŸ (é•¿åº¦: {len(key)})")
        return key

    def _create_retry_session(self, retries: int = 3, backoff_factor: float = 0.5) -> requests.Session:
        session = requests.Session()
        retry = Retry(
            total=retries,
            read=retries,
            connect=retries,
            backoff_factor=backoff_factor,
            status_forcelist=[500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.headers.update(self.HEADERS)
        return session

    def get_all_work_links(self) -> List[str]:
        """ä» Sitemap è·å–æ‰€æœ‰ä½œå“é“¾æ¥"""
        logger.info(f"æ­£åœ¨è¯»å– Sitemap: {self.SITEMAP_URL}")
        try:
            response = self.session.get(self.SITEMAP_URL, timeout=self.TIMEOUT)
            response.raise_for_status()
            
            # ç®€å•çš„ XML è§£æ (é¿å…å¼•å…¥ lxml ä¾èµ–)
            soup = BeautifulSoup(response.content, 'html.parser') # xml parser needs lxml usually, html.parser handles basic tags ok
            
            links = []
            for loc in soup.find_all('loc'):
                url = loc.get_text().strip()
                if self._is_valid_work_link(url):
                    links.append(url)
            
            # å»é‡
            links = sorted(list(set(links)))
            logger.info(f"Sitemap ä¸­æ‰¾åˆ° {len(links)} ä¸ªæœ‰æ•ˆä½œå“é“¾æ¥")
            return links
            
        except Exception as e:
            logger.error(f"Sitemap è¯»å–å¤±è´¥: {e}")
            # Fallback to main page scan if sitemap fails
            return self._fallback_scan_main_page()

    def _fallback_scan_main_page(self):
        """å¤‡ç”¨æ–¹æ¡ˆï¼šä»ä¸»é¡µæ‰«æé“¾æ¥"""
        logger.info("å°è¯•æ‰«æä¸»é¡µé“¾æ¥ (å¤‡ç”¨æ–¹æ¡ˆ)...")
        try:
            r = self.session.get(self.BASE_URL, timeout=self.TIMEOUT)
            soup = BeautifulSoup(r.text, 'html.parser')
            links = set()
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith('/'):
                    full = urljoin(self.BASE_URL, href)
                    if self._is_valid_work_link(full):
                        links.add(full)
            return list(links)
        except Exception as e:
            logger.error(f"ä¸»é¡µæ‰«æå¤±è´¥: {e}")
            return []

    def _is_valid_work_link(self, url: str) -> bool:
        """è¿‡æ»¤éä½œå“é“¾æ¥"""
        if not url.startswith(self.BASE_URL):
            return False
            
        path = url.replace(self.BASE_URL, '')
        
        # æ’é™¤åˆ—è¡¨
        excludes = [
            '/', '/rss', '/feed', '/filter', '/aaajiao', 
            '/contact', '/cv', '/about', '/index', '/sitemap'
        ]
        
        if path in ['/', '']: return False
        
        for ex in excludes:
            if ex in path and len(path) < 20: # simple heuristic
                if path == ex or path.startswith(ex + '/'):
                    return False
        
        # Cargo ç‰¹æ€§: å¾€å¾€ä½œå“é“¾æ¥éƒ½å¾ˆçŸ­ï¼Œæˆ–è€…åŒ…å«ç‰¹å®šå…³é”®è¯
        # è¿™é‡Œä¸»è¦æ’é™¤ filter é¡µé¢
        if '/filter/' in path: return False
        
        return True

    def extract_work_details(self, url: str, retry_count: int = 0) -> Optional[Dict[str, Any]]:
        """æå–è¯¦æƒ… (ä½¿ç”¨ Firecrawl AI æå–ï¼Œå¸¦ç¼“å­˜å’Œé‡è¯•)"""
        max_retries = 3
        
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            cached = self._load_cache(url)
            if cached:
                logger.debug(f"å‘½ä¸­ç¼“å­˜: {url}")
                return cached
        
        # 2. é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait()
        
        try:
            logger.info(f"[{retry_count+1}/{max_retries}] æ­£åœ¨æŠ“å–: {url}")
            
            fc_endpoint = "https://api.firecrawl.dev/v1/scrape"
            
            schema = {
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "The English title of the work"},
                    "title_cn": {"type": "string", "description": "The Chinese title of the work. If not explicitly found, leave empty."},
                    "year": {"type": "string", "description": "Creation year or year range (e.g. 2018-2022)"},
                    "type": {"type": "string", "description": "The art category (e.g. Video Installation, Software, Website)"},
                    "materials": {"type": "string", "description": "Materials list (e.g. LED screen, 3D printing)"},
                    "description_en": {"type": "string", "description": "Detailed work description in English. Exclude navigation text."},
                    "description_cn": {"type": "string", "description": "Detailed work description in Chinese. Exclude navigation text."},
                    "video_link": {"type": "string", "description": "Vimeo URL if present"}
                },
                "required": ["title"]
            }
            
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {
                    "schema": schema,
                    "systemPrompt": "You are an art archivist. Extract the artwork metadata from the portfolio page. Ignore navigation links like 'Previous/Next project'. The title usually appears as 'English Title / Chinese Title'. Separate them."
                }
            }
            
            headers = {
                "Authorization": f"Bearer {self.firecrawl_key}",
                "Content-Type": "application/json"
            }
            
            resp = requests.post(fc_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
            
            if resp.status_code == 200:
                data = resp.json()
                if 'data' in data and 'extract' in data['data']:
                    result = data['data']['extract']
                    
                    work = {
                        'url': url,
                        'title': result.get('title', ''),
                        'title_cn': result.get('title_cn', ''),
                        'type': result.get('type', ''),
                        'materials': result.get('materials', ''),
                        'year': result.get('year', ''),
                        'description_cn': result.get('description_cn', ''),
                        'description_en': result.get('description_en', ''),
                        'video_link': result.get('video_link', ''),
                        'size': '',
                        'duration': '',
                        'tags': []
                    }
                    
                    # åå¤„ç†ï¼šå¦‚æœ AI æ²¡åˆ†æ¸…æ ‡é¢˜
                    if not work['title_cn'] and '/' in work['title']:
                        parts = work['title'].split('/')
                        work['title'] = parts[0].strip()
                        if len(parts) > 1:
                            work['title_cn'] = parts[1].strip()
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    if self.use_cache:
                        self._save_cache(url, work)
                            
                    return work
                else:
                    logger.error(f"Firecrawl è¿”å›æ ¼å¼å¼‚å¸¸: {data}")
                    
            elif resp.status_code == 429:
                # Rate Limit - æŒ‡æ•°é€€é¿é‡è¯•
                if retry_count >= max_retries:
                    logger.error(f"é‡è¯•æ¬¡æ•°è¶…é™: {url}")
                    return None
                wait_time = 2 ** retry_count  # 1s, 2s, 4s
                logger.warning(f"Rate Limitï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                time.sleep(wait_time)
                return self.extract_work_details(url, retry_count + 1)
                
            else:
                logger.error(f"Firecrawl Error {resp.status_code}: {resp.text[:200]}")
                
            return None

        except Exception as e:
            logger.error(f"API è¯·æ±‚é”™è¯¯ {url}: {e}")
            return None
    
    # ==================== ç¼“å­˜ç³»ç»Ÿ ====================
    
    def _get_cache_path(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_dir = os.path.join(os.path.dirname(__file__), '.cache')
        return os.path.join(cache_dir, f"{url_hash}.pkl")
    
    def _load_cache(self, url: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                pass
        return None
    
    def _save_cache(self, url: str, data: Dict):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        cache_dir = os.path.dirname(cache_path)
        os.makedirs(cache_dir, exist_ok=True)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.debug(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # ==================== æ•°æ®éªŒè¯ ====================
    
    def validate_work(self, work: Dict) -> bool:
        """éªŒè¯ä½œå“æ•°æ®å®Œæ•´æ€§"""
        if not work.get('title'):
            logger.warning(f"ä½œå“ç¼ºå°‘æ ‡é¢˜: {work.get('url')}")
            return False
        return True

    def scrape_all(self):
        """æŠ“å–æ‰€æœ‰ä½œå“ï¼ˆå¸¦è¿›åº¦æ¡å’ŒéªŒè¯ï¼‰"""
        work_links = self.get_all_work_links()
        total = len(work_links)
        valid_count = 0
        failed_count = 0
        
        logger.info(f"å¼€å§‹æŠ“å– {total} ä¸ªä½œå“...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self.extract_work_details, url): url for url in work_links}
            
            # ä½¿ç”¨ tqdm è¿›åº¦æ¡
            for future in tqdm(concurrent.futures.as_completed(future_to_url), 
                               total=total, 
                               desc="æŠ“å–è¿›åº¦",
                               unit="ä½œå“"):
                url = future_to_url[future]
                try:
                    data = future.result()
                    if data and self.validate_work(data):
                        self.works.append(data)
                        valid_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
                    failed_count += 1

        logger.info(f"æŠ“å–å®Œæˆï¼æœ‰æ•ˆ: {valid_count}/{total}, å¤±è´¥: {failed_count}")
        return self.works

    def save_to_json(self, filename: str = 'aaajiao_works.json'):
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.works, f, ensure_ascii=False, indent=2)

    def generate_markdown(self, filename: str = 'aaajiao_portfolio.md'):
        """ç”Ÿæˆ Markdown æ ¼å¼çš„ä½œå“é›†æ–‡æ¡£"""
        lines = [
            "# aaajiao ä½œå“é›† / aaajiao Portfolio\n",
            f"Source: {self.BASE_URL}\n",
            "Generated by aaajiao Scraper v3 (Firecrawl Edition)\n",
            "\n---\n\n"
        ]
        
        # Sort by year
        sorted_works = sorted(self.works, key=lambda x: x.get('year') or '0000', reverse=True)
        
        current_year = None
        for work in sorted_works:
            year = work.get('year', 'Unknown')
            if year != current_year:
                lines.append(f"## {year}\n\n")
                current_year = year
                
            title = work.get('title', 'Untitled')
            title_cn = work.get('title_cn', '')
            
            header = f"### [{title}]({work['url']})"
            if title_cn:
                header += f" / {title_cn}"
            lines.append(header + "\n\n")
            
            if work.get('type'): 
                lines.append(f"**Type**: {work['type']}\n\n")
            if work.get('materials'):
                lines.append(f"**Materials**: {work['materials']}\n\n")
            if work.get('video_link'): 
                lines.append(f"**Video**: {work['video_link']}\n\n")
            
            if work.get('description_cn'):
                lines.append(f"> {work['description_cn']}\n\n")
                
            if work.get('description_en'):
                lines.append(f"{work['description_en']}\n\n")
                 
            lines.append("---\n")
            
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("".join(lines))
        
        logger.info(f"Markdown æ–‡ä»¶å·²ç”Ÿæˆ: {filename}")

    # ==================== Agent æ¨¡å¼ ====================
    
    def agent_search(self, prompt: str, urls: Optional[List[str]] = None, max_credits: int = 50) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨ Firecrawl Agent è¿›è¡Œå¼€æ”¾å¼æŸ¥è¯¢
        
        Args:
            prompt: æŸ¥è¯¢æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            urls: å¯é€‰ï¼ŒæŒ‡å®šè¦æœç´¢çš„ URL åˆ—è¡¨
            max_credits: æœ€å¤§æ¶ˆè€— credits æ•°ï¼ˆæ§åˆ¶æˆæœ¬ï¼‰
            
        Returns:
            Agent è¿”å›çš„ç»“æ„åŒ–æ•°æ®
        """
        logger.info(f"ğŸ¤– å¯åŠ¨ Agent ä»»åŠ¡...")
        logger.info(f"   Prompt: {prompt}")
        if urls:
            logger.info(f"   URLs: {urls}")
        
        agent_endpoint = "https://api.firecrawl.dev/v2/agent"
        
        headers = {
            "Authorization": f"Bearer {self.firecrawl_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "prompt": prompt,
            "maxCredits": max_credits
        }
        
        if urls:
            payload["urls"] = urls
        
        try:
            # 1. å¯åŠ¨ Agent ä»»åŠ¡
            resp = requests.post(agent_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
            
            if resp.status_code != 200:
                logger.error(f"Agent å¯åŠ¨å¤±è´¥: {resp.status_code} - {resp.text[:200]}")
                return None
            
            result = resp.json()
            
            if not result.get("success"):
                logger.error(f"Agent å¯åŠ¨å¤±è´¥: {result}")
                return None
            
            job_id = result.get("id")
            if not job_id:
                # åŒæ­¥æ¨¡å¼ï¼šç›´æ¥è¿”å›ç»“æœ
                if result.get("status") == "completed":
                    logger.info(f"âœ… Agent ä»»åŠ¡å®Œæˆ (credits: {result.get('creditsUsed', 'N/A')})")
                    return result.get("data")
                logger.error(f"Agent è¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                return None
            
            # 2. è½®è¯¢ç­‰å¾…ä»»åŠ¡å®Œæˆ
            logger.info(f"   ä»»åŠ¡ ID: {job_id}")
            status_endpoint = f"{agent_endpoint}/{job_id}"
            max_wait = 300  # æœ€é•¿ç­‰å¾… 5 åˆ†é’Ÿ
            poll_interval = 5  # æ¯ 5 ç§’æŸ¥è¯¢ä¸€æ¬¡
            elapsed = 0
            
            while elapsed < max_wait:
                time.sleep(poll_interval)
                elapsed += poll_interval
                
                status_resp = requests.get(status_endpoint, headers=headers, timeout=self.FC_TIMEOUT)
                
                if status_resp.status_code != 200:
                    logger.warning(f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {status_resp.status_code}")
                    continue
                
                status_data = status_resp.json()
                status = status_data.get("status")
                
                if status == "processing":
                    logger.info(f"   â³ å¤„ç†ä¸­... ({elapsed}s)")
                    continue
                elif status == "completed":
                    credits_used = status_data.get("creditsUsed", "N/A")
                    logger.info(f"âœ… Agent ä»»åŠ¡å®Œæˆ (è€—æ—¶: {elapsed}s, credits: {credits_used})")
                    return status_data.get("data")
                elif status == "failed":
                    logger.error(f"Agent ä»»åŠ¡å¤±è´¥: {status_data}")
                    return None
                else:
                    logger.warning(f"æœªçŸ¥çŠ¶æ€: {status}")
            
            logger.error(f"Agent ä»»åŠ¡è¶…æ—¶ ({max_wait}s)")
            return None
            
        except Exception as e:
            logger.error(f"Agent è¯·æ±‚é”™è¯¯: {e}")
            return None


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="aaajiao ä½œå“é›†çˆ¬è™« - Firecrawl Edition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æŠ“å–æ‰€æœ‰ä½œå“ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
  python3 aaajiao_scraper.py
  
  # Agent æ¨¡å¼ï¼šå¼€æ”¾å¼æŸ¥è¯¢
  python3 aaajiao_scraper.py --agent "Find all video installations by aaajiao"
  
  # Agent æ¨¡å¼ + æŒ‡å®š URL
  python3 aaajiao_scraper.py --agent "Summarize this artwork" --urls "https://eventstructure.com/Absurd-Reality-Check"
        """
    )
    
    parser.add_argument(
        "--agent", "-a",
        type=str,
        metavar="PROMPT",
        help="ä½¿ç”¨ Agent æ¨¡å¼è¿›è¡Œå¼€æ”¾å¼æŸ¥è¯¢"
    )
    
    parser.add_argument(
        "--urls", "-u",
        type=str,
        metavar="URL1,URL2",
        help="Agent æ¨¡å¼ä¸‹æŒ‡å®šçš„ URL åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰"
    )
    
    parser.add_argument(
        "--max-credits",
        type=int,
        default=50,
        help="Agent æ¨¡å¼ä¸‹çš„æœ€å¤§ credits æ¶ˆè€—ï¼ˆé»˜è®¤: 50ï¼‰"
    )
    
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æŠ“å–"
    )
    
    args = parser.parse_args()
    
    scraper = AaajiaoScraper(use_cache=not args.no_cache)
    
    if args.agent:
        # Agent æ¨¡å¼
        urls = args.urls.split(",") if args.urls else None
        result = scraper.agent_search(args.agent, urls=urls, max_credits=args.max_credits)
        
        if result:
            print("\n" + "="*50)
            print("ğŸ“‹ Agent ç»“æœ:")
            print("="*50)
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print("âŒ Agent æŸ¥è¯¢å¤±è´¥")
            sys.exit(1)
    else:
        # é»˜è®¤æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰ä½œå“
        scraper.scrape_all()
        scraper.save_to_json()
        scraper.generate_markdown()


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
aaajiao ä½œå“é›†çˆ¬è™« (Optimized v3 - Firecrawl Edition)
ä» https://eventstructure.com/ æŠ“å–æ‰€æœ‰ä½œå“è¯¦ç»†ä¿¡æ¯

v3 æ”¹è¿›ï¼š
1. ä½¿ç”¨ Firecrawl AI æå–ç»“æ„åŒ–æ•°æ® (ç²¾å‡†åº¦å¤§å¹…æå‡)
2. API Key å®‰å…¨ç®¡ç† (ç¯å¢ƒå˜é‡)
3. æ™ºèƒ½é€Ÿç‡æ§åˆ¶ (é¿å… Rate Limit)
4. æœ¬åœ°ç¼“å­˜ (èŠ‚çœ API è°ƒç”¨)
5. å®æ—¶è¿›åº¦æ¡ (ç”¨æˆ·å‹å¥½)
"""

import os
import sys
import time
import re
import json
import logging
import hashlib
import pickle
import concurrent.futures
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin
import argparse
from threading import Lock
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


class RateLimiter:
    """çº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶å™¨"""
    def __init__(self, calls_per_minute: int = 5):
        self.interval = 60.0 / calls_per_minute
        self.last_call = 0
        self.lock = Lock()
    
    def wait(self):
        """ç­‰å¾…ç›´åˆ°å…è®¸ä¸‹ä¸€æ¬¡è°ƒç”¨"""
        with self.lock:
            elapsed = time.time() - self.last_call
            if elapsed < self.interval:
                sleep_time = self.interval - elapsed
                logger.debug(f"Rate limit: sleeping {sleep_time:.2f}s")
                time.sleep(sleep_time)
            self.last_call = time.time()

class AaajiaoScraper:
    BASE_URL = "https://eventstructure.com"
    SITEMAP_URL = "https://eventstructure.com/sitemap.xml"
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    MAX_WORKERS = 2  # é™ä½å¹¶å‘æ•°ï¼Œé…åˆé€Ÿç‡æ§åˆ¶
    TIMEOUT = 15
    FC_TIMEOUT = 30  # Firecrawl ä¸“ç”¨è¶…æ—¶

    def __init__(self, use_cache: bool = True):
        self.session = self._create_retry_session()
        self.works: List[Dict[str, Any]] = []
        self.use_cache = use_cache
        
        # åŠ è½½ API Key
        self.firecrawl_key = self._load_api_key()
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨ (5 calls/min)
        self.rate_limiter = RateLimiter(calls_per_minute=5)
        
        logger.info(f"Scraper åˆå§‹åŒ–å®Œæˆ (ç¼“å­˜: {'å¼€å¯' if use_cache else 'å…³é—­'})")
    
    def _load_api_key(self) -> str:
        """ä»ç¯å¢ƒå˜é‡æˆ– .env æ–‡ä»¶åŠ è½½ API Key"""
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
        key = os.getenv("FIRECRAWL_API_KEY")
        
        # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•è¯»å– .env æ–‡ä»¶
        if not key:
            env_file = os.path.join(os.path.dirname(__file__), '.env')
            if os.path.exists(env_file):
                with open(env_file, 'r') as f:
                    for line in f:
                        if line.startswith('FIRECRAWL_API_KEY='):
                            key = line.split('=', 1)[1].strip()
                            break
        
        if not key:
            raise ValueError(
                "æœªæ‰¾åˆ° Firecrawl API Keyï¼\n"
                "è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export FIRECRAWL_API_KEY='your-key'\n"
                "æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶"
            )
        
        logger.info(f"API Key åŠ è½½æˆåŠŸ (é•¿åº¦: {len(key)})")
        return key

    def _create_retry_session(self, retries: int = 3, backoff_factor: float = 0.5) -> requests.Session:
        session = requests.Session()
        retry = Retry(
            total=retries,
            read=retries,
            connect=retries,
            backoff_factor=backoff_factor,
            status_forcelist=[500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.headers.update(self.HEADERS)
        return session

    def get_all_work_links(self) -> List[str]:
        """ä» Sitemap è·å–æ‰€æœ‰ä½œå“é“¾æ¥"""
        logger.info(f"æ­£åœ¨è¯»å– Sitemap: {self.SITEMAP_URL}")
        try:
            response = self.session.get(self.SITEMAP_URL, timeout=self.TIMEOUT)
            response.raise_for_status()
            
            # ç®€å•çš„ XML è§£æ (é¿å…å¼•å…¥ lxml ä¾èµ–)
            soup = BeautifulSoup(response.content, 'html.parser') # xml parser needs lxml usually, html.parser handles basic tags ok
            
            links = []
            for loc in soup.find_all('loc'):
                url = loc.get_text().strip()
                if self._is_valid_work_link(url):
                    links.append(url)
            
            # å»é‡
            links = sorted(list(set(links)))
            logger.info(f"Sitemap ä¸­æ‰¾åˆ° {len(links)} ä¸ªæœ‰æ•ˆä½œå“é“¾æ¥")
            return links
            
        except Exception as e:
            logger.error(f"Sitemap è¯»å–å¤±è´¥: {e}")
            # Fallback to main page scan if sitemap fails
            return self._fallback_scan_main_page()

    def _fallback_scan_main_page(self):
        """å¤‡ç”¨æ–¹æ¡ˆï¼šä»ä¸»é¡µæ‰«æé“¾æ¥"""
        logger.info("å°è¯•æ‰«æä¸»é¡µé“¾æ¥ (å¤‡ç”¨æ–¹æ¡ˆ)...")
        try:
            r = self.session.get(self.BASE_URL, timeout=self.TIMEOUT)
            soup = BeautifulSoup(r.text, 'html.parser')
            links = set()
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith('/'):
                    full = urljoin(self.BASE_URL, href)
                    if self._is_valid_work_link(full):
                        links.add(full)
            return list(links)
        except Exception as e:
            logger.error(f"ä¸»é¡µæ‰«æå¤±è´¥: {e}")
            return []

    def _is_valid_work_link(self, url: str) -> bool:
        """è¿‡æ»¤éä½œå“é“¾æ¥"""
        if not url.startswith(self.BASE_URL):
            return False
            
        path = url.replace(self.BASE_URL, '')
        
        # æ’é™¤åˆ—è¡¨
        excludes = [
            '/', '/rss', '/feed', '/filter', '/aaajiao', 
            '/contact', '/cv', '/about', '/index', '/sitemap'
        ]
        
        if path in ['/', '']: return False
        
        for ex in excludes:
            if ex in path and len(path) < 20: # simple heuristic
                if path == ex or path.startswith(ex + '/'):
                    return False
        
        # Cargo ç‰¹æ€§: å¾€å¾€ä½œå“é“¾æ¥éƒ½å¾ˆçŸ­ï¼Œæˆ–è€…åŒ…å«ç‰¹å®šå…³é”®è¯
        # è¿™é‡Œä¸»è¦æ’é™¤ filter é¡µé¢
        if '/filter/' in path: return False
        
        return True

    def extract_work_details(self, url: str, retry_count: int = 0) -> Optional[Dict[str, Any]]:
        """æå–è¯¦æƒ… (ä½¿ç”¨ Firecrawl AI æå–ï¼Œå¸¦ç¼“å­˜å’Œé‡è¯•)"""
        max_retries = 3
        
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.use_cache:
            cached = self._load_cache(url)
            if cached:
                logger.debug(f"å‘½ä¸­ç¼“å­˜: {url}")
                return cached
        
        # 2. é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait()
        
        try:
            logger.info(f"[{retry_count+1}/{max_retries}] æ­£åœ¨æŠ“å–: {url}")
            
            fc_endpoint = "https://api.firecrawl.dev/v2/scrape"
            
            schema = {
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "The English title of the work"},
                    "title_cn": {"type": "string", "description": "The Chinese title of the work. If not explicitly found, leave empty."},
                    "year": {"type": "string", "description": "Creation year or year range (e.g. 2018-2022)"},
                    "type": {"type": "string", "description": "The art category (e.g. Video Installation, Software, Website)"},
                    "materials": {"type": "string", "description": "Materials list (e.g. LED screen, 3D printing)"},
                    "description_en": {"type": "string", "description": "Detailed work description in English. Exclude navigation text."},
                    "description_cn": {"type": "string", "description": "Detailed work description in Chinese. Exclude navigation text."},
                    "video_link": {"type": "string", "description": "Vimeo URL if present"}
                },
                "required": ["title"]
            }
            
            payload = {
                "url": url,
                "formats": ["extract"],
                "extract": {
                    "schema": schema,
                    "systemPrompt": "You are an art archivist. Extract the artwork metadata from the portfolio page. Ignore navigation links like 'Previous/Next project'. The title usually appears as 'English Title / Chinese Title'. Separate them."
                }
            }
            
            headers = {
                "Authorization": f"Bearer {self.firecrawl_key}",
                "Content-Type": "application/json"
            }
            
            resp = requests.post(fc_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
            
            if resp.status_code == 200:
                data = resp.json()
                if 'data' in data and 'extract' in data['data']:
                    result = data['data']['extract']
                    
                    work = {
                        'url': url,
                        'title': result.get('title', ''),
                        'title_cn': result.get('title_cn', ''),
                        'type': result.get('type', ''),
                        'materials': result.get('materials', ''),
                        'year': result.get('year', ''),
                        'description_cn': result.get('description_cn', ''),
                        'description_en': result.get('description_en', ''),
                        'video_link': result.get('video_link', ''),
                        'size': '',
                        'duration': '',
                        'tags': []
                    }
                    
                    # åå¤„ç†ï¼šå¦‚æœ AI æ²¡åˆ†æ¸…æ ‡é¢˜
                    if not work['title_cn'] and '/' in work['title']:
                        parts = work['title'].split('/')
                        work['title'] = parts[0].strip()
                        if len(parts) > 1:
                            work['title_cn'] = parts[1].strip()
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    if self.use_cache:
                        self._save_cache(url, work)
                            
                    return work
                else:
                    logger.error(f"Firecrawl è¿”å›æ ¼å¼å¼‚å¸¸: {data}")
                    
            elif resp.status_code == 429:
                # Rate Limit - æŒ‡æ•°é€€é¿é‡è¯•
                if retry_count >= max_retries:
                    logger.error(f"é‡è¯•æ¬¡æ•°è¶…é™: {url}")
                    return None
                wait_time = 2 ** retry_count  # 1s, 2s, 4s
                logger.warning(f"Rate Limitï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                time.sleep(wait_time)
                return self.extract_work_details(url, retry_count + 1)
                
            else:
                logger.error(f"Firecrawl Error {resp.status_code}: {resp.text[:200]}")
                
            return None

        except Exception as e:
            logger.error(f"API è¯·æ±‚é”™è¯¯ {url}: {e}")
            return None
    
    # ==================== ç¼“å­˜ç³»ç»Ÿ ====================
    
    def _get_cache_path(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_dir = os.path.join(os.path.dirname(__file__), '.cache')
        return os.path.join(cache_dir, f"{url_hash}.pkl")
    
    def _load_cache(self, url: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                pass
        return None
    
    def _save_cache(self, url: str, data: Dict):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(url)
        cache_dir = os.path.dirname(cache_path)
        os.makedirs(cache_dir, exist_ok=True)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.debug(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # ==================== æ•°æ®éªŒè¯ ====================
    
    def validate_work(self, work: Dict) -> bool:
        """éªŒè¯ä½œå“æ•°æ®å®Œæ•´æ€§"""
        if not work.get('title'):
            logger.warning(f"ä½œå“ç¼ºå°‘æ ‡é¢˜: {work.get('url')}")
            return False
        return True

    def scrape_all(self):
        """æŠ“å–æ‰€æœ‰ä½œå“ï¼ˆå¸¦è¿›åº¦æ¡å’ŒéªŒè¯ï¼‰"""
        work_links = self.get_all_work_links()
        total = len(work_links)
        valid_count = 0
        failed_count = 0
        
        logger.info(f"å¼€å§‹æŠ“å– {total} ä¸ªä½œå“...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self.extract_work_details, url): url for url in work_links}
            
            # ä½¿ç”¨ tqdm è¿›åº¦æ¡
            for future in tqdm(concurrent.futures.as_completed(future_to_url), 
                               total=total, 
                               desc="æŠ“å–è¿›åº¦",
                               unit="ä½œå“"):
                url = future_to_url[future]
                try:
                    data = future.result()
                    if data and self.validate_work(data):
                        self.works.append(data)
                        valid_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
                    failed_count += 1

        logger.info(f"æŠ“å–å®Œæˆï¼æœ‰æ•ˆ: {valid_count}/{total}, å¤±è´¥: {failed_count}")
        return self.works

    def save_to_json(self, filename: str = 'aaajiao_works.json'):
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.works, f, ensure_ascii=False, indent=2)

    def generate_markdown(self, filename: str = 'aaajiao_portfolio.md'):
        """ç”Ÿæˆ Markdown æ ¼å¼çš„ä½œå“é›†æ–‡æ¡£"""
        lines = [
            "# aaajiao ä½œå“é›† / aaajiao Portfolio\n",
            f"Source: {self.BASE_URL}\n",
            "Generated by aaajiao Scraper v3 (Firecrawl Edition)\n",
            "\n---\n\n"
        ]
        
        # Sort by year
        sorted_works = sorted(self.works, key=lambda x: x.get('year') or '0000', reverse=True)
        
        current_year = None
        for work in sorted_works:
            year = work.get('year', 'Unknown')
            if year != current_year:
                lines.append(f"## {year}\n\n")
                current_year = year
                
            title = work.get('title', 'Untitled')
            title_cn = work.get('title_cn', '')
            
            header = f"### [{title}]({work['url']})"
            if title_cn:
                header += f" / {title_cn}"
            lines.append(header + "\n\n")
            
            if work.get('type'): 
                lines.append(f"**Type**: {work['type']}\n\n")
            if work.get('materials'):
                lines.append(f"**Materials**: {work['materials']}\n\n")
            if work.get('video_link'): 
                lines.append(f"**Video**: {work['video_link']}\n\n")
            
            if work.get('description_cn'):
                lines.append(f"> {work['description_cn']}\n\n")
                
            if work.get('description_en'):
                lines.append(f"{work['description_en']}\n\n")
                 
            lines.append("---\n")
            
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("".join(lines))
        
        logger.info(f"Markdown æ–‡ä»¶å·²ç”Ÿæˆ: {filename}")

    # ==================== Discovery Mode ====================

    def discover_urls_with_scroll(self, url: str, scroll_mode: str = "auto") -> List[str]:
        """
        Phase 1: ä½¿ç”¨ Scrape æ¨¡å¼ + æ»šåŠ¨åŠ¨ä½œå»å‘ç°ä½œå“é“¾æ¥
        
        Args:
            url: ç›®æ ‡åˆ—è¡¨é¡µ URL
            scroll_mode: æ»šåŠ¨æ¨¡å¼ ("auto", "horizontal", "vertical")
            
        Returns:
            å‘ç°çš„ä½œå“ URL åˆ—è¡¨
        """
        logger.info(f"ğŸ•µï¸  å¯åŠ¨ Discovery Phase: {url} (Mode: {scroll_mode})")
        
        # 1. é…ç½®æ»šåŠ¨åŠ¨ä½œ (æŒ‰ç…§ Firecrawl å®˜æ–¹æ–‡æ¡£æ ¼å¼)
        actions = []
        
        # åˆå§‹ç­‰å¾…é¡µé¢åŠ è½½
        actions.append({"type": "wait", "milliseconds": 2000})
        
        if scroll_mode == "horizontal":
            # æ¨ªå‘æ»šåŠ¨ï¼šä½¿ç”¨ executeJavascript (åŸç”Ÿ scroll ä¸æ”¯æŒ horizontal)
            # å‘å³æ»šåŠ¨ 5 æ¬¡ï¼Œæ¯æ¬¡ 2000px
            for i in range(5):
                actions.append({
                    "type": "executeJavascript", 
                    "script": "window.scrollBy(2000, 0);"
                })
                actions.append({"type": "wait", "milliseconds": 1500})
                
        elif scroll_mode == "vertical":
            # å‚ç›´æ»šåŠ¨ï¼šä½¿ç”¨åŸç”Ÿ scroll action (å®˜æ–¹æ”¯æŒ up/down)
            for _ in range(3):
                actions.append({"type": "scroll", "direction": "down"})
                actions.append({"type": "wait", "milliseconds": 1000})
            
        else:  # auto æ¨¡å¼
            # æ··åˆæ¨¡å¼ï¼šå…ˆæ¨ªå‘æ»šåŠ¨ï¼Œå†å‚ç›´æ»šåŠ¨
            # 1. æ¨ªå‘æ»šåŠ¨ (executeJavascript)
            for i in range(5):
                actions.append({
                    "type": "executeJavascript", 
                    "script": "window.scrollBy(2000, 0);"
                })
                actions.append({"type": "wait", "milliseconds": 1200})
                
            # 2. å‚ç›´æ»šåŠ¨ (åŸç”Ÿ scroll)
            for _ in range(3):
                actions.append({"type": "scroll", "direction": "down"})
                actions.append({"type": "wait", "milliseconds": 1000})
        
        payload = {
            "url": url,
            "formats": ["html"],
            "actions": actions,
            "onlyMainContent": False  # è·å–å®Œæ•´ DOM ä»¥ä¾¿æå–é“¾æ¥
        }
        
        # ä½¿ç”¨ v2 endpoint (å®˜æ–¹æ–‡æ¡£æ¨è)
        endpoint = "https://api.firecrawl.dev/v2/scrape"
        headers = {
            "Authorization": f"Bearer {self.firecrawl_key}",
            "Content-Type": "application/json"
        }
        
        try:
            logger.info(f"   æ­£åœ¨æ‰§è¡Œ Scrape + Actions (å…± {len(actions)} æ­¥)...")
            resp = requests.post(endpoint, json=payload, headers=headers, timeout=120)
            
            if resp.status_code != 200:
                logger.error(f"Scrape å¤±è´¥: {resp.status_code} - {resp.text[:200]}")
                return []
                
            data = resp.json()
            if not data.get("success"):
                logger.error(f"Scrape ä»»åŠ¡å¤±è´¥: {data}")
                return []
                
            html_content = data.get("data", {}).get("html", "")
            if not html_content:
                logger.error("æœªè·å–åˆ° HTML å†…å®¹")
                return []
                
            # 2. ä» HTML æå–é“¾æ¥
            logger.info(f"   è·å–åˆ° HTML ({len(html_content)} chars)ï¼Œæ­£åœ¨æå–é“¾æ¥...")
            return self._extract_links_from_html(html_content, url)
            
        except Exception as e:
            logger.error(f"Discovery å¼‚å¸¸: {e}")
            return []

    def _extract_links_from_html(self, html: str, base_url: str) -> List[str]:
        """ä» HTML ä¸­æå–æœ‰ä»·å€¼çš„ä½œå“é“¾æ¥"""
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin
        
        soup = BeautifulSoup(html, 'html.parser')
        links = set()
        
        # aaajiao ç½‘ç«™ (eventstructure.com) ç‰¹å®šçš„é“¾æ¥æ¨¡å¼
        # é€šå¸¸æ˜¯ /Title-of-Work æ ¼å¼
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urljoin(base_url, href)
            
            # è¿‡æ»¤é€»è¾‘ï¼šåªä¿ç•™åƒæ˜¯ä½œå“è¯¦æƒ…é¡µçš„é“¾æ¥
            # æ’é™¤é¦–é¡µã€å…³äºé¡µç­‰
            if base_url in full_url and full_url != base_url:
                # æ’é™¤å¸¸è§éä½œå“é¡µé¢
                if not any(x in full_url.lower() for x in ['contact', 'about', 'cv', 'text', 'press', 'index']):
                    links.add(full_url)
                    
        sorted_links = sorted(list(links))
        logger.info(f"   å‘ç° {len(sorted_links)} ä¸ªæ½œåœ¨ä½œå“é“¾æ¥")
        return sorted_links

    # ==================== Agent æ¨¡å¼ ====================
    
    def agent_search(self, prompt: str, urls: Optional[List[str]] = None, max_credits: int = 50) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨ Firecrawl Agent è¿›è¡Œå¼€æ”¾å¼æŸ¥è¯¢
        
        Args:
            prompt: æŸ¥è¯¢æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            urls: å¯é€‰ï¼ŒæŒ‡å®šè¦æœç´¢çš„ URL åˆ—è¡¨
            max_credits: æœ€å¤§æ¶ˆè€— credits æ•°ï¼ˆæ§åˆ¶æˆæœ¬ï¼‰
            
        Returns:
            Agent è¿”å›çš„ç»“æ„åŒ–æ•°æ®
        """
        logger.info(f"ğŸ¤– å¯åŠ¨ Agent ä»»åŠ¡...")
        logger.info(f"   Prompt: {prompt}")
        if urls:
            logger.info(f"   URLs: {urls}")
        
        agent_endpoint = "https://api.firecrawl.dev/v2/agent"
        
        headers = {
            "Authorization": f"Bearer {self.firecrawl_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "prompt": prompt,
            "maxCredits": max_credits
        }
        
        if urls:
            payload["urls"] = urls
        
        try:
            # 1. å¯åŠ¨ Agent ä»»åŠ¡
            resp = requests.post(agent_endpoint, json=payload, headers=headers, timeout=self.FC_TIMEOUT)
            
            if resp.status_code != 200:
                logger.error(f"Agent å¯åŠ¨å¤±è´¥: {resp.status_code} - {resp.text[:200]}")
                return None
            
            result = resp.json()
            
            if not result.get("success"):
                logger.error(f"Agent å¯åŠ¨å¤±è´¥: {result}")
                return None
            
            job_id = result.get("id")
            if not job_id:
                # åŒæ­¥æ¨¡å¼ï¼šç›´æ¥è¿”å›ç»“æœ
                if result.get("status") == "completed":
                    logger.info(f"âœ… Agent ä»»åŠ¡å®Œæˆ (credits: {result.get('creditsUsed', 'N/A')})")
                    return result.get("data")
                logger.error(f"Agent è¿”å›æ ¼å¼å¼‚å¸¸: {result}")
                return None
            
            # 2. è½®è¯¢ç­‰å¾…ä»»åŠ¡å®Œæˆ
            logger.info(f"   ä»»åŠ¡ ID: {job_id}")
            status_endpoint = f"{agent_endpoint}/{job_id}"
            max_wait = 300  # æœ€é•¿ç­‰å¾… 5 åˆ†é’Ÿ
            poll_interval = 5  # æ¯ 5 ç§’æŸ¥è¯¢ä¸€æ¬¡
            elapsed = 0
            
            while elapsed < max_wait:
                time.sleep(poll_interval)
                elapsed += poll_interval
                
                status_resp = requests.get(status_endpoint, headers=headers, timeout=self.FC_TIMEOUT)
                
                if status_resp.status_code != 200:
                    logger.warning(f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {status_resp.status_code}")
                    continue
                
                status_data = status_resp.json()
                status = status_data.get("status")
                
                if status == "processing":
                    logger.info(f"   â³ å¤„ç†ä¸­... ({elapsed}s)")
                    continue
                elif status == "completed":
                    credits_used = status_data.get("creditsUsed", "N/A")
                    logger.info(f"âœ… Agent ä»»åŠ¡å®Œæˆ (è€—æ—¶: {elapsed}s, credits: {credits_used})")
                    return status_data.get("data")
                elif status == "failed":
                    logger.error(f"Agent ä»»åŠ¡å¤±è´¥: {status_data}")
                    return None
                else:
                    logger.warning(f"æœªçŸ¥çŠ¶æ€: {status}")
            
            logger.error(f"Agent ä»»åŠ¡è¶…æ—¶ ({max_wait}s)")
            return None
            
        except Exception as e:
            logger.error(f"Agent è¯·æ±‚é”™è¯¯: {e}")
            return None

    # ==================== å›¾ç‰‡ä¸‹è½½å’ŒæŠ¥å‘Šç”Ÿæˆ ====================
    
    def download_images(self, image_urls: List[str], output_dir: str, timestamp: str = "") -> List[str]:
        """
        ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
        
        Args:
            image_urls: å›¾ç‰‡ URL åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            timestamp: æ—¶é—´æˆ³ï¼Œç”¨äºåˆ›å»ºç‹¬ç«‹çš„å›¾ç‰‡æ–‡ä»¶å¤¹
            
        Returns:
            æœ¬åœ°å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹å
        folder_name = f"images_{timestamp}" if timestamp else "images"
        images_dir = os.path.join(output_dir, folder_name)
        os.makedirs(images_dir, exist_ok=True)
        
        local_paths = []
        for i, url in enumerate(image_urls):
            try:
                # æ¸…ç† URL
                url = url.strip()
                if not url or not url.startswith(('http://', 'https://')):
                    continue
                
                # ç”Ÿæˆæ–‡ä»¶å
                ext = os.path.splitext(url.split('?')[0])[-1] or '.jpg'
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                    ext = '.jpg'
                filename = f"{i+1:02d}_image{ext}"
                local_path = os.path.join(images_dir, filename)
                
                logger.info(f"   ä¸‹è½½å›¾ç‰‡ [{i+1}/{len(image_urls)}]: {filename}")
                
                # ä¸‹è½½
                resp = self.session.get(url, timeout=30, stream=True)
                if resp.status_code == 200:
                    with open(local_path, 'wb') as f:
                        for chunk in resp.iter_content(chunk_size=8192):
                            f.write(chunk)
                    local_paths.append(f"{folder_name}/{filename}")
                else:
                    logger.warning(f"   ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {resp.status_code})")
                    
            except Exception as e:
                logger.warning(f"   ä¸‹è½½å¼‚å¸¸: {url} - {e}")
                
        return local_paths
    
    def generate_agent_report(self, data: Dict[str, Any], output_dir: str, prompt: str = ""):
        """
        æ ¹æ® Agent è¿”å›çš„æ•°æ®ç”Ÿæˆ Markdown æŠ¥å‘Šå’Œä¸‹è½½å›¾ç‰‡
        
        Args:
            data: Agent è¿”å›çš„æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            prompt: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ prompt
        """
        from datetime import datetime
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"ğŸ“ ç”ŸæˆæŠ¥å‘Šåˆ°: {output_dir}")
        
        # 1. æå–å›¾ç‰‡ URL å¹¶ä¸‹è½½
        image_urls = self._extract_image_urls(data)
        local_images = []
        
        if image_urls:
            logger.info(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(image_urls)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            local_images = self.download_images(image_urls, output_dir, timestamp=timestamp)
            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(local_images)} å¼ å›¾ç‰‡")
        
        # 2. ç”Ÿæˆ Markdown æŠ¥å‘Šï¼ˆå¸¦æ—¶é—´æˆ³æ–‡ä»¶åï¼‰
        report_filename = f"report_{timestamp}.md"
        report_path = os.path.join(output_dir, report_filename)
        
        lines = []
        
        # æ ‡é¢˜
        title = data.get('title', data.get('artwork_title', 'Untitled'))
        if isinstance(title, str):
            lines.append(f"# {title}\n\n")
        
        # æŸ¥è¯¢ä¿¡æ¯
        lines.append(f"> **æŸ¥è¯¢æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        if prompt:
            lines.append(f"> **Prompt:** {prompt}\n")
        lines.append("\n---\n\n")
        
        # å…ƒæ•°æ®è¡¨æ ¼
        metadata_fields = [
            ('artist', 'è‰ºæœ¯å®¶'),
            ('year', 'å¹´ä»½'),
            ('artwork_type', 'ç±»å‹'),
            ('type', 'ç±»å‹'),
            ('materials', 'ææ–™'),
            ('dimensions', 'å°ºå¯¸'),
            ('duration', 'æ—¶é•¿'),
        ]
        
        metadata_lines = []
        for key, label in metadata_fields:
            value = data.get(key)
            if value and key != 'title':
                metadata_lines.append(f"**{label}:** {value}")
        
        if metadata_lines:
            lines.append("\n".join(metadata_lines))
            lines.append("\n\n")
        
        # å›¾ç‰‡
        if local_images:
            lines.append("## å›¾ç‰‡\n\n")
            for img_path in local_images:
                lines.append(f"![{img_path}]({img_path})\n\n")
        
        # æè¿°/æ¦‚å¿µ
        for field in ['description', 'summary', 'concept', 'description_en', 'description_cn']:
            value = data.get(field)
            if value and isinstance(value, str):
                lines.append(f"## æè¿°\n\n{value}\n\n")
                break
        
        # å±•è§ˆä¿¡æ¯
        exhibition = data.get('exhibition')
        if exhibition and isinstance(exhibition, dict):
            lines.append("## å±•è§ˆä¿¡æ¯\n\n")
            for key, value in exhibition.items():
                if value:
                    lines.append(f"- **{key}:** {value}\n")
            lines.append("\n")
        
        # å…¶ä»–å­—æ®µï¼ˆJSON æ ¼å¼ï¼‰
        excluded = {'title', 'artist', 'year', 'artwork_type', 'type', 'materials', 
                   'dimensions', 'duration', 'description', 'summary', 'concept',
                   'description_en', 'description_cn', 'exhibition', 'image_urls', 'images'}
        
        other_data = {k: v for k, v in data.items() if k not in excluded and v}
        if other_data:
            lines.append("## å…¶ä»–ä¿¡æ¯\n\n")
            lines.append("```json\n")
            lines.append(json.dumps(other_data, indent=2, ensure_ascii=False))
            lines.append("\n```\n")
        
        # å†™å…¥æ–‡ä»¶
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("".join(lines))
        
        logger.info(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # åŒæ—¶ä¿å­˜åŸå§‹ JSONï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        json_filename = f"data_{timestamp}.json"
        json_path = os.path.join(output_dir, json_filename)
        
        # åœ¨ JSON ä¸­ä¹Ÿä¿å­˜ prompt ä¿¡æ¯
        output_data = {
            "_meta": {
                "prompt": prompt,
                "timestamp": datetime.now().isoformat(),
            },
            **data
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ JSON æ•°æ®å·²ä¿å­˜: {json_path}")
    
    def _extract_image_urls(self, data: Dict[str, Any]) -> List[str]:
        """ä» Agent è¿”å›æ•°æ®ä¸­æå–æ‰€æœ‰å›¾ç‰‡ URL"""
        urls = []
        
        # å¸¸è§çš„å›¾ç‰‡å­—æ®µå
        image_fields = ['image_urls', 'images', 'image', 'imageUrls', 'imageUrl', 
                       'cover_image', 'thumbnail', 'photos', 'gallery']
        
        def extract_from_value(value):
            if isinstance(value, str):
                if value.startswith(('http://', 'https://')) and any(ext in value.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', 'image']):
                    urls.append(value)
            elif isinstance(value, list):
                for item in value:
                    extract_from_value(item)
            elif isinstance(value, dict):
                for v in value.values():
                    extract_from_value(v)
        
        # ä¼˜å…ˆæ£€æŸ¥å·²çŸ¥å­—æ®µ
        for field in image_fields:
            if field in data:
                extract_from_value(data[field])
        
        # é€’å½’æœç´¢æ‰€æœ‰å€¼
        if not urls:
            extract_from_value(data)
        
        return list(set(urls))  # å»é‡


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="aaajiao ä½œå“é›†çˆ¬è™« - Firecrawl Edition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æŠ“å–æ‰€æœ‰ä½œå“ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
  python3 aaajiao_scraper.py
  
  # Agent æ¨¡å¼ï¼šå¼€æ”¾å¼æŸ¥è¯¢
  python3 aaajiao_scraper.py --agent "Find all video installations by aaajiao"
  
  # Agent æ¨¡å¼ + æŒ‡å®š URL + å›¾ç‰‡ä¸‹è½½
  python3 aaajiao_scraper.py --agent "Get complete info including images" --urls "https://eventstructure.com/Absurd-Reality-Check" --output-dir ./agent_output
        """
    )
    
    parser.add_argument(
        "--agent", "-a",
        type=str,
        metavar="PROMPT",
        help="ä½¿ç”¨ Agent æ¨¡å¼è¿›è¡Œå¼€æ”¾å¼æŸ¥è¯¢"
    )
    
    parser.add_argument(
        "--urls", "-u",
        type=str,
        metavar="URL1,URL2",
        help="Agent æ¨¡å¼ä¸‹æŒ‡å®šçš„ URL åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰"
    )
    
    parser.add_argument(
        "--max-credits",
        type=int,
        default=50,
        help="Agent æ¨¡å¼ä¸‹çš„æœ€å¤§ credits æ¶ˆè€—ï¼ˆé»˜è®¤: 50ï¼‰"
    )
    
    parser.add_argument(
        "--discovery-url", "-d",
        type=str,
        help="[New] ä½¿ç”¨ Scrape+Agent æ¨¡å¼ï¼šå…ˆæ»šåŠ¨å‘ç° URLï¼Œå†ç”¨ Agent æå–"
    )
    
    parser.add_argument(
        "--scroll-mode",
        choices=["auto", "horizontal", "vertical"],
        default="auto",
        help="Discovery æ¨¡å¼ä¸‹çš„æ»šåŠ¨ç­–ç•¥ (default: auto)"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        type=str,
        metavar="DIR",
        help="Agent æ¨¡å¼ä¸‹çš„è¾“å‡ºç›®å½•ï¼ˆå°†ä¸‹è½½å›¾ç‰‡å¹¶ç”Ÿæˆ Markdown æŠ¥å‘Šï¼‰"
    )
    
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æŠ“å–"
    )
    
    args = parser.parse_args()
    
    scraper = AaajiaoScraper(use_cache=not args.no_cache)
    
    if args.discovery_url:
        # ====================== Discovery Mode ======================
        logger.info(f"ğŸš€ å¯åŠ¨æ··åˆæ¨¡å¼ (Scrape Discovery -> Agent Extraction) [Scroll: {args.scroll_mode}]")
        
        # Phase 1: Discovery
        found_urls = scraper.discover_urls_with_scroll(args.discovery_url, scroll_mode=args.scroll_mode)
        
        if not found_urls:
            logger.error("âŒ æœªå‘ç°ä»»ä½•é“¾æ¥ï¼Œé€€å‡º")
            sys.exit(1)
            
        logger.info(f"ğŸ“‹ å…±å‘ç° {len(found_urls)} ä¸ªä½œå“é“¾æ¥")
        
        # é™åˆ¶æ•°é‡ç”¨äºæµ‹è¯• (å¯é€‰ï¼Œè¿™é‡Œå…ˆå¤„ç†å‰ 5 ä¸ªé¿å…æ¶ˆè€—è¿‡å¤š)
        # found_urls = found_urls[:5] 
        # logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ 5 ä¸ªé“¾æ¥")
        
        # Phase 2: Agent Extraction
        prompt = args.agent or "Deeply analyze these artworks. Extract title, year, materials, description, concept, and exhibition history."
        enhanced_prompt = prompt
        
        if args.output_dir:
            if "image" not in prompt.lower():
                enhanced_prompt = f"{prompt}. Also extract all image URLs for each artwork."
        
        logger.info("ğŸ¤– æäº¤ Agent æ‰¹é‡å¤„ç†ä»»åŠ¡ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
        
        # ä¼ é€’å‘ç°çš„æ‰€æœ‰ URL ç»™ Agent
        # æ³¨æ„ï¼šURL å¤ªå¤šå¯èƒ½ä¼šå¯¼è‡´ Agent ä»»åŠ¡è¿‡å¤§ï¼ŒFirecrawl å»ºè®®ä¸€æ¬¡å¤„ç†å°‘é‡ URL
        # è¿™é‡Œæ¼”ç¤ºåŸç†ï¼Œå®é™…ä½¿ç”¨å¯èƒ½éœ€è¦åˆ‡ç‰‡åˆ†æ‰¹å¤„ç†
        
        result = scraper.agent_search(enhanced_prompt, urls=found_urls, max_credits=args.max_credits)
        
        if result:
            print("\n" + "="*50)
            print("ğŸ“‹ Discovery + Agent ç»“æœ:")
            print("="*50)
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            if args.output_dir:
                scraper.generate_agent_report(result, args.output_dir, prompt=enhanced_prompt)
        else:
            print("âŒ Agent ä»»åŠ¡å¤±è´¥")
            sys.exit(1)
            
    elif args.agent:
        # ====================== Standard Agent Mode ======================
        # Agent æ¨¡å¼ - å¢å¼º prompt ä»¥è¯·æ±‚å›¾ç‰‡
        enhanced_prompt = args.agent
        if args.output_dir:
            # è‡ªåŠ¨æ·»åŠ å›¾ç‰‡è¯·æ±‚åˆ° prompt
            if "image" not in args.agent.lower():
                enhanced_prompt = f"{args.agent}. Also extract all image URLs from the page."
        
        urls = args.urls.split(",") if args.urls else None
        result = scraper.agent_search(enhanced_prompt, urls=urls, max_credits=args.max_credits)
        
        if result:
            print("\n" + "="*50)
            print("ğŸ“‹ Agent ç»“æœ:")
            print("="*50)
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œä¸‹è½½å›¾ç‰‡å¹¶ç”ŸæˆæŠ¥å‘Š
            if args.output_dir:
                scraper.generate_agent_report(result, args.output_dir, prompt=enhanced_prompt)
        else:
            print("âŒ Agent æŸ¥è¯¢å¤±è´¥")
            sys.exit(1)
    else:
        # ====================== Standard Scrape Mode ======================
        # é»˜è®¤æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰ä½œå“
        scraper.scrape_all()
        scraper.save_to_json()
        scraper.generate_markdown()


if __name__ == "__main__":
    main()